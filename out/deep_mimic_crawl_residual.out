wandb: Currently logged in as: ankitaghosh0907 (dh-project). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /cluster/home/anghosh/DHProject/deep-mimic/log/wandb/run-20230529_221457-478migll
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 2023-05-29-22-14-49-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M-residual_1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/dh-project/DH-Project
wandb: üöÄ View run at https://wandb.ai/dh-project/DH-Project/runs/478migll
wandb: WARNING Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
- config file path = /cluster/home/anghosh/DHProject/deep-mimic/data/conf/bob_env_crawl_residual.json
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Logging to /cluster/home/anghosh/DHProject/deep-mimic/log/2023-05-29-22-14-49-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M-residual_1
/cluster/home/anghosh/venv/lib64/python3.9/site-packages/stable_baselines3/common/callbacks.py:345: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x2b77a24c28e0> != <stable_baselines3.common.vec_env.vec_video_recorder.VecVideoRecorder object at 0x2b77a24d3580>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
Ignoring input line: position 0 0.90 0
Saving video to /cluster/home/anghosh/DHProject/deep-mimic/log/2023-05-29-22-14-49-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M-residual_1/eval-video-PylocoVanilla-v0-step-0-to-step-1000.mp4
Eval num_timesteps=20, episode_reward=222.08 +/- 0.00
Episode length: 527.00 +/- 0.00
Success rate: 100.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 527      |
|    mean_reward     | 222      |
|    success_rate    | 1        |
| time/              |          |
|    total_timesteps | 20       |
---------------------------------
New best mean reward!
--------------------------------------
| err_terms/              |          |
|    com_err              | 0.78     |
|    end_effectors_err    | 3.38     |
|    height_err           | -0.0833  |
|    joints_err           | 0.652    |
|    joints_vel_err       | 220      |
|    root_ori_err         | 0.0396   |
| reward_terms/           |          |
|    com_reward           | 0.0179   |
|    end_effectors_reward | 0.0184   |
|    height_reward        | 0.0666   |
|    joints_reward        | 0.155    |
|    joints_vel_reward    | 1.47e-07 |
|    root_ori_reward      | 0.005    |
|    smoothness1_reward   | 0        |
|    smoothness2_reward   | 0        |
|    smoothness_reward    | 0        |
| rollout/                |          |
|    ep_len_mean          | 250      |
|    ep_rew_mean          | 66       |
| time/                   |          |
|    collect_time         | 6.38e+03 |
|    fps                  | 12       |
|    iterations           | 1        |
|    time_elapsed         | 6376     |
|    total_timesteps      | 81920    |
--------------------------------------
------------------------------------------
| err_terms/               |             |
|    com_err               | 0.625       |
|    end_effectors_err     | 2.81        |
|    height_err            | -0.0843     |
|    joints_err            | 0.656       |
|    joints_vel_err        | 225         |
|    root_ori_err          | 0.161       |
| reward_terms/            |             |
|    com_reward            | 0.0188      |
|    end_effectors_reward  | 0.0202      |
|    height_reward         | 0.0668      |
|    joints_reward         | 0.154       |
|    joints_vel_reward     | 2.32e-07    |
|    root_ori_reward       | 0.00358     |
|    smoothness1_reward    | 0           |
|    smoothness2_reward    | 0           |
|    smoothness_reward     | 0           |
| rollout/                 |             |
|    ep_len_mean           | 322         |
|    ep_rew_mean           | 84.4        |
| time/                    |             |
|    collect_time          | 112         |
|    evaluate_actions_time | 0.00197     |
|    fps                   | 25          |
|    iterations            | 2           |
|    time_elapsed          | 6519        |
|    total_timesteps       | 163840      |
|    train_time            | 30.9        |
| train/                   |             |
|    approx_kl             | 0.108194396 |
|    clip_fraction         | 0.579       |
|    clip_range            | 0.2         |
|    entropy_loss          | 25.6        |
|    explained_variance    | -0.315      |
|    learning_rate_log_std | 0.0003      |
|    learning_rate_policy  | 5e-05       |
|    learning_rate_value   | 0.01        |
|    loss                  | 0.0756      |
|    n_updates             | 10          |
|    policy_gradient_loss  | -0.0498     |
|    std                   | 0.135       |
|    value_loss            | 1.34        |
------------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.6        |
|    end_effectors_err     | 2.36       |
|    height_err            | -0.0569    |
|    joints_err            | 0.706      |
|    joints_vel_err        | 225        |
|    root_ori_err          | 0.614      |
| reward_terms/            |            |
|    com_reward            | 0.028      |
|    end_effectors_reward  | 0.0264     |
|    height_reward         | 0.0536     |
|    joints_reward         | 0.154      |
|    joints_vel_reward     | 1.83e-07   |
|    root_ori_reward       | 0.00392    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 304        |
|    ep_rew_mean           | 82.6       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 36         |
|    iterations            | 3          |
|    time_elapsed          | 6664       |
|    total_timesteps       | 245760     |
|    train_time            | 31.4       |
| train/                   |            |
|    approx_kl             | 0.09914491 |
|    clip_fraction         | 0.548      |
|    clip_range            | 0.2        |
|    entropy_loss          | 25.8       |
|    explained_variance    | 0.541      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.0464     |
|    n_updates             | 20         |
|    policy_gradient_loss  | -0.0766    |
|    std                   | 0.135      |
|    value_loss            | 0.336      |
-----------------------------------------
------------------------------------------
| err_terms/               |             |
|    com_err               | 0.775       |
|    end_effectors_err     | 3.24        |
|    height_err            | -0.0758     |
|    joints_err            | 0.617       |
|    joints_vel_err        | 208         |
|    root_ori_err          | 0.0824      |
| reward_terms/            |             |
|    com_reward            | 0.0161      |
|    end_effectors_reward  | 0.0163      |
|    height_reward         | 0.0704      |
|    joints_reward         | 0.165       |
|    joints_vel_reward     | 1.67e-07    |
|    root_ori_reward       | 0.00646     |
|    smoothness1_reward    | 0           |
|    smoothness2_reward    | 0           |
|    smoothness_reward     | 0           |
| rollout/                 |             |
|    ep_len_mean           | 285         |
|    ep_rew_mean           | 81.5        |
| time/                    |             |
|    collect_time          | 111         |
|    evaluate_actions_time | 0.00203     |
|    fps                   | 48          |
|    iterations            | 4           |
|    time_elapsed          | 6808        |
|    total_timesteps       | 327680      |
|    train_time            | 31.6        |
| train/                   |             |
|    approx_kl             | 0.103796646 |
|    clip_fraction         | 0.56        |
|    clip_range            | 0.2         |
|    entropy_loss          | 25.9        |
|    explained_variance    | 0.588       |
|    learning_rate_log_std | 0.0003      |
|    learning_rate_policy  | 5e-05       |
|    learning_rate_value   | 0.01        |
|    loss                  | 0.0762      |
|    n_updates             | 30          |
|    policy_gradient_loss  | -0.0759     |
|    std                   | 0.134       |
|    value_loss            | 0.352       |
------------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.664      |
|    end_effectors_err     | 2.8        |
|    height_err            | -0.117     |
|    joints_err            | 0.611      |
|    joints_vel_err        | 215        |
|    root_ori_err          | 0.851      |
| reward_terms/            |            |
|    com_reward            | 0.0226     |
|    end_effectors_reward  | 0.0201     |
|    height_reward         | 0.0548     |
|    joints_reward         | 0.174      |
|    joints_vel_reward     | 2.08e-07   |
|    root_ori_reward       | 0.00627    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 278        |
|    ep_rew_mean           | 80.3       |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 58         |
|    iterations            | 5          |
|    time_elapsed          | 6951       |
|    total_timesteps       | 409600     |
|    train_time            | 31.1       |
| train/                   |            |
|    approx_kl             | 0.10898372 |
|    clip_fraction         | 0.576      |
|    clip_range            | 0.2        |
|    entropy_loss          | 26.1       |
|    explained_variance    | 0.565      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.104      |
|    n_updates             | 40         |
|    policy_gradient_loss  | -0.074     |
|    std                   | 0.134      |
|    value_loss            | 0.374      |
-----------------------------------------
------------------------------------------
| err_terms/               |             |
|    com_err               | 0.505       |
|    end_effectors_err     | 1.39        |
|    height_err            | -0.0603     |
|    joints_err            | 0.558       |
|    joints_vel_err        | 212         |
|    root_ori_err          | 0.0761      |
| reward_terms/            |             |
|    com_reward            | 0.0255      |
|    end_effectors_reward  | 0.0263      |
|    height_reward         | 0.0772      |
|    joints_reward         | 0.187       |
|    joints_vel_reward     | 2.39e-07    |
|    root_ori_reward       | 0.00776     |
|    smoothness1_reward    | 0           |
|    smoothness2_reward    | 0           |
|    smoothness_reward     | 0           |
| rollout/                 |             |
|    ep_len_mean           | 283         |
|    ep_rew_mean           | 85.1        |
| time/                    |             |
|    collect_time          | 113         |
|    evaluate_actions_time | 0.00201     |
|    fps                   | 69          |
|    iterations            | 6           |
|    time_elapsed          | 7095        |
|    total_timesteps       | 491520      |
|    train_time            | 31.6        |
| train/                   |             |
|    approx_kl             | 0.115480885 |
|    clip_fraction         | 0.582       |
|    clip_range            | 0.2         |
|    entropy_loss          | 26.3        |
|    explained_variance    | 0.545       |
|    learning_rate_log_std | 0.0003      |
|    learning_rate_policy  | 5e-05       |
|    learning_rate_value   | 0.01        |
|    loss                  | 0.166       |
|    n_updates             | 50          |
|    policy_gradient_loss  | -0.0716     |
|    std                   | 0.133       |
|    value_loss            | 0.443       |
------------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.997     |
|    end_effectors_err     | 5.35      |
|    height_err            | -0.0585   |
|    joints_err            | 0.538     |
|    joints_vel_err        | 207       |
|    root_ori_err          | 0.0909    |
| reward_terms/            |           |
|    com_reward            | 0.0104    |
|    end_effectors_reward  | 0.0131    |
|    height_reward         | 0.0786    |
|    joints_reward         | 0.19      |
|    joints_vel_reward     | 1.33e-07  |
|    root_ori_reward       | 0.00366   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 286       |
|    ep_rew_mean           | 88.1      |
| time/                    |           |
|    collect_time          | 113       |
|    evaluate_actions_time | 0.002     |
|    fps                   | 79        |
|    iterations            | 7         |
|    time_elapsed          | 7240      |
|    total_timesteps       | 573440    |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 0.1236691 |
|    clip_fraction         | 0.593     |
|    clip_range            | 0.2       |
|    entropy_loss          | 26.4      |
|    explained_variance    | 0.542     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.128     |
|    n_updates             | 60        |
|    policy_gradient_loss  | -0.073    |
|    std                   | 0.133     |
|    value_loss            | 0.463     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.887      |
|    end_effectors_err     | 4.77       |
|    height_err            | -0.0542    |
|    joints_err            | 0.538      |
|    joints_vel_err        | 204        |
|    root_ori_err          | 0.146      |
| reward_terms/            |            |
|    com_reward            | 0.0148     |
|    end_effectors_reward  | 0.0152     |
|    height_reward         | 0.0792     |
|    joints_reward         | 0.19       |
|    joints_vel_reward     | 4.07e-07   |
|    root_ori_reward       | 0.000578   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 242        |
|    ep_rew_mean           | 76.7       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 88         |
|    iterations            | 8          |
|    time_elapsed          | 7386       |
|    total_timesteps       | 655360     |
|    train_time            | 31.4       |
| train/                   |            |
|    approx_kl             | 0.13164294 |
|    clip_fraction         | 0.605      |
|    clip_range            | 0.2        |
|    entropy_loss          | 26.6       |
|    explained_variance    | 0.577      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.133      |
|    n_updates             | 70         |
|    policy_gradient_loss  | -0.0732    |
|    std                   | 0.132      |
|    value_loss            | 0.437      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.61       |
|    end_effectors_err     | 2.73       |
|    height_err            | -0.0598    |
|    joints_err            | 0.524      |
|    joints_vel_err        | 200        |
|    root_ori_err          | 0.101      |
| reward_terms/            |            |
|    com_reward            | 0.0222     |
|    end_effectors_reward  | 0.0193     |
|    height_reward         | 0.077      |
|    joints_reward         | 0.193      |
|    joints_vel_reward     | 1.1e-06    |
|    root_ori_reward       | 0.000797   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 261        |
|    ep_rew_mean           | 85.7       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 97         |
|    iterations            | 9          |
|    time_elapsed          | 7532       |
|    total_timesteps       | 737280     |
|    train_time            | 31.9       |
| train/                   |            |
|    approx_kl             | 0.13382652 |
|    clip_fraction         | 0.606      |
|    clip_range            | 0.2        |
|    entropy_loss          | 26.8       |
|    explained_variance    | 0.571      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.141      |
|    n_updates             | 80         |
|    policy_gradient_loss  | -0.0686    |
|    std                   | 0.131      |
|    value_loss            | 0.495      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.241      |
|    end_effectors_err     | 0.313      |
|    height_err            | -0.0221    |
|    joints_err            | 0.472      |
|    joints_vel_err        | 206        |
|    root_ori_err          | 0.063      |
| reward_terms/            |            |
|    com_reward            | 0.0495     |
|    end_effectors_reward  | 0.0415     |
|    height_reward         | 0.0957     |
|    joints_reward         | 0.212      |
|    joints_vel_reward     | 3.39e-07   |
|    root_ori_reward       | 0.0198     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 273        |
|    ep_rew_mean           | 90.6       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 106        |
|    iterations            | 10         |
|    time_elapsed          | 7677       |
|    total_timesteps       | 819200     |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.14122833 |
|    clip_fraction         | 0.621      |
|    clip_range            | 0.2        |
|    entropy_loss          | 27         |
|    explained_variance    | 0.564      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.234      |
|    n_updates             | 90         |
|    policy_gradient_loss  | -0.0714    |
|    std                   | 0.131      |
|    value_loss            | 0.492      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 1.12      |
|    end_effectors_err     | 6.15      |
|    height_err            | -0.168    |
|    joints_err            | 0.548     |
|    joints_vel_err        | 211       |
|    root_ori_err          | 1.44      |
| reward_terms/            |           |
|    com_reward            | 0.00976   |
|    end_effectors_reward  | 0.0105    |
|    height_reward         | 0.0317    |
|    joints_reward         | 0.195     |
|    joints_vel_reward     | 4.79e-07  |
|    root_ori_reward       | 0.00248   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 298       |
|    ep_rew_mean           | 99.7      |
| time/                    |           |
|    collect_time          | 113       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 115       |
|    iterations            | 11        |
|    time_elapsed          | 7821      |
|    total_timesteps       | 901120    |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 0.1466469 |
|    clip_fraction         | 0.627     |
|    clip_range            | 0.2       |
|    entropy_loss          | 27.2      |
|    explained_variance    | 0.563     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.182     |
|    n_updates             | 100       |
|    policy_gradient_loss  | -0.0704   |
|    std                   | 0.13      |
|    value_loss            | 0.507     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.683      |
|    end_effectors_err     | 3.02       |
|    height_err            | -0.0293    |
|    joints_err            | 0.423      |
|    joints_vel_err        | 194        |
|    root_ori_err          | 0.118      |
| reward_terms/            |            |
|    com_reward            | 0.0236     |
|    end_effectors_reward  | 0.0208     |
|    height_reward         | 0.0917     |
|    joints_reward         | 0.229      |
|    joints_vel_reward     | 5.14e-07   |
|    root_ori_reward       | 0.0055     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 274        |
|    ep_rew_mean           | 95.1       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00198    |
|    fps                   | 123        |
|    iterations            | 12         |
|    time_elapsed          | 7966       |
|    total_timesteps       | 983040     |
|    train_time            | 31         |
| train/                   |            |
|    approx_kl             | 0.15018246 |
|    clip_fraction         | 0.631      |
|    clip_range            | 0.2        |
|    entropy_loss          | 27.4       |
|    explained_variance    | 0.56       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.131      |
|    n_updates             | 110        |
|    policy_gradient_loss  | -0.0689    |
|    std                   | 0.13       |
|    value_loss            | 0.526      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.947      |
|    end_effectors_err     | 4.65       |
|    height_err            | -0.042     |
|    joints_err            | 0.442      |
|    joints_vel_err        | 196        |
|    root_ori_err          | 0.0452     |
| reward_terms/            |            |
|    com_reward            | 0.0111     |
|    end_effectors_reward  | 0.0141     |
|    height_reward         | 0.0847     |
|    joints_reward         | 0.226      |
|    joints_vel_reward     | 9.48e-07   |
|    root_ori_reward       | 0.00831    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 292        |
|    ep_rew_mean           | 103        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 131        |
|    iterations            | 13         |
|    time_elapsed          | 8111       |
|    total_timesteps       | 1064960    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.15507242 |
|    clip_fraction         | 0.635      |
|    clip_range            | 0.2        |
|    entropy_loss          | 27.6       |
|    explained_variance    | 0.587      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.182      |
|    n_updates             | 120        |
|    policy_gradient_loss  | -0.0702    |
|    std                   | 0.129      |
|    value_loss            | 0.551      |
-----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 0.388    |
|    end_effectors_err     | 0.856    |
|    height_err            | -0.0269  |
|    joints_err            | 0.398    |
|    joints_vel_err        | 209      |
|    root_ori_err          | 0.0137   |
| reward_terms/            |          |
|    com_reward            | 0.0331   |
|    end_effectors_reward  | 0.0322   |
|    height_reward         | 0.0919   |
|    joints_reward         | 0.243    |
|    joints_vel_reward     | 1.75e-07 |
|    root_ori_reward       | 0.00901  |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 288      |
|    ep_rew_mean           | 104      |
| time/                    |          |
|    collect_time          | 113      |
|    evaluate_actions_time | 0.00202  |
|    fps                   | 138      |
|    iterations            | 14       |
|    time_elapsed          | 8256     |
|    total_timesteps       | 1146880  |
|    train_time            | 31.9     |
| train/                   |          |
|    approx_kl             | 0.167248 |
|    clip_fraction         | 0.645    |
|    clip_range            | 0.2      |
|    entropy_loss          | 27.8     |
|    explained_variance    | 0.577    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | 0.0997   |
|    n_updates             | 130      |
|    policy_gradient_loss  | -0.07    |
|    std                   | 0.128    |
|    value_loss            | 0.539    |
---------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.154      |
|    end_effectors_err     | 0.192      |
|    height_err            | -0.0222    |
|    joints_err            | 0.362      |
|    joints_vel_err        | 187        |
|    root_ori_err          | 0.0688     |
| reward_terms/            |            |
|    com_reward            | 0.0725     |
|    end_effectors_reward  | 0.0446     |
|    height_reward         | 0.0909     |
|    joints_reward         | 0.264      |
|    joints_vel_reward     | 3.43e-07   |
|    root_ori_reward       | 0.00541    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 303        |
|    ep_rew_mean           | 111        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00205    |
|    fps                   | 146        |
|    iterations            | 15         |
|    time_elapsed          | 8401       |
|    total_timesteps       | 1228800    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.16370925 |
|    clip_fraction         | 0.648      |
|    clip_range            | 0.2        |
|    entropy_loss          | 28.1       |
|    explained_variance    | 0.554      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.238      |
|    n_updates             | 140        |
|    policy_gradient_loss  | -0.0683    |
|    std                   | 0.128      |
|    value_loss            | 0.579      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.571      |
|    end_effectors_err     | 1.77       |
|    height_err            | -0.0306    |
|    joints_err            | 0.436      |
|    joints_vel_err        | 188        |
|    root_ori_err          | 0.0375     |
| reward_terms/            |            |
|    com_reward            | 0.0131     |
|    end_effectors_reward  | 0.0219     |
|    height_reward         | 0.0906     |
|    joints_reward         | 0.232      |
|    joints_vel_reward     | 9.23e-07   |
|    root_ori_reward       | 0.00709    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 276        |
|    ep_rew_mean           | 102        |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.00206    |
|    fps                   | 153        |
|    iterations            | 16         |
|    time_elapsed          | 8546       |
|    total_timesteps       | 1310720    |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.17697407 |
|    clip_fraction         | 0.658      |
|    clip_range            | 0.2        |
|    entropy_loss          | 28.3       |
|    explained_variance    | 0.576      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.369      |
|    n_updates             | 150        |
|    policy_gradient_loss  | -0.068     |
|    std                   | 0.127      |
|    value_loss            | 0.566      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.668      |
|    end_effectors_err     | 2.43       |
|    height_err            | -0.0415    |
|    joints_err            | 0.401      |
|    joints_vel_err        | 202        |
|    root_ori_err          | 0.0834     |
| reward_terms/            |            |
|    com_reward            | 0.0203     |
|    end_effectors_reward  | 0.0196     |
|    height_reward         | 0.085      |
|    joints_reward         | 0.241      |
|    joints_vel_reward     | 1.34e-07   |
|    root_ori_reward       | 0.00302    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 281        |
|    ep_rew_mean           | 107        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 160        |
|    iterations            | 17         |
|    time_elapsed          | 8691       |
|    total_timesteps       | 1392640    |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.19101758 |
|    clip_fraction         | 0.657      |
|    clip_range            | 0.2        |
|    entropy_loss          | 28.5       |
|    explained_variance    | 0.55       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.0887     |
|    n_updates             | 160        |
|    policy_gradient_loss  | -0.0662    |
|    std                   | 0.127      |
|    value_loss            | 0.596      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.432      |
|    end_effectors_err     | 1.04       |
|    height_err            | -0.0366    |
|    joints_err            | 0.382      |
|    joints_vel_err        | 194        |
|    root_ori_err          | 0.0316     |
| reward_terms/            |            |
|    com_reward            | 0.0287     |
|    end_effectors_reward  | 0.0296     |
|    height_reward         | 0.0889     |
|    joints_reward         | 0.249      |
|    joints_vel_reward     | 5.75e-07   |
|    root_ori_reward       | 0.00556    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 298        |
|    ep_rew_mean           | 115        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 166        |
|    iterations            | 18         |
|    time_elapsed          | 8836       |
|    total_timesteps       | 1474560    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.17771736 |
|    clip_fraction         | 0.658      |
|    clip_range            | 0.2        |
|    entropy_loss          | 28.7       |
|    explained_variance    | 0.523      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.276      |
|    n_updates             | 170        |
|    policy_gradient_loss  | -0.0666    |
|    std                   | 0.126      |
|    value_loss            | 0.652      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.666      |
|    end_effectors_err     | 2.47       |
|    height_err            | -0.0344    |
|    joints_err            | 0.373      |
|    joints_vel_err        | 179        |
|    root_ori_err          | 0.0418     |
| reward_terms/            |            |
|    com_reward            | 0.0189     |
|    end_effectors_reward  | 0.0207     |
|    height_reward         | 0.0901     |
|    joints_reward         | 0.249      |
|    joints_vel_reward     | 1.15e-06   |
|    root_ori_reward       | 0.0111     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 247        |
|    ep_rew_mean           | 97.4       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 173        |
|    iterations            | 19         |
|    time_elapsed          | 8981       |
|    total_timesteps       | 1556480    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.18597277 |
|    clip_fraction         | 0.666      |
|    clip_range            | 0.2        |
|    entropy_loss          | 28.9       |
|    explained_variance    | 0.561      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.208      |
|    n_updates             | 180        |
|    policy_gradient_loss  | -0.0664    |
|    std                   | 0.125      |
|    value_loss            | 0.606      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.138      |
|    end_effectors_err     | 0.205      |
|    height_err            | -0.00641   |
|    joints_err            | 0.318      |
|    joints_vel_err        | 173        |
|    root_ori_err          | 0.0416     |
| reward_terms/            |            |
|    com_reward            | 0.0744     |
|    end_effectors_reward  | 0.0443     |
|    height_reward         | 0.0992     |
|    joints_reward         | 0.278      |
|    joints_vel_reward     | 8.11e-07   |
|    root_ori_reward       | 0.0127     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 240        |
|    ep_rew_mean           | 95.2       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00206    |
|    fps                   | 179        |
|    iterations            | 20         |
|    time_elapsed          | 9127       |
|    total_timesteps       | 1638400    |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.18444869 |
|    clip_fraction         | 0.662      |
|    clip_range            | 0.2        |
|    entropy_loss          | 29.1       |
|    explained_variance    | 0.511      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.173      |
|    n_updates             | 190        |
|    policy_gradient_loss  | -0.0654    |
|    std                   | 0.125      |
|    value_loss            | 0.709      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.08       |
|    end_effectors_err     | 0.0293     |
|    height_err            | 0.0401     |
|    joints_err            | 0.284      |
|    joints_vel_err        | 181        |
|    root_ori_err          | 0.00932    |
| reward_terms/            |            |
|    com_reward            | 0.091      |
|    end_effectors_reward  | 0.0491     |
|    height_reward         | 0.0909     |
|    joints_reward         | 0.302      |
|    joints_vel_reward     | 1.43e-07   |
|    root_ori_reward       | 0.0251     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 281        |
|    ep_rew_mean           | 112        |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 185        |
|    iterations            | 21         |
|    time_elapsed          | 9271       |
|    total_timesteps       | 1720320    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.19253911 |
|    clip_fraction         | 0.672      |
|    clip_range            | 0.2        |
|    entropy_loss          | 29.4       |
|    explained_variance    | 0.531      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.224      |
|    n_updates             | 200        |
|    policy_gradient_loss  | -0.0659    |
|    std                   | 0.124      |
|    value_loss            | 0.624      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.956      |
|    end_effectors_err     | 4.88       |
|    height_err            | -0.0172    |
|    joints_err            | 0.371      |
|    joints_vel_err        | 184        |
|    root_ori_err          | 0.0535     |
| reward_terms/            |            |
|    com_reward            | 0.00895    |
|    end_effectors_reward  | 0.0131     |
|    height_reward         | 0.0908     |
|    joints_reward         | 0.252      |
|    joints_vel_reward     | 7.52e-07   |
|    root_ori_reward       | 0.0069     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 302        |
|    ep_rew_mean           | 121        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 191        |
|    iterations            | 22         |
|    time_elapsed          | 9416       |
|    total_timesteps       | 1802240    |
|    train_time            | 31.2       |
| train/                   |            |
|    approx_kl             | 0.19126971 |
|    clip_fraction         | 0.67       |
|    clip_range            | 0.2        |
|    entropy_loss          | 29.6       |
|    explained_variance    | 0.543      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.275      |
|    n_updates             | 210        |
|    policy_gradient_loss  | -0.0645    |
|    std                   | 0.123      |
|    value_loss            | 0.694      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.706      |
|    end_effectors_err     | 2.69       |
|    height_err            | -0.033     |
|    joints_err            | 0.347      |
|    joints_vel_err        | 182        |
|    root_ori_err          | 0.0844     |
| reward_terms/            |            |
|    com_reward            | 0.0179     |
|    end_effectors_reward  | 0.0197     |
|    height_reward         | 0.0903     |
|    joints_reward         | 0.265      |
|    joints_vel_reward     | 9.14e-07   |
|    root_ori_reward       | 0.00827    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 286        |
|    ep_rew_mean           | 117        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 197        |
|    iterations            | 23         |
|    time_elapsed          | 9561       |
|    total_timesteps       | 1884160    |
|    train_time            | 31.9       |
| train/                   |            |
|    approx_kl             | 0.19953904 |
|    clip_fraction         | 0.675      |
|    clip_range            | 0.2        |
|    entropy_loss          | 29.8       |
|    explained_variance    | 0.554      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.133      |
|    n_updates             | 220        |
|    policy_gradient_loss  | -0.0658    |
|    std                   | 0.123      |
|    value_loss            | 0.635      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.696      |
|    end_effectors_err     | 3.39       |
|    height_err            | -0.0281    |
|    joints_err            | 0.326      |
|    joints_vel_err        | 189        |
|    root_ori_err          | 0.138      |
| reward_terms/            |            |
|    com_reward            | 0.0198     |
|    end_effectors_reward  | 0.0144     |
|    height_reward         | 0.0913     |
|    joints_reward         | 0.273      |
|    joints_vel_reward     | 1.77e-06   |
|    root_ori_reward       | 0.000347   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 292        |
|    ep_rew_mean           | 120        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 202        |
|    iterations            | 24         |
|    time_elapsed          | 9707       |
|    total_timesteps       | 1966080    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.20294575 |
|    clip_fraction         | 0.676      |
|    clip_range            | 0.2        |
|    entropy_loss          | 30.1       |
|    explained_variance    | 0.524      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.28       |
|    n_updates             | 230        |
|    policy_gradient_loss  | -0.0638    |
|    std                   | 0.122      |
|    value_loss            | 0.711      |
-----------------------------------------
policy converted successfully and saved.
Eval num_timesteps=2000000, episode_reward=40.25 +/- 0.00
Episode length: 61.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.395      |
|    end_effectors_err     | 0.944      |
|    height_err            | -0.0165    |
|    joints_err            | 0.333      |
|    joints_vel_err        | 178        |
|    root_ori_err          | 0.022      |
| eval/                    |            |
|    mean_ep_length        | 61         |
|    mean_reward           | 40.3       |
|    success_rate          | 0          |
| reward_terms/            |            |
|    com_reward            | 0.032      |
|    end_effectors_reward  | 0.0314     |
|    height_reward         | 0.0941     |
|    joints_reward         | 0.27       |
|    joints_vel_reward     | 1.14e-06   |
|    root_ori_reward       | 0.00535    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| time/                    |            |
|    evaluate_actions_time | 0.00204    |
|    total_timesteps       | 2000000    |
|    train_time            | 31.9       |
| train/                   |            |
|    approx_kl             | 0.20893566 |
|    clip_fraction         | 0.684      |
|    clip_range            | 0.2        |
|    entropy_loss          | 30.3       |
|    explained_variance    | 0.579      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.195      |
|    n_updates             | 240        |
|    policy_gradient_loss  | -0.0643    |
|    std                   | 0.122      |
|    value_loss            | 0.593      |
-----------------------------------------
--------------------------------------
| err_terms/              |          |
|    com_err              | 0.853    |
|    end_effectors_err    | 3.86     |
|    height_err           | -0.0132  |
|    joints_err           | 0.332    |
|    joints_vel_err       | 190      |
|    root_ori_err         | 0.0759   |
| reward_terms/           |          |
|    com_reward           | 0.0137   |
|    end_effectors_reward | 0.0147   |
|    height_reward        | 0.0916   |
|    joints_reward        | 0.271    |
|    joints_vel_reward    | 6.75e-07 |
|    root_ori_reward      | 0.004    |
|    smoothness1_reward   | 0        |
|    smoothness2_reward   | 0        |
|    smoothness_reward    | 0        |
| rollout/                |          |
|    ep_len_mean          | 324      |
|    ep_rew_mean          | 133      |
| time/                   |          |
|    collect_time         | 2.03e+03 |
|    fps                  | 174      |
|    iterations           | 25       |
|    time_elapsed         | 11765    |
|    total_timesteps      | 2048000  |
--------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.599      |
|    end_effectors_err     | 2.16       |
|    height_err            | -0.0192    |
|    joints_err            | 0.329      |
|    joints_vel_err        | 178        |
|    root_ori_err          | 0.0386     |
| reward_terms/            |            |
|    com_reward            | 0.0224     |
|    end_effectors_reward  | 0.0223     |
|    height_reward         | 0.0938     |
|    joints_reward         | 0.272      |
|    joints_vel_reward     | 8.65e-07   |
|    root_ori_reward       | 0.00667    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 327        |
|    ep_rew_mean           | 137        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 178        |
|    iterations            | 26         |
|    time_elapsed          | 11910      |
|    total_timesteps       | 2129920    |
|    train_time            | 31.5       |
| train/                   |            |
|    approx_kl             | 0.21247582 |
|    clip_fraction         | 0.687      |
|    clip_range            | 0.2        |
|    entropy_loss          | 30.5       |
|    explained_variance    | 0.535      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.119      |
|    n_updates             | 250        |
|    policy_gradient_loss  | -0.0661    |
|    std                   | 0.121      |
|    value_loss            | 0.582      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.04       |
|    end_effectors_err     | 6.56       |
|    height_err            | -0.0286    |
|    joints_err            | 0.316      |
|    joints_vel_err        | 181        |
|    root_ori_err          | 0.215      |
| reward_terms/            |            |
|    com_reward            | 0.00772    |
|    end_effectors_reward  | 0.0112     |
|    height_reward         | 0.0898     |
|    joints_reward         | 0.279      |
|    joints_vel_reward     | 1.1e-06    |
|    root_ori_reward       | 0.00306    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 298        |
|    ep_rew_mean           | 126        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 183        |
|    iterations            | 27         |
|    time_elapsed          | 12056      |
|    total_timesteps       | 2211840    |
|    train_time            | 31.5       |
| train/                   |            |
|    approx_kl             | 0.21761914 |
|    clip_fraction         | 0.689      |
|    clip_range            | 0.2        |
|    entropy_loss          | 30.7       |
|    explained_variance    | 0.544      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.168      |
|    n_updates             | 260        |
|    policy_gradient_loss  | -0.0649    |
|    std                   | 0.12       |
|    value_loss            | 0.617      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.682      |
|    end_effectors_err     | 2.43       |
|    height_err            | -0.0231    |
|    joints_err            | 0.301      |
|    joints_vel_err        | 187        |
|    root_ori_err          | 0.0526     |
| reward_terms/            |            |
|    com_reward            | 0.0169     |
|    end_effectors_reward  | 0.019      |
|    height_reward         | 0.0925     |
|    joints_reward         | 0.288      |
|    joints_vel_reward     | 7.06e-07   |
|    root_ori_reward       | 0.00844    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 302        |
|    ep_rew_mean           | 129        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 187        |
|    iterations            | 28         |
|    time_elapsed          | 12201      |
|    total_timesteps       | 2293760    |
|    train_time            | 32         |
| train/                   |            |
|    approx_kl             | 0.29557818 |
|    clip_fraction         | 0.69       |
|    clip_range            | 0.2        |
|    entropy_loss          | 31         |
|    explained_variance    | 0.527      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.352      |
|    n_updates             | 270        |
|    policy_gradient_loss  | -0.0637    |
|    std                   | 0.12       |
|    value_loss            | 0.668      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.152      |
|    end_effectors_err     | 0.155      |
|    height_err            | -0.0254    |
|    joints_err            | 0.268      |
|    joints_vel_err        | 175        |
|    root_ori_err          | 0.057      |
| reward_terms/            |            |
|    com_reward            | 0.0724     |
|    end_effectors_reward  | 0.0455     |
|    height_reward         | 0.0866     |
|    joints_reward         | 0.306      |
|    joints_vel_reward     | 6.3e-07    |
|    root_ori_reward       | 0.00946    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 311        |
|    ep_rew_mean           | 133        |
| time/                    |            |
|    collect_time          | 111        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 192        |
|    iterations            | 29         |
|    time_elapsed          | 12344      |
|    total_timesteps       | 2375680    |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.22234914 |
|    clip_fraction         | 0.691      |
|    clip_range            | 0.2        |
|    entropy_loss          | 31.2       |
|    explained_variance    | 0.539      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.45       |
|    n_updates             | 280        |
|    policy_gradient_loss  | -0.0625    |
|    std                   | 0.119      |
|    value_loss            | 0.691      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.425      |
|    end_effectors_err     | 1          |
|    height_err            | 7.38e-05   |
|    joints_err            | 0.296      |
|    joints_vel_err        | 171        |
|    root_ori_err          | 0.0113     |
| reward_terms/            |            |
|    com_reward            | 0.0253     |
|    end_effectors_reward  | 0.0295     |
|    height_reward         | 0.0977     |
|    joints_reward         | 0.29       |
|    joints_vel_reward     | 8.52e-07   |
|    root_ori_reward       | 0.00826    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 335        |
|    ep_rew_mean           | 144        |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 196        |
|    iterations            | 30         |
|    time_elapsed          | 12488      |
|    total_timesteps       | 2457600    |
|    train_time            | 31.3       |
| train/                   |            |
|    approx_kl             | 0.23300521 |
|    clip_fraction         | 0.698      |
|    clip_range            | 0.2        |
|    entropy_loss          | 31.4       |
|    explained_variance    | 0.531      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.302      |
|    n_updates             | 290        |
|    policy_gradient_loss  | -0.0635    |
|    std                   | 0.119      |
|    value_loss            | 0.58       |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.395      |
|    end_effectors_err     | 0.827      |
|    height_err            | -0.0126    |
|    joints_err            | 0.28       |
|    joints_vel_err        | 175        |
|    root_ori_err          | 0.0194     |
| reward_terms/            |            |
|    com_reward            | 0.0276     |
|    end_effectors_reward  | 0.032      |
|    height_reward         | 0.0939     |
|    joints_reward         | 0.297      |
|    joints_vel_reward     | 1.35e-06   |
|    root_ori_reward       | 0.0196     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 289        |
|    ep_rew_mean           | 126        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 201        |
|    iterations            | 31         |
|    time_elapsed          | 12633      |
|    total_timesteps       | 2539520    |
|    train_time            | 31.9       |
| train/                   |            |
|    approx_kl             | 0.23546354 |
|    clip_fraction         | 0.699      |
|    clip_range            | 0.2        |
|    entropy_loss          | 31.6       |
|    explained_variance    | 0.483      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.275      |
|    n_updates             | 300        |
|    policy_gradient_loss  | -0.0636    |
|    std                   | 0.118      |
|    value_loss            | 0.624      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.623     |
|    end_effectors_err     | 2.29      |
|    height_err            | -0.00382  |
|    joints_err            | 0.282     |
|    joints_vel_err        | 173       |
|    root_ori_err          | 0.0391    |
| reward_terms/            |           |
|    com_reward            | 0.0217    |
|    end_effectors_reward  | 0.0194    |
|    height_reward         | 0.0957    |
|    joints_reward         | 0.295     |
|    joints_vel_reward     | 1.75e-06  |
|    root_ori_reward       | 0.00604   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 337       |
|    ep_rew_mean           | 147       |
| time/                    |           |
|    collect_time          | 112       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 205       |
|    iterations            | 32        |
|    time_elapsed          | 12777     |
|    total_timesteps       | 2621440   |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 0.3514418 |
|    clip_fraction         | 0.703     |
|    clip_range            | 0.2       |
|    entropy_loss          | 31.8      |
|    explained_variance    | 0.491     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.381     |
|    n_updates             | 310       |
|    policy_gradient_loss  | -0.0637   |
|    std                   | 0.118     |
|    value_loss            | 0.607     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.203     |
|    end_effectors_err     | 0.9       |
|    height_err            | 0.00024   |
|    joints_err            | 0.297     |
|    joints_vel_err        | 193       |
|    root_ori_err          | 0.0959    |
| reward_terms/            |           |
|    com_reward            | 0.0615    |
|    end_effectors_reward  | 0.0304    |
|    height_reward         | 0.0942    |
|    joints_reward         | 0.289     |
|    joints_vel_reward     | 4.54e-07  |
|    root_ori_reward       | 0.00134   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 315       |
|    ep_rew_mean           | 139       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 209       |
|    iterations            | 33        |
|    time_elapsed          | 12923     |
|    total_timesteps       | 2703360   |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 0.2536931 |
|    clip_fraction         | 0.709     |
|    clip_range            | 0.2       |
|    entropy_loss          | 32        |
|    explained_variance    | 0.526     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.177     |
|    n_updates             | 320       |
|    policy_gradient_loss  | -0.0657   |
|    std                   | 0.117     |
|    value_loss            | 0.54      |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.449      |
|    end_effectors_err     | 1.03       |
|    height_err            | -0.0167    |
|    joints_err            | 0.258      |
|    joints_vel_err        | 167        |
|    root_ori_err          | 0.0111     |
| reward_terms/            |            |
|    com_reward            | 0.0229     |
|    end_effectors_reward  | 0.0294     |
|    height_reward         | 0.0956     |
|    joints_reward         | 0.308      |
|    joints_vel_reward     | 9.43e-07   |
|    root_ori_reward       | 0.0111     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 318        |
|    ep_rew_mean           | 142        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 213        |
|    iterations            | 34         |
|    time_elapsed          | 13069      |
|    total_timesteps       | 2785280    |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.25219387 |
|    clip_fraction         | 0.707      |
|    clip_range            | 0.2        |
|    entropy_loss          | 32.2       |
|    explained_variance    | 0.533      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.429      |
|    n_updates             | 330        |
|    policy_gradient_loss  | -0.0639    |
|    std                   | 0.117      |
|    value_loss            | 0.577      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.61       |
|    end_effectors_err     | 2.25       |
|    height_err            | -0.0183    |
|    joints_err            | 0.274      |
|    joints_vel_err        | 165        |
|    root_ori_err          | 0.0834     |
| reward_terms/            |            |
|    com_reward            | 0.0233     |
|    end_effectors_reward  | 0.0213     |
|    height_reward         | 0.0947     |
|    joints_reward         | 0.3        |
|    joints_vel_reward     | 2.77e-06   |
|    root_ori_reward       | 0.00615    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 316        |
|    ep_rew_mean           | 142        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 216        |
|    iterations            | 35         |
|    time_elapsed          | 13214      |
|    total_timesteps       | 2867200    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.25727126 |
|    clip_fraction         | 0.707      |
|    clip_range            | 0.2        |
|    entropy_loss          | 32.5       |
|    explained_variance    | 0.554      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.0208     |
|    n_updates             | 340        |
|    policy_gradient_loss  | -0.0619    |
|    std                   | 0.116      |
|    value_loss            | 0.616      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.107      |
|    end_effectors_err     | 0.0916     |
|    height_err            | 0.0303     |
|    joints_err            | 0.214      |
|    joints_vel_err        | 156        |
|    root_ori_err          | 0.0148     |
| reward_terms/            |            |
|    com_reward            | 0.0829     |
|    end_effectors_reward  | 0.0473     |
|    height_reward         | 0.0946     |
|    joints_reward         | 0.334      |
|    joints_vel_reward     | 7.9e-07    |
|    root_ori_reward       | 0.0172     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 292        |
|    ep_rew_mean           | 132        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 220        |
|    iterations            | 36         |
|    time_elapsed          | 13360      |
|    total_timesteps       | 2949120    |
|    train_time            | 31.5       |
| train/                   |            |
|    approx_kl             | 0.26100695 |
|    clip_fraction         | 0.712      |
|    clip_range            | 0.2        |
|    entropy_loss          | 32.7       |
|    explained_variance    | 0.533      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.233      |
|    n_updates             | 350        |
|    policy_gradient_loss  | -0.0614    |
|    std                   | 0.115      |
|    value_loss            | 0.639      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.187     |
|    end_effectors_err     | 0.227     |
|    height_err            | -0.0202   |
|    joints_err            | 0.212     |
|    joints_vel_err        | 165       |
|    root_ori_err          | 0.0708    |
| reward_terms/            |           |
|    com_reward            | 0.0639    |
|    end_effectors_reward  | 0.0436    |
|    height_reward         | 0.0956    |
|    joints_reward         | 0.337     |
|    joints_vel_reward     | 2.08e-06  |
|    root_ori_reward       | 0.00951   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 331       |
|    ep_rew_mean           | 149       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00205   |
|    fps                   | 224       |
|    iterations            | 37        |
|    time_elapsed          | 13507     |
|    total_timesteps       | 3031040   |
|    train_time            | 32.1      |
| train/                   |           |
|    approx_kl             | 0.2591118 |
|    clip_fraction         | 0.709     |
|    clip_range            | 0.2       |
|    entropy_loss          | 32.9      |
|    explained_variance    | 0.547     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.172     |
|    n_updates             | 360       |
|    policy_gradient_loss  | -0.0619   |
|    std                   | 0.115     |
|    value_loss            | 0.677     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.06       |
|    end_effectors_err     | 8.4        |
|    height_err            | -0.0312    |
|    joints_err            | 0.27       |
|    joints_vel_err        | 177        |
|    root_ori_err          | 0.378      |
| reward_terms/            |            |
|    com_reward            | 0.00724    |
|    end_effectors_reward  | 0.0028     |
|    height_reward         | 0.09       |
|    joints_reward         | 0.302      |
|    joints_vel_reward     | 1.06e-06   |
|    root_ori_reward       | 0.000111   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 347        |
|    ep_rew_mean           | 157        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 228        |
|    iterations            | 38         |
|    time_elapsed          | 13651      |
|    total_timesteps       | 3112960    |
|    train_time            | 31.9       |
| train/                   |            |
|    approx_kl             | 0.27267128 |
|    clip_fraction         | 0.711      |
|    clip_range            | 0.2        |
|    entropy_loss          | 33.1       |
|    explained_variance    | 0.572      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.708      |
|    n_updates             | 370        |
|    policy_gradient_loss  | -0.0629    |
|    std                   | 0.114      |
|    value_loss            | 0.609      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.164      |
|    end_effectors_err     | 0.275      |
|    height_err            | -0.0209    |
|    joints_err            | 0.182      |
|    joints_vel_err        | 152        |
|    root_ori_err          | 0.0127     |
| reward_terms/            |            |
|    com_reward            | 0.0679     |
|    end_effectors_reward  | 0.0425     |
|    height_reward         | 0.0941     |
|    joints_reward         | 0.353      |
|    joints_vel_reward     | 1.66e-06   |
|    root_ori_reward       | 0.0142     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 366        |
|    ep_rew_mean           | 166        |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.00199    |
|    fps                   | 231        |
|    iterations            | 39         |
|    time_elapsed          | 13795      |
|    total_timesteps       | 3194880    |
|    train_time            | 31.3       |
| train/                   |            |
|    approx_kl             | 0.28068763 |
|    clip_fraction         | 0.716      |
|    clip_range            | 0.2        |
|    entropy_loss          | 33.3       |
|    explained_variance    | 0.571      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.25       |
|    n_updates             | 380        |
|    policy_gradient_loss  | -0.0624    |
|    std                   | 0.114      |
|    value_loss            | 0.544      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.851     |
|    end_effectors_err     | 3.73      |
|    height_err            | -0.00582  |
|    joints_err            | 0.251     |
|    joints_vel_err        | 160       |
|    root_ori_err          | 0.0352    |
| reward_terms/            |           |
|    com_reward            | 0.0077    |
|    end_effectors_reward  | 0.0147    |
|    height_reward         | 0.0961    |
|    joints_reward         | 0.312     |
|    joints_vel_reward     | 2.55e-06  |
|    root_ori_reward       | 0.00933   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 298       |
|    ep_rew_mean           | 138       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 235       |
|    iterations            | 40        |
|    time_elapsed          | 13941     |
|    total_timesteps       | 3276800   |
|    train_time            | 31.3      |
| train/                   |           |
|    approx_kl             | 0.2873799 |
|    clip_fraction         | 0.718     |
|    clip_range            | 0.2       |
|    entropy_loss          | 33.6      |
|    explained_variance    | 0.566     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.228     |
|    n_updates             | 390       |
|    policy_gradient_loss  | -0.0633   |
|    std                   | 0.113     |
|    value_loss            | 0.541     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.014      |
|    end_effectors_err     | 0.00252    |
|    height_err            | 0.00423    |
|    joints_err            | 0.0491     |
|    joints_vel_err        | 182        |
|    root_ori_err          | 0.00173    |
| reward_terms/            |            |
|    com_reward            | 0.0997     |
|    end_effectors_reward  | 0.0499     |
|    height_reward         | 0.0999     |
|    joints_reward         | 0.455      |
|    joints_vel_reward     | 4.36e-06   |
|    root_ori_reward       | 0.0507     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 312        |
|    ep_rew_mean           | 145        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 238        |
|    iterations            | 41         |
|    time_elapsed          | 14087      |
|    total_timesteps       | 3358720    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.28160548 |
|    clip_fraction         | 0.717      |
|    clip_range            | 0.2        |
|    entropy_loss          | 33.8       |
|    explained_variance    | 0.539      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.126      |
|    n_updates             | 400        |
|    policy_gradient_loss  | -0.0605    |
|    std                   | 0.113      |
|    value_loss            | 0.625      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.794     |
|    end_effectors_err     | 3.56      |
|    height_err            | -0.0221   |
|    joints_err            | 0.242     |
|    joints_vel_err        | 158       |
|    root_ori_err          | 0.0746    |
| reward_terms/            |           |
|    com_reward            | 0.0114    |
|    end_effectors_reward  | 0.0139    |
|    height_reward         | 0.0932    |
|    joints_reward         | 0.318     |
|    joints_vel_reward     | 4.17e-06  |
|    root_ori_reward       | 0.000923  |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 350       |
|    ep_rew_mean           | 163       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 241       |
|    iterations            | 42        |
|    time_elapsed          | 14232     |
|    total_timesteps       | 3440640   |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 0.2899808 |
|    clip_fraction         | 0.723     |
|    clip_range            | 0.2       |
|    entropy_loss          | 34        |
|    explained_variance    | 0.56      |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.172     |
|    n_updates             | 410       |
|    policy_gradient_loss  | -0.0613   |
|    std                   | 0.112     |
|    value_loss            | 0.603     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.0334     |
|    end_effectors_err     | 0.0284     |
|    height_err            | 0.0192     |
|    joints_err            | 0.175      |
|    joints_vel_err        | 131        |
|    root_ori_err          | 0.00338    |
| reward_terms/            |            |
|    com_reward            | 0.0979     |
|    end_effectors_reward  | 0.0491     |
|    height_reward         | 0.097      |
|    joints_reward         | 0.36       |
|    joints_vel_reward     | 4e-06      |
|    root_ori_reward       | 0.0351     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 341        |
|    ep_rew_mean           | 160        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 244        |
|    iterations            | 43         |
|    time_elapsed          | 14378      |
|    total_timesteps       | 3522560    |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.30242163 |
|    clip_fraction         | 0.726      |
|    clip_range            | 0.2        |
|    entropy_loss          | 34.2       |
|    explained_variance    | 0.533      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.0602     |
|    n_updates             | 420        |
|    policy_gradient_loss  | -0.0652    |
|    std                   | 0.112      |
|    value_loss            | 0.492      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.416      |
|    end_effectors_err     | 1.1        |
|    height_err            | -0.002     |
|    joints_err            | 0.243      |
|    joints_vel_err        | 158        |
|    root_ori_err          | 0.082      |
| reward_terms/            |            |
|    com_reward            | 0.0231     |
|    end_effectors_reward  | 0.0273     |
|    height_reward         | 0.0971     |
|    joints_reward         | 0.321      |
|    joints_vel_reward     | 1.97e-06   |
|    root_ori_reward       | 0.00518    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 311        |
|    ep_rew_mean           | 146        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00205    |
|    fps                   | 248        |
|    iterations            | 44         |
|    time_elapsed          | 14524      |
|    total_timesteps       | 3604480    |
|    train_time            | 31.9       |
| train/                   |            |
|    approx_kl             | 0.30674726 |
|    clip_fraction         | 0.724      |
|    clip_range            | 0.2        |
|    entropy_loss          | 34.4       |
|    explained_variance    | 0.582      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.124      |
|    n_updates             | 430        |
|    policy_gradient_loss  | -0.0631    |
|    std                   | 0.111      |
|    value_loss            | 0.534      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.535      |
|    end_effectors_err     | 1.65       |
|    height_err            | -0.00259   |
|    joints_err            | 0.222      |
|    joints_vel_err        | 152        |
|    root_ori_err          | 0.0312     |
| reward_terms/            |            |
|    com_reward            | 0.0221     |
|    end_effectors_reward  | 0.0242     |
|    height_reward         | 0.0943     |
|    joints_reward         | 0.33       |
|    joints_vel_reward     | 6.26e-06   |
|    root_ori_reward       | 0.00555    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 346        |
|    ep_rew_mean           | 163        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 251        |
|    iterations            | 45         |
|    time_elapsed          | 14668      |
|    total_timesteps       | 3686400    |
|    train_time            | 31.3       |
| train/                   |            |
|    approx_kl             | 0.29726568 |
|    clip_fraction         | 0.72       |
|    clip_range            | 0.2        |
|    entropy_loss          | 34.6       |
|    explained_variance    | 0.499      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.412      |
|    n_updates             | 440        |
|    policy_gradient_loss  | -0.0596    |
|    std                   | 0.111      |
|    value_loss            | 0.702      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.552      |
|    end_effectors_err     | 1.73       |
|    height_err            | -0.0206    |
|    joints_err            | 0.22       |
|    joints_vel_err        | 160        |
|    root_ori_err          | 0.0297     |
| reward_terms/            |            |
|    com_reward            | 0.0245     |
|    end_effectors_reward  | 0.0241     |
|    height_reward         | 0.0951     |
|    joints_reward         | 0.33       |
|    joints_vel_reward     | 5.36e-06   |
|    root_ori_reward       | 0.0126     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 335        |
|    ep_rew_mean           | 158        |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 254        |
|    iterations            | 46         |
|    time_elapsed          | 14812      |
|    total_timesteps       | 3768320    |
|    train_time            | 31.3       |
| train/                   |            |
|    approx_kl             | 0.32423487 |
|    clip_fraction         | 0.728      |
|    clip_range            | 0.2        |
|    entropy_loss          | 34.9       |
|    explained_variance    | 0.528      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.26       |
|    n_updates             | 450        |
|    policy_gradient_loss  | -0.0618    |
|    std                   | 0.11       |
|    value_loss            | 0.565      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.599     |
|    end_effectors_err     | 2.08      |
|    height_err            | -0.0216   |
|    joints_err            | 0.231     |
|    joints_vel_err        | 156       |
|    root_ori_err          | 0.0479    |
| reward_terms/            |           |
|    com_reward            | 0.0202    |
|    end_effectors_reward  | 0.0221    |
|    height_reward         | 0.0929    |
|    joints_reward         | 0.324     |
|    joints_vel_reward     | 5.59e-06  |
|    root_ori_reward       | 0.00571   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 352       |
|    ep_rew_mean           | 168       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 257       |
|    iterations            | 47        |
|    time_elapsed          | 14958     |
|    total_timesteps       | 3850240   |
|    train_time            | 31.6      |
| train/                   |           |
|    approx_kl             | 0.3179182 |
|    clip_fraction         | 0.729     |
|    clip_range            | 0.2       |
|    entropy_loss          | 35.1      |
|    explained_variance    | 0.515     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.435     |
|    n_updates             | 460       |
|    policy_gradient_loss  | -0.0608   |
|    std                   | 0.109     |
|    value_loss            | 0.58      |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.668      |
|    end_effectors_err     | 3.01       |
|    height_err            | 0.00475    |
|    joints_err            | 0.233      |
|    joints_vel_err        | 152        |
|    root_ori_err          | 0.0881     |
| reward_terms/            |            |
|    com_reward            | 0.022      |
|    end_effectors_reward  | 0.0172     |
|    height_reward         | 0.0962     |
|    joints_reward         | 0.323      |
|    joints_vel_reward     | 9.3e-06    |
|    root_ori_reward       | 0.00324    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 378        |
|    ep_rew_mean           | 178        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 260        |
|    iterations            | 48         |
|    time_elapsed          | 15104      |
|    total_timesteps       | 3932160    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.31887853 |
|    clip_fraction         | 0.731      |
|    clip_range            | 0.2        |
|    entropy_loss          | 35.3       |
|    explained_variance    | 0.571      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.197      |
|    n_updates             | 470        |
|    policy_gradient_loss  | -0.0601    |
|    std                   | 0.109      |
|    value_loss            | 0.572      |
-----------------------------------------
policy converted successfully and saved.
Saving video to /cluster/home/anghosh/DHProject/deep-mimic/log/2023-05-29-22-14-49-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M-residual_1/eval-video-PylocoVanilla-v0-step-2940-to-step-3940.mp4
Eval num_timesteps=4000000, episode_reward=283.25 +/- 0.00
Episode length: 527.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.535      |
|    end_effectors_err     | 1.73       |
|    height_err            | -0.0198    |
|    joints_err            | 0.226      |
|    joints_vel_err        | 154        |
|    root_ori_err          | 0.0383     |
| eval/                    |            |
|    mean_ep_length        | 527        |
|    mean_reward           | 283        |
|    success_rate          | 1          |
| reward_terms/            |            |
|    com_reward            | 0.0209     |
|    end_effectors_reward  | 0.0238     |
|    height_reward         | 0.0943     |
|    joints_reward         | 0.328      |
|    joints_vel_reward     | 4.56e-06   |
|    root_ori_reward       | 0.009      |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| time/                    |            |
|    evaluate_actions_time | 0.00204    |
|    total_timesteps       | 4000000    |
|    train_time            | 32.2       |
| train/                   |            |
|    approx_kl             | 0.33833134 |
|    clip_fraction         | 0.737      |
|    clip_range            | 0.2        |
|    entropy_loss          | 35.6       |
|    explained_variance    | 0.622      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.0222     |
|    n_updates             | 480        |
|    policy_gradient_loss  | -0.0664    |
|    std                   | 0.108      |
|    value_loss            | 0.438      |
-----------------------------------------
New best mean reward!
--------------------------------------
| err_terms/              |          |
|    com_err              | 0.258    |
|    end_effectors_err    | 0.428    |
|    height_err           | -0.0177  |
|    joints_err           | 0.239    |
|    joints_vel_err       | 149      |
|    root_ori_err         | 0.0303   |
| reward_terms/           |          |
|    com_reward           | 0.0504   |
|    end_effectors_reward | 0.0393   |
|    height_reward        | 0.0968   |
|    joints_reward        | 0.32     |
|    joints_vel_reward    | 2.28e-06 |
|    root_ori_reward      | 0.0126   |
|    smoothness1_reward   | 0        |
|    smoothness2_reward   | 0        |
|    smoothness_reward    | 0        |
| rollout/                |          |
|    ep_len_mean          | 375      |
|    ep_rew_mean          | 178      |
| time/                   |          |
|    collect_time         | 6.39e+03 |
|    fps                  | 186      |
|    iterations           | 49       |
|    time_elapsed         | 21528    |
|    total_timesteps      | 4014080  |
--------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 0.648    |
|    end_effectors_err     | 2.24     |
|    height_err            | -0.0178  |
|    joints_err            | 0.224    |
|    joints_vel_err        | 147      |
|    root_ori_err          | 0.0441   |
| reward_terms/            |          |
|    com_reward            | 0.0146   |
|    end_effectors_reward  | 0.0198   |
|    height_reward         | 0.0954   |
|    joints_reward         | 0.33     |
|    joints_vel_reward     | 8.28e-06 |
|    root_ori_reward       | 0.00799  |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 356      |
|    ep_rew_mean           | 172      |
| time/                    |          |
|    collect_time          | 114      |
|    evaluate_actions_time | 0.00201  |
|    fps                   | 188      |
|    iterations            | 50       |
|    time_elapsed          | 21673    |
|    total_timesteps       | 4096000  |
|    train_time            | 31.6     |
| train/                   |          |
|    approx_kl             | 0.327148 |
|    clip_fraction         | 0.733    |
|    clip_range            | 0.2      |
|    entropy_loss          | 35.9     |
|    explained_variance    | 0.592    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | 0.292    |
|    n_updates             | 490      |
|    policy_gradient_loss  | -0.0648  |
|    std                   | 0.108    |
|    value_loss            | 0.452    |
---------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.885      |
|    end_effectors_err     | 4.13       |
|    height_err            | -0.0149    |
|    joints_err            | 0.211      |
|    joints_vel_err        | 152        |
|    root_ori_err          | 0.0396     |
| reward_terms/            |            |
|    com_reward            | 0.0107     |
|    end_effectors_reward  | 0.0141     |
|    height_reward         | 0.0954     |
|    joints_reward         | 0.337      |
|    joints_vel_reward     | 5.27e-06   |
|    root_ori_reward       | 0.0098     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 370        |
|    ep_rew_mean           | 177        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 191        |
|    iterations            | 51         |
|    time_elapsed          | 21818      |
|    total_timesteps       | 4177920    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.33296257 |
|    clip_fraction         | 0.734      |
|    clip_range            | 0.2        |
|    entropy_loss          | 36.1       |
|    explained_variance    | 0.543      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.00613    |
|    n_updates             | 500        |
|    policy_gradient_loss  | -0.0639    |
|    std                   | 0.107      |
|    value_loss            | 0.45       |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.599      |
|    end_effectors_err     | 1.91       |
|    height_err            | -0.0132    |
|    joints_err            | 0.216      |
|    joints_vel_err        | 159        |
|    root_ori_err          | 0.0463     |
| reward_terms/            |            |
|    com_reward            | 0.0169     |
|    end_effectors_reward  | 0.0218     |
|    height_reward         | 0.0945     |
|    joints_reward         | 0.334      |
|    joints_vel_reward     | 7.1e-06    |
|    root_ori_reward       | 0.0149     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 373        |
|    ep_rew_mean           | 179        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 193        |
|    iterations            | 52         |
|    time_elapsed          | 21964      |
|    total_timesteps       | 4259840    |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.34710622 |
|    clip_fraction         | 0.733      |
|    clip_range            | 0.2        |
|    entropy_loss          | 36.3       |
|    explained_variance    | 0.616      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.161      |
|    n_updates             | 510        |
|    policy_gradient_loss  | -0.0644    |
|    std                   | 0.107      |
|    value_loss            | 0.432      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.786      |
|    end_effectors_err     | 5.14       |
|    height_err            | -0.0212    |
|    joints_err            | 0.223      |
|    joints_vel_err        | 145        |
|    root_ori_err          | 0.228      |
| reward_terms/            |            |
|    com_reward            | 0.017      |
|    end_effectors_reward  | 0.0129     |
|    height_reward         | 0.0945     |
|    joints_reward         | 0.33       |
|    joints_vel_reward     | 7.13e-06   |
|    root_ori_reward       | 0.00111    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 315        |
|    ep_rew_mean           | 154        |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00205    |
|    fps                   | 196        |
|    iterations            | 53         |
|    time_elapsed          | 22111      |
|    total_timesteps       | 4341760    |
|    train_time            | 32.5       |
| train/                   |            |
|    approx_kl             | 0.33728892 |
|    clip_fraction         | 0.733      |
|    clip_range            | 0.2        |
|    entropy_loss          | 36.6       |
|    explained_variance    | 0.602      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.09       |
|    n_updates             | 520        |
|    policy_gradient_loss  | -0.0621    |
|    std                   | 0.106      |
|    value_loss            | 0.498      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.55      |
|    end_effectors_err     | 1.97      |
|    height_err            | -0.0211   |
|    joints_err            | 0.207     |
|    joints_vel_err        | 135       |
|    root_ori_err          | 0.0557    |
| reward_terms/            |           |
|    com_reward            | 0.0215    |
|    end_effectors_reward  | 0.0216    |
|    height_reward         | 0.0935    |
|    joints_reward         | 0.34      |
|    joints_vel_reward     | 1.14e-05  |
|    root_ori_reward       | 0.00329   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 364       |
|    ep_rew_mean           | 177       |
| time/                    |           |
|    collect_time          | 113       |
|    evaluate_actions_time | 0.002     |
|    fps                   | 198       |
|    iterations            | 54        |
|    time_elapsed          | 22256     |
|    total_timesteps       | 4423680   |
|    train_time            | 31.4      |
| train/                   |           |
|    approx_kl             | 0.3389078 |
|    clip_fraction         | 0.731     |
|    clip_range            | 0.2       |
|    entropy_loss          | 36.8      |
|    explained_variance    | 0.561     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.000648 |
|    n_updates             | 530       |
|    policy_gradient_loss  | -0.0585   |
|    std                   | 0.106     |
|    value_loss            | 0.602     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.661      |
|    end_effectors_err     | 2.68       |
|    height_err            | -0.0136    |
|    joints_err            | 0.192      |
|    joints_vel_err        | 153        |
|    root_ori_err          | 0.0848     |
| reward_terms/            |            |
|    com_reward            | 0.0174     |
|    end_effectors_reward  | 0.0181     |
|    height_reward         | 0.0961     |
|    joints_reward         | 0.349      |
|    joints_vel_reward     | 6.31e-06   |
|    root_ori_reward       | 0.000901   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 378        |
|    ep_rew_mean           | 185        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00206    |
|    fps                   | 201        |
|    iterations            | 55         |
|    time_elapsed          | 22403      |
|    total_timesteps       | 4505600    |
|    train_time            | 32         |
| train/                   |            |
|    approx_kl             | 0.34544376 |
|    clip_fraction         | 0.741      |
|    clip_range            | 0.2        |
|    entropy_loss          | 37         |
|    explained_variance    | 0.572      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.095      |
|    n_updates             | 540        |
|    policy_gradient_loss  | -0.0594    |
|    std                   | 0.105      |
|    value_loss            | 0.53       |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.0409     |
|    end_effectors_err     | 0.0653     |
|    height_err            | -0.00404   |
|    joints_err            | 0.168      |
|    joints_vel_err        | 121        |
|    root_ori_err          | 0.00475    |
| reward_terms/            |            |
|    com_reward            | 0.0971     |
|    end_effectors_reward  | 0.048      |
|    height_reward         | 0.0997     |
|    joints_reward         | 0.367      |
|    joints_vel_reward     | 1.34e-05   |
|    root_ori_reward       | 0.0227     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 371        |
|    ep_rew_mean           | 182        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 203        |
|    iterations            | 56         |
|    time_elapsed          | 22547      |
|    total_timesteps       | 4587520    |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.35492128 |
|    clip_fraction         | 0.742      |
|    clip_range            | 0.2        |
|    entropy_loss          | 37.3       |
|    explained_variance    | 0.578      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.524      |
|    n_updates             | 550        |
|    policy_gradient_loss  | -0.0629    |
|    std                   | 0.104      |
|    value_loss            | 0.486      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.701     |
|    end_effectors_err     | 2.91      |
|    height_err            | -0.000948 |
|    joints_err            | 0.194     |
|    joints_vel_err        | 140       |
|    root_ori_err          | 0.0442    |
| reward_terms/            |           |
|    com_reward            | 0.02      |
|    end_effectors_reward  | 0.0179    |
|    height_reward         | 0.0969    |
|    joints_reward         | 0.348     |
|    joints_vel_reward     | 1.12e-05  |
|    root_ori_reward       | 0.0115    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 390       |
|    ep_rew_mean           | 192       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 205       |
|    iterations            | 57        |
|    time_elapsed          | 22694     |
|    total_timesteps       | 4669440   |
|    train_time            | 31.5      |
| train/                   |           |
|    approx_kl             | 0.3658175 |
|    clip_fraction         | 0.744     |
|    clip_range            | 0.2       |
|    entropy_loss          | 37.5      |
|    explained_variance    | 0.612     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.151     |
|    n_updates             | 560       |
|    policy_gradient_loss  | -0.0627   |
|    std                   | 0.104     |
|    value_loss            | 0.465     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.397      |
|    end_effectors_err     | 1.07       |
|    height_err            | -0.0214    |
|    joints_err            | 0.2        |
|    joints_vel_err        | 149        |
|    root_ori_err          | 0.076      |
| reward_terms/            |            |
|    com_reward            | 0.0326     |
|    end_effectors_reward  | 0.0291     |
|    height_reward         | 0.094      |
|    joints_reward         | 0.346      |
|    joints_vel_reward     | 8.66e-06   |
|    root_ori_reward       | 0.00305    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 369        |
|    ep_rew_mean           | 184        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 208        |
|    iterations            | 58         |
|    time_elapsed          | 22840      |
|    total_timesteps       | 4751360    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.35967177 |
|    clip_fraction         | 0.744      |
|    clip_range            | 0.2        |
|    entropy_loss          | 37.8       |
|    explained_variance    | 0.6        |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.101      |
|    n_updates             | 570        |
|    policy_gradient_loss  | -0.0644    |
|    std                   | 0.103      |
|    value_loss            | 0.419      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.732      |
|    end_effectors_err     | 2.78       |
|    height_err            | -0.00416   |
|    joints_err            | 0.194      |
|    joints_vel_err        | 138        |
|    root_ori_err          | 0.0189     |
| reward_terms/            |            |
|    com_reward            | 0.011      |
|    end_effectors_reward  | 0.0166     |
|    height_reward         | 0.0972     |
|    joints_reward         | 0.347      |
|    joints_vel_reward     | 7.41e-06   |
|    root_ori_reward       | 0.00811    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 380        |
|    ep_rew_mean           | 188        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 210        |
|    iterations            | 59         |
|    time_elapsed          | 22986      |
|    total_timesteps       | 4833280    |
|    train_time            | 31.2       |
| train/                   |            |
|    approx_kl             | 0.36728325 |
|    clip_fraction         | 0.744      |
|    clip_range            | 0.2        |
|    entropy_loss          | 38         |
|    explained_variance    | 0.577      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.269      |
|    n_updates             | 580        |
|    policy_gradient_loss  | -0.0637    |
|    std                   | 0.103      |
|    value_loss            | 0.44       |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.67       |
|    end_effectors_err     | 2.27       |
|    height_err            | -0.0145    |
|    joints_err            | 0.193      |
|    joints_vel_err        | 143        |
|    root_ori_err          | 0.0174     |
| reward_terms/            |            |
|    com_reward            | 0.0141     |
|    end_effectors_reward  | 0.0193     |
|    height_reward         | 0.0938     |
|    joints_reward         | 0.349      |
|    joints_vel_reward     | 1.06e-05   |
|    root_ori_reward       | 0.0123     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 366        |
|    ep_rew_mean           | 182        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00205    |
|    fps                   | 212        |
|    iterations            | 60         |
|    time_elapsed          | 23131      |
|    total_timesteps       | 4915200    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.38870758 |
|    clip_fraction         | 0.751      |
|    clip_range            | 0.2        |
|    entropy_loss          | 38.2       |
|    explained_variance    | 0.607      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0238    |
|    n_updates             | 590        |
|    policy_gradient_loss  | -0.0688    |
|    std                   | 0.102      |
|    value_loss            | 0.377      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.713      |
|    end_effectors_err     | 3.01       |
|    height_err            | -0.00469   |
|    joints_err            | 0.196      |
|    joints_vel_err        | 146        |
|    root_ori_err          | 0.0911     |
| reward_terms/            |            |
|    com_reward            | 0.0153     |
|    end_effectors_reward  | 0.0153     |
|    height_reward         | 0.0979     |
|    joints_reward         | 0.347      |
|    joints_vel_reward     | 1.53e-05   |
|    root_ori_reward       | 0.00145    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 377        |
|    ep_rew_mean           | 189        |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00199    |
|    fps                   | 214        |
|    iterations            | 61         |
|    time_elapsed          | 23277      |
|    total_timesteps       | 4997120    |
|    train_time            | 31.1       |
| train/                   |            |
|    approx_kl             | 0.39323375 |
|    clip_fraction         | 0.747      |
|    clip_range            | 0.2        |
|    entropy_loss          | 38.5       |
|    explained_variance    | 0.563      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.0876     |
|    n_updates             | 600        |
|    policy_gradient_loss  | -0.0654    |
|    std                   | 0.102      |
|    value_loss            | 0.409      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.615      |
|    end_effectors_err     | 2.14       |
|    height_err            | -0.0168    |
|    joints_err            | 0.173      |
|    joints_vel_err        | 140        |
|    root_ori_err          | 0.0592     |
| reward_terms/            |            |
|    com_reward            | 0.0201     |
|    end_effectors_reward  | 0.0225     |
|    height_reward         | 0.0943     |
|    joints_reward         | 0.362      |
|    joints_vel_reward     | 9.56e-06   |
|    root_ori_reward       | 0.00715    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 375        |
|    ep_rew_mean           | 188        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 216        |
|    iterations            | 62         |
|    time_elapsed          | 23423      |
|    total_timesteps       | 5079040    |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.40177974 |
|    clip_fraction         | 0.75       |
|    clip_range            | 0.2        |
|    entropy_loss          | 38.7       |
|    explained_variance    | 0.571      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.355      |
|    n_updates             | 610        |
|    policy_gradient_loss  | -0.066     |
|    std                   | 0.101      |
|    value_loss            | 0.396      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.633      |
|    end_effectors_err     | 2.25       |
|    height_err            | -0.00898   |
|    joints_err            | 0.183      |
|    joints_vel_err        | 143        |
|    root_ori_err          | 0.0332     |
| reward_terms/            |            |
|    com_reward            | 0.0187     |
|    end_effectors_reward  | 0.0204     |
|    height_reward         | 0.0962     |
|    joints_reward         | 0.354      |
|    joints_vel_reward     | 1.4e-05    |
|    root_ori_reward       | 0.0142     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 368        |
|    ep_rew_mean           | 186        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 218        |
|    iterations            | 63         |
|    time_elapsed          | 23567      |
|    total_timesteps       | 5160960    |
|    train_time            | 31.5       |
| train/                   |            |
|    approx_kl             | 0.39615422 |
|    clip_fraction         | 0.742      |
|    clip_range            | 0.2        |
|    entropy_loss          | 39         |
|    explained_variance    | 0.569      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.0548     |
|    n_updates             | 620        |
|    policy_gradient_loss  | -0.0613    |
|    std                   | 0.101      |
|    value_loss            | 0.448      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.489      |
|    end_effectors_err     | 1.83       |
|    height_err            | 0.00022    |
|    joints_err            | 0.171      |
|    joints_vel_err        | 132        |
|    root_ori_err          | 0.0665     |
| reward_terms/            |            |
|    com_reward            | 0.0312     |
|    end_effectors_reward  | 0.0241     |
|    height_reward         | 0.0972     |
|    joints_reward         | 0.362      |
|    joints_vel_reward     | 1.07e-05   |
|    root_ori_reward       | 0.00663    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 372        |
|    ep_rew_mean           | 189        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 221        |
|    iterations            | 64         |
|    time_elapsed          | 23713      |
|    total_timesteps       | 5242880    |
|    train_time            | 31.3       |
| train/                   |            |
|    approx_kl             | 0.40927348 |
|    clip_fraction         | 0.751      |
|    clip_range            | 0.2        |
|    entropy_loss          | 39.2       |
|    explained_variance    | 0.615      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.055     |
|    n_updates             | 630        |
|    policy_gradient_loss  | -0.063     |
|    std                   | 0.1        |
|    value_loss            | 0.401      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.556      |
|    end_effectors_err     | 1.85       |
|    height_err            | -0.0106    |
|    joints_err            | 0.193      |
|    joints_vel_err        | 135        |
|    root_ori_err          | 0.0132     |
| reward_terms/            |            |
|    com_reward            | 0.0247     |
|    end_effectors_reward  | 0.0235     |
|    height_reward         | 0.095      |
|    joints_reward         | 0.348      |
|    joints_vel_reward     | 8.68e-06   |
|    root_ori_reward       | 0.0185     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 398        |
|    ep_rew_mean           | 202        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 223        |
|    iterations            | 65         |
|    time_elapsed          | 23859      |
|    total_timesteps       | 5324800    |
|    train_time            | 32         |
| train/                   |            |
|    approx_kl             | 0.40757108 |
|    clip_fraction         | 0.747      |
|    clip_range            | 0.2        |
|    entropy_loss          | 39.5       |
|    explained_variance    | 0.6        |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0221    |
|    n_updates             | 640        |
|    policy_gradient_loss  | -0.0629    |
|    std                   | 0.0996     |
|    value_loss            | 0.436      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.499     |
|    end_effectors_err     | 1.47      |
|    height_err            | -0.0141   |
|    joints_err            | 0.169     |
|    joints_vel_err        | 136       |
|    root_ori_err          | 0.0252    |
| reward_terms/            |           |
|    com_reward            | 0.0282    |
|    end_effectors_reward  | 0.0265    |
|    height_reward         | 0.0956    |
|    joints_reward         | 0.364     |
|    joints_vel_reward     | 2.31e-05  |
|    root_ori_reward       | 0.0198    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 401       |
|    ep_rew_mean           | 204       |
| time/                    |           |
|    collect_time          | 113       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 225       |
|    iterations            | 66        |
|    time_elapsed          | 24005     |
|    total_timesteps       | 5406720   |
|    train_time            | 32.1      |
| train/                   |           |
|    approx_kl             | 0.4100333 |
|    clip_fraction         | 0.749     |
|    clip_range            | 0.2       |
|    entropy_loss          | 39.7      |
|    explained_variance    | 0.604     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.183     |
|    n_updates             | 650       |
|    policy_gradient_loss  | -0.0634   |
|    std                   | 0.099     |
|    value_loss            | 0.407     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.283      |
|    end_effectors_err     | 0.807      |
|    height_err            | -0.00844   |
|    joints_err            | 0.176      |
|    joints_vel_err        | 138        |
|    root_ori_err          | 0.0368     |
| reward_terms/            |            |
|    com_reward            | 0.0464     |
|    end_effectors_reward  | 0.032      |
|    height_reward         | 0.0975     |
|    joints_reward         | 0.359      |
|    joints_vel_reward     | 9.96e-06   |
|    root_ori_reward       | 0.00231    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 397        |
|    ep_rew_mean           | 202        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 227        |
|    iterations            | 67         |
|    time_elapsed          | 24150      |
|    total_timesteps       | 5488640    |
|    train_time            | 31.9       |
| train/                   |            |
|    approx_kl             | 0.41222233 |
|    clip_fraction         | 0.75       |
|    clip_range            | 0.2        |
|    entropy_loss          | 40         |
|    explained_variance    | 0.594      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.213      |
|    n_updates             | 660        |
|    policy_gradient_loss  | -0.0607    |
|    std                   | 0.0984     |
|    value_loss            | 0.459      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.721      |
|    end_effectors_err     | 2.77       |
|    height_err            | -0.0162    |
|    joints_err            | 0.173      |
|    joints_vel_err        | 128        |
|    root_ori_err          | 0.0394     |
| reward_terms/            |            |
|    com_reward            | 0.0134     |
|    end_effectors_reward  | 0.0181     |
|    height_reward         | 0.0964     |
|    joints_reward         | 0.362      |
|    joints_vel_reward     | 1.47e-05   |
|    root_ori_reward       | 0.0158     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 394        |
|    ep_rew_mean           | 202        |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 229        |
|    iterations            | 68         |
|    time_elapsed          | 24294      |
|    total_timesteps       | 5570560    |
|    train_time            | 32.2       |
| train/                   |            |
|    approx_kl             | 0.42910194 |
|    clip_fraction         | 0.756      |
|    clip_range            | 0.2        |
|    entropy_loss          | 40.2       |
|    explained_variance    | 0.644      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0302    |
|    n_updates             | 670        |
|    policy_gradient_loss  | -0.0654    |
|    std                   | 0.0979     |
|    value_loss            | 0.366      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.562     |
|    end_effectors_err     | 1.92      |
|    height_err            | -0.0175   |
|    joints_err            | 0.168     |
|    joints_vel_err        | 136       |
|    root_ori_err          | 0.0685    |
| reward_terms/            |           |
|    com_reward            | 0.0216    |
|    end_effectors_reward  | 0.0217    |
|    height_reward         | 0.096     |
|    joints_reward         | 0.365     |
|    joints_vel_reward     | 1.78e-05  |
|    root_ori_reward       | 0.0125    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 397       |
|    ep_rew_mean           | 202       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 231       |
|    iterations            | 69        |
|    time_elapsed          | 24440     |
|    total_timesteps       | 5652480   |
|    train_time            | 31.3      |
| train/                   |           |
|    approx_kl             | 0.4428439 |
|    clip_fraction         | 0.757     |
|    clip_range            | 0.2       |
|    entropy_loss          | 40.5      |
|    explained_variance    | 0.636     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0793    |
|    n_updates             | 680       |
|    policy_gradient_loss  | -0.0652   |
|    std                   | 0.0974    |
|    value_loss            | 0.366     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.718      |
|    end_effectors_err     | 2.84       |
|    height_err            | -0.0135    |
|    joints_err            | 0.165      |
|    joints_vel_err        | 129        |
|    root_ori_err          | 0.0583     |
| reward_terms/            |            |
|    com_reward            | 0.0156     |
|    end_effectors_reward  | 0.0178     |
|    height_reward         | 0.097      |
|    joints_reward         | 0.367      |
|    joints_vel_reward     | 1.74e-05   |
|    root_ori_reward       | 0.00545    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 405        |
|    ep_rew_mean           | 208        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 233        |
|    iterations            | 70         |
|    time_elapsed          | 24586      |
|    total_timesteps       | 5734400    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.46764356 |
|    clip_fraction         | 0.755      |
|    clip_range            | 0.2        |
|    entropy_loss          | 40.7       |
|    explained_variance    | 0.687      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0333    |
|    n_updates             | 690        |
|    policy_gradient_loss  | -0.0679    |
|    std                   | 0.097      |
|    value_loss            | 0.323      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.541      |
|    end_effectors_err     | 1.66       |
|    height_err            | -0.00696   |
|    joints_err            | 0.162      |
|    joints_vel_err        | 127        |
|    root_ori_err          | 0.0422     |
| reward_terms/            |            |
|    com_reward            | 0.0201     |
|    end_effectors_reward  | 0.0236     |
|    height_reward         | 0.0968     |
|    joints_reward         | 0.369      |
|    joints_vel_reward     | 1.21e-05   |
|    root_ori_reward       | 0.0118     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 395        |
|    ep_rew_mean           | 204        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 235        |
|    iterations            | 71         |
|    time_elapsed          | 24733      |
|    total_timesteps       | 5816320    |
|    train_time            | 31.9       |
| train/                   |            |
|    approx_kl             | 0.46032572 |
|    clip_fraction         | 0.759      |
|    clip_range            | 0.2        |
|    entropy_loss          | 40.9       |
|    explained_variance    | 0.654      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.513      |
|    n_updates             | 700        |
|    policy_gradient_loss  | -0.0696    |
|    std                   | 0.0965     |
|    value_loss            | 0.302      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.454      |
|    end_effectors_err     | 1.44       |
|    height_err            | -0.01      |
|    joints_err            | 0.153      |
|    joints_vel_err        | 123        |
|    root_ori_err          | 0.0416     |
| reward_terms/            |            |
|    com_reward            | 0.0328     |
|    end_effectors_reward  | 0.0274     |
|    height_reward         | 0.0962     |
|    joints_reward         | 0.375      |
|    joints_vel_reward     | 2.14e-05   |
|    root_ori_reward       | 0.00813    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 400        |
|    ep_rew_mean           | 207        |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 237        |
|    iterations            | 72         |
|    time_elapsed          | 24877      |
|    total_timesteps       | 5898240    |
|    train_time            | 31.5       |
| train/                   |            |
|    approx_kl             | 0.45791093 |
|    clip_fraction         | 0.757      |
|    clip_range            | 0.2        |
|    entropy_loss          | 41.2       |
|    explained_variance    | 0.654      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.023      |
|    n_updates             | 710        |
|    policy_gradient_loss  | -0.0661    |
|    std                   | 0.096      |
|    value_loss            | 0.322      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.126     |
|    end_effectors_err     | 0.428     |
|    height_err            | 0.0093    |
|    joints_err            | 0.174     |
|    joints_vel_err        | 125       |
|    root_ori_err          | 0.027     |
| reward_terms/            |           |
|    com_reward            | 0.0761    |
|    end_effectors_reward  | 0.039     |
|    height_reward         | 0.0979    |
|    joints_reward         | 0.359     |
|    joints_vel_reward     | 3.5e-05   |
|    root_ori_reward       | 0.0103    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 399       |
|    ep_rew_mean           | 207       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 238       |
|    iterations            | 73        |
|    time_elapsed          | 25022     |
|    total_timesteps       | 5980160   |
|    train_time            | 31.5      |
| train/                   |           |
|    approx_kl             | 0.4639391 |
|    clip_fraction         | 0.758     |
|    clip_range            | 0.2       |
|    entropy_loss          | 41.4      |
|    explained_variance    | 0.629     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.149     |
|    n_updates             | 720       |
|    policy_gradient_loss  | -0.0674   |
|    std                   | 0.0954    |
|    value_loss            | 0.338     |
----------------------------------------
policy converted successfully and saved.
Saving video to /cluster/home/anghosh/DHProject/deep-mimic/log/2023-05-29-22-14-49-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M-residual_1/eval-video-PylocoVanilla-v0-step-5575-to-step-6575.mp4
Eval num_timesteps=6000000, episode_reward=293.40 +/- 0.00
Episode length: 527.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.288     |
|    end_effectors_err     | 0.872     |
|    height_err            | -0.00494  |
|    joints_err            | 0.169     |
|    joints_vel_err        | 126       |
|    root_ori_err          | 0.0643    |
| eval/                    |           |
|    mean_ep_length        | 527       |
|    mean_reward           | 293       |
|    success_rate          | 1         |
| reward_terms/            |           |
|    com_reward            | 0.0423    |
|    end_effectors_reward  | 0.0303    |
|    height_reward         | 0.0982    |
|    joints_reward         | 0.365     |
|    joints_vel_reward     | 2.33e-05  |
|    root_ori_reward       | 0.000834  |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| time/                    |           |
|    evaluate_actions_time | 0.00204   |
|    total_timesteps       | 6000000   |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 0.4680069 |
|    clip_fraction         | 0.758     |
|    clip_range            | 0.2       |
|    entropy_loss          | 41.7      |
|    explained_variance    | 0.614     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.244     |
|    n_updates             | 730       |
|    policy_gradient_loss  | -0.0657   |
|    std                   | 0.0949    |
|    value_loss            | 0.345     |
----------------------------------------
New best mean reward!
--------------------------------------
| err_terms/              |          |
|    com_err              | 0.494    |
|    end_effectors_err    | 1.55     |
|    height_err           | -0.0168  |
|    joints_err           | 0.155    |
|    joints_vel_err       | 121      |
|    root_ori_err         | 0.0739   |
| reward_terms/           |          |
|    com_reward           | 0.0247   |
|    end_effectors_reward | 0.0244   |
|    height_reward        | 0.0957   |
|    joints_reward        | 0.374    |
|    joints_vel_reward    | 1.96e-05 |
|    root_ori_reward      | 0.00427  |
|    smoothness1_reward   | 0        |
|    smoothness2_reward   | 0        |
|    smoothness_reward    | 0        |
| rollout/                |          |
|    ep_len_mean          | 396      |
|    ep_rew_mean          | 206      |
| time/                   |          |
|    collect_time         | 6.4e+03  |
|    fps                  | 192      |
|    iterations           | 74       |
|    time_elapsed         | 31451    |
|    total_timesteps      | 6062080  |
--------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.615      |
|    end_effectors_err     | 2.22       |
|    height_err            | -0.00524   |
|    joints_err            | 0.16       |
|    joints_vel_err        | 120        |
|    root_ori_err          | 0.0281     |
| reward_terms/            |            |
|    com_reward            | 0.0188     |
|    end_effectors_reward  | 0.0195     |
|    height_reward         | 0.0966     |
|    joints_reward         | 0.37       |
|    joints_vel_reward     | 2.61e-05   |
|    root_ori_reward       | 0.0121     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 366        |
|    ep_rew_mean           | 193        |
| time/                    |            |
|    collect_time          | 116        |
|    evaluate_actions_time | 0.00205    |
|    fps                   | 194        |
|    iterations            | 75         |
|    time_elapsed          | 31600      |
|    total_timesteps       | 6144000    |
|    train_time            | 32.4       |
| train/                   |            |
|    approx_kl             | 0.45586103 |
|    clip_fraction         | 0.754      |
|    clip_range            | 0.2        |
|    entropy_loss          | 42         |
|    explained_variance    | 0.603      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.16       |
|    n_updates             | 740        |
|    policy_gradient_loss  | -0.0618    |
|    std                   | 0.0944     |
|    value_loss            | 0.383      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.52      |
|    end_effectors_err     | 1.48      |
|    height_err            | -0.0166   |
|    joints_err            | 0.149     |
|    joints_vel_err        | 129       |
|    root_ori_err          | 0.0309    |
| reward_terms/            |           |
|    com_reward            | 0.0223    |
|    end_effectors_reward  | 0.025     |
|    height_reward         | 0.0959    |
|    joints_reward         | 0.379     |
|    joints_vel_reward     | 1.14e-05  |
|    root_ori_reward       | 0.015     |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 366       |
|    ep_rew_mean           | 192       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 196       |
|    iterations            | 76        |
|    time_elapsed          | 31745     |
|    total_timesteps       | 6225920   |
|    train_time            | 31.5      |
| train/                   |           |
|    approx_kl             | 0.4624545 |
|    clip_fraction         | 0.76      |
|    clip_range            | 0.2       |
|    entropy_loss          | 42.2      |
|    explained_variance    | 0.602     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.216     |
|    n_updates             | 750       |
|    policy_gradient_loss  | -0.0642   |
|    std                   | 0.0939    |
|    value_loss            | 0.388     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.417      |
|    end_effectors_err     | 0.995      |
|    height_err            | -0.0171    |
|    joints_err            | 0.153      |
|    joints_vel_err        | 124        |
|    root_ori_err          | 0.0307     |
| reward_terms/            |            |
|    com_reward            | 0.0267     |
|    end_effectors_reward  | 0.0299     |
|    height_reward         | 0.0961     |
|    joints_reward         | 0.375      |
|    joints_vel_reward     | 2.08e-05   |
|    root_ori_reward       | 0.0102     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 391        |
|    ep_rew_mean           | 205        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 197        |
|    iterations            | 77         |
|    time_elapsed          | 31891      |
|    total_timesteps       | 6307840    |
|    train_time            | 32         |
| train/                   |            |
|    approx_kl             | 0.47300273 |
|    clip_fraction         | 0.758      |
|    clip_range            | 0.2        |
|    entropy_loss          | 42.4       |
|    explained_variance    | 0.555      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.158      |
|    n_updates             | 760        |
|    policy_gradient_loss  | -0.0586    |
|    std                   | 0.0935     |
|    value_loss            | 0.473      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.476      |
|    end_effectors_err     | 1.77       |
|    height_err            | -0.00271   |
|    joints_err            | 0.168      |
|    joints_vel_err        | 121        |
|    root_ori_err          | 0.0661     |
| reward_terms/            |            |
|    com_reward            | 0.0282     |
|    end_effectors_reward  | 0.024      |
|    height_reward         | 0.0966     |
|    joints_reward         | 0.365      |
|    joints_vel_reward     | 2.43e-05   |
|    root_ori_reward       | 0.00156    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 390        |
|    ep_rew_mean           | 205        |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00199    |
|    fps                   | 199        |
|    iterations            | 78         |
|    time_elapsed          | 32037      |
|    total_timesteps       | 6389760    |
|    train_time            | 31.2       |
| train/                   |            |
|    approx_kl             | 0.49181956 |
|    clip_fraction         | 0.766      |
|    clip_range            | 0.2        |
|    entropy_loss          | 42.7       |
|    explained_variance    | 0.593      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.00242    |
|    n_updates             | 770        |
|    policy_gradient_loss  | -0.0638    |
|    std                   | 0.0929     |
|    value_loss            | 0.385      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.546      |
|    end_effectors_err     | 1.96       |
|    height_err            | -0.00745   |
|    joints_err            | 0.158      |
|    joints_vel_err        | 123        |
|    root_ori_err          | 0.0615     |
| reward_terms/            |            |
|    com_reward            | 0.0234     |
|    end_effectors_reward  | 0.0219     |
|    height_reward         | 0.0967     |
|    joints_reward         | 0.372      |
|    joints_vel_reward     | 1.61e-05   |
|    root_ori_reward       | 0.00412    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 393        |
|    ep_rew_mean           | 208        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00206    |
|    fps                   | 201        |
|    iterations            | 79         |
|    time_elapsed          | 32183      |
|    total_timesteps       | 6471680    |
|    train_time            | 32.3       |
| train/                   |            |
|    approx_kl             | 0.51196444 |
|    clip_fraction         | 0.767      |
|    clip_range            | 0.2        |
|    entropy_loss          | 43         |
|    explained_variance    | 0.583      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0185    |
|    n_updates             | 780        |
|    policy_gradient_loss  | -0.068     |
|    std                   | 0.0924     |
|    value_loss            | 0.335      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.738      |
|    end_effectors_err     | 2.95       |
|    height_err            | -0.00739   |
|    joints_err            | 0.152      |
|    joints_vel_err        | 127        |
|    root_ori_err          | 0.036      |
| reward_terms/            |            |
|    com_reward            | 0.0163     |
|    end_effectors_reward  | 0.0179     |
|    height_reward         | 0.0969     |
|    joints_reward         | 0.376      |
|    joints_vel_reward     | 2.22e-05   |
|    root_ori_reward       | 0.0146     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 401        |
|    ep_rew_mean           | 213        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 202        |
|    iterations            | 80         |
|    time_elapsed          | 32328      |
|    total_timesteps       | 6553600    |
|    train_time            | 31.5       |
| train/                   |            |
|    approx_kl             | 0.51387453 |
|    clip_fraction         | 0.762      |
|    clip_range            | 0.2        |
|    entropy_loss          | 43.2       |
|    explained_variance    | 0.61       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.359      |
|    n_updates             | 790        |
|    policy_gradient_loss  | -0.0619    |
|    std                   | 0.092      |
|    value_loss            | 0.378      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.768     |
|    end_effectors_err     | 3.55      |
|    height_err            | -0.00376  |
|    joints_err            | 0.145     |
|    joints_vel_err        | 119       |
|    root_ori_err          | 0.0363    |
| reward_terms/            |           |
|    com_reward            | 0.0183    |
|    end_effectors_reward  | 0.0152    |
|    height_reward         | 0.0976    |
|    joints_reward         | 0.38      |
|    joints_vel_reward     | 2.23e-05  |
|    root_ori_reward       | 0.00199   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 394       |
|    ep_rew_mean           | 210       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 204       |
|    iterations            | 81        |
|    time_elapsed          | 32475     |
|    total_timesteps       | 6635520   |
|    train_time            | 31.4      |
| train/                   |           |
|    approx_kl             | 0.5056397 |
|    clip_fraction         | 0.762     |
|    clip_range            | 0.2       |
|    entropy_loss          | 43.4      |
|    explained_variance    | 0.619     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.232     |
|    n_updates             | 800       |
|    policy_gradient_loss  | -0.063    |
|    std                   | 0.0916    |
|    value_loss            | 0.357     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.538      |
|    end_effectors_err     | 2          |
|    height_err            | -0.0075    |
|    joints_err            | 0.151      |
|    joints_vel_err        | 133        |
|    root_ori_err          | 0.114      |
| reward_terms/            |            |
|    com_reward            | 0.0225     |
|    end_effectors_reward  | 0.0218     |
|    height_reward         | 0.0971     |
|    joints_reward         | 0.377      |
|    joints_vel_reward     | 2.53e-05   |
|    root_ori_reward       | 0.00408    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 400        |
|    ep_rew_mean           | 212        |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 205        |
|    iterations            | 82         |
|    time_elapsed          | 32619      |
|    total_timesteps       | 6717440    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.54748476 |
|    clip_fraction         | 0.768      |
|    clip_range            | 0.2        |
|    entropy_loss          | 43.6       |
|    explained_variance    | 0.683      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0406    |
|    n_updates             | 810        |
|    policy_gradient_loss  | -0.0713    |
|    std                   | 0.0911     |
|    value_loss            | 0.266      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.526     |
|    end_effectors_err     | 1.59      |
|    height_err            | 0.00357   |
|    joints_err            | 0.155     |
|    joints_vel_err        | 118       |
|    root_ori_err          | 0.0418    |
| reward_terms/            |           |
|    com_reward            | 0.0214    |
|    end_effectors_reward  | 0.0242    |
|    height_reward         | 0.098     |
|    joints_reward         | 0.374     |
|    joints_vel_reward     | 2.81e-05  |
|    root_ori_reward       | 0.0197    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 380       |
|    ep_rew_mean           | 203       |
| time/                    |           |
|    collect_time          | 113       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 207       |
|    iterations            | 83        |
|    time_elapsed          | 32764     |
|    total_timesteps       | 6799360   |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 0.5294425 |
|    clip_fraction         | 0.762     |
|    clip_range            | 0.2       |
|    entropy_loss          | 43.9      |
|    explained_variance    | 0.639     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0324    |
|    n_updates             | 820       |
|    policy_gradient_loss  | -0.0652   |
|    std                   | 0.0905    |
|    value_loss            | 0.342     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.61       |
|    end_effectors_err     | 2.18       |
|    height_err            | -0.0114    |
|    joints_err            | 0.15       |
|    joints_vel_err        | 117        |
|    root_ori_err          | 0.0448     |
| reward_terms/            |            |
|    com_reward            | 0.0173     |
|    end_effectors_reward  | 0.0203     |
|    height_reward         | 0.0969     |
|    joints_reward         | 0.376      |
|    joints_vel_reward     | 3.9e-05    |
|    root_ori_reward       | 0.00369    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 401        |
|    ep_rew_mean           | 214        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 209        |
|    iterations            | 84         |
|    time_elapsed          | 32910      |
|    total_timesteps       | 6881280    |
|    train_time            | 32.3       |
| train/                   |            |
|    approx_kl             | 0.54092807 |
|    clip_fraction         | 0.763      |
|    clip_range            | 0.2        |
|    entropy_loss          | 44.1       |
|    explained_variance    | 0.577      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.0584     |
|    n_updates             | 830        |
|    policy_gradient_loss  | -0.0621    |
|    std                   | 0.09       |
|    value_loss            | 0.4        |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.731      |
|    end_effectors_err     | 3.03       |
|    height_err            | -0.00265   |
|    joints_err            | 0.149      |
|    joints_vel_err        | 116        |
|    root_ori_err          | 0.0481     |
| reward_terms/            |            |
|    com_reward            | 0.0193     |
|    end_effectors_reward  | 0.0161     |
|    height_reward         | 0.0962     |
|    joints_reward         | 0.378      |
|    joints_vel_reward     | 3.1e-05    |
|    root_ori_reward       | 0.0019     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 399        |
|    ep_rew_mean           | 214        |
| time/                    |            |
|    collect_time          | 116        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 210        |
|    iterations            | 85         |
|    time_elapsed          | 33058      |
|    total_timesteps       | 6963200    |
|    train_time            | 31.4       |
| train/                   |            |
|    approx_kl             | 0.55079144 |
|    clip_fraction         | 0.771      |
|    clip_range            | 0.2        |
|    entropy_loss          | 44.4       |
|    explained_variance    | 0.666      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.0514     |
|    n_updates             | 840        |
|    policy_gradient_loss  | -0.0674    |
|    std                   | 0.0895     |
|    value_loss            | 0.309      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.562     |
|    end_effectors_err     | 1.69      |
|    height_err            | -0.00702  |
|    joints_err            | 0.15      |
|    joints_vel_err        | 121       |
|    root_ori_err          | 0.0397    |
| reward_terms/            |           |
|    com_reward            | 0.0169    |
|    end_effectors_reward  | 0.0226    |
|    height_reward         | 0.0952    |
|    joints_reward         | 0.377     |
|    joints_vel_reward     | 1.84e-05  |
|    root_ori_reward       | 0.0144    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 388       |
|    ep_rew_mean           | 208       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 212       |
|    iterations            | 86        |
|    time_elapsed          | 33204     |
|    total_timesteps       | 7045120   |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 0.5595404 |
|    clip_fraction         | 0.772     |
|    clip_range            | 0.2       |
|    entropy_loss          | 44.7      |
|    explained_variance    | 0.668     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0327   |
|    n_updates             | 850       |
|    policy_gradient_loss  | -0.0707   |
|    std                   | 0.0889    |
|    value_loss            | 0.29      |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.431      |
|    end_effectors_err     | 1.11       |
|    height_err            | -0.00514   |
|    joints_err            | 0.145      |
|    joints_vel_err        | 117        |
|    root_ori_err          | 0.0319     |
| reward_terms/            |            |
|    com_reward            | 0.0283     |
|    end_effectors_reward  | 0.0286     |
|    height_reward         | 0.0965     |
|    joints_reward         | 0.38       |
|    joints_vel_reward     | 3.26e-05   |
|    root_ori_reward       | 0.0129     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 383        |
|    ep_rew_mean           | 207        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 213        |
|    iterations            | 87         |
|    time_elapsed          | 33349      |
|    total_timesteps       | 7127040    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.56583655 |
|    clip_fraction         | 0.774      |
|    clip_range            | 0.2        |
|    entropy_loss          | 45         |
|    explained_variance    | 0.673      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0751    |
|    n_updates             | 860        |
|    policy_gradient_loss  | -0.0694    |
|    std                   | 0.0885     |
|    value_loss            | 0.265      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.678      |
|    end_effectors_err     | 3.96       |
|    height_err            | -0.0116    |
|    joints_err            | 0.14       |
|    joints_vel_err        | 126        |
|    root_ori_err          | 0.207      |
| reward_terms/            |            |
|    com_reward            | 0.0202     |
|    end_effectors_reward  | 0.0124     |
|    height_reward         | 0.0975     |
|    joints_reward         | 0.384      |
|    joints_vel_reward     | 3.26e-05   |
|    root_ori_reward       | 0.000449   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 404        |
|    ep_rew_mean           | 219        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 215        |
|    iterations            | 88         |
|    time_elapsed          | 33494      |
|    total_timesteps       | 7208960    |
|    train_time            | 31.4       |
| train/                   |            |
|    approx_kl             | 0.55920684 |
|    clip_fraction         | 0.766      |
|    clip_range            | 0.2        |
|    entropy_loss          | 45.2       |
|    explained_variance    | 0.601      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.233      |
|    n_updates             | 870        |
|    policy_gradient_loss  | -0.0618    |
|    std                   | 0.088      |
|    value_loss            | 0.397      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.96      |
|    end_effectors_err     | 6.6       |
|    height_err            | -0.0137   |
|    joints_err            | 0.141     |
|    joints_vel_err        | 116       |
|    root_ori_err          | 0.275     |
| reward_terms/            |           |
|    com_reward            | 0.00737   |
|    end_effectors_reward  | 0.0047    |
|    height_reward         | 0.0965    |
|    joints_reward         | 0.384     |
|    joints_vel_reward     | 3.72e-05  |
|    root_ori_reward       | 0.00012   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 397       |
|    ep_rew_mean           | 214       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 216       |
|    iterations            | 89        |
|    time_elapsed          | 33641     |
|    total_timesteps       | 7290880   |
|    train_time            | 31.3      |
| train/                   |           |
|    approx_kl             | 0.5835228 |
|    clip_fraction         | 0.771     |
|    clip_range            | 0.2       |
|    entropy_loss          | 45.5      |
|    explained_variance    | 0.617     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.054    |
|    n_updates             | 880       |
|    policy_gradient_loss  | -0.065    |
|    std                   | 0.0875    |
|    value_loss            | 0.343     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.561     |
|    end_effectors_err     | 1.8       |
|    height_err            | -0.014    |
|    joints_err            | 0.14      |
|    joints_vel_err        | 116       |
|    root_ori_err          | 0.0225    |
| reward_terms/            |           |
|    com_reward            | 0.0237    |
|    end_effectors_reward  | 0.0231    |
|    height_reward         | 0.0968    |
|    joints_reward         | 0.384     |
|    joints_vel_reward     | 2.69e-05  |
|    root_ori_reward       | 0.0124    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 416       |
|    ep_rew_mean           | 226       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 218       |
|    iterations            | 90        |
|    time_elapsed          | 33788     |
|    total_timesteps       | 7372800   |
|    train_time            | 31.4      |
| train/                   |           |
|    approx_kl             | 0.5923104 |
|    clip_fraction         | 0.775     |
|    clip_range            | 0.2       |
|    entropy_loss          | 45.7      |
|    explained_variance    | 0.693     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0739   |
|    n_updates             | 890       |
|    policy_gradient_loss  | -0.0679   |
|    std                   | 0.087     |
|    value_loss            | 0.282     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.598     |
|    end_effectors_err     | 2.14      |
|    height_err            | 0.00497   |
|    joints_err            | 0.129     |
|    joints_vel_err        | 112       |
|    root_ori_err          | 0.0382    |
| reward_terms/            |           |
|    com_reward            | 0.019     |
|    end_effectors_reward  | 0.0207    |
|    height_reward         | 0.0977    |
|    joints_reward         | 0.392     |
|    joints_vel_reward     | 4.48e-05  |
|    root_ori_reward       | 0.0056    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 408       |
|    ep_rew_mean           | 222       |
| time/                    |           |
|    collect_time          | 113       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 219       |
|    iterations            | 91        |
|    time_elapsed          | 33933     |
|    total_timesteps       | 7454720   |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 0.6050535 |
|    clip_fraction         | 0.775     |
|    clip_range            | 0.2       |
|    entropy_loss          | 46        |
|    explained_variance    | 0.726     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0775   |
|    n_updates             | 900       |
|    policy_gradient_loss  | -0.0734   |
|    std                   | 0.0866    |
|    value_loss            | 0.23      |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.542      |
|    end_effectors_err     | 1.8        |
|    height_err            | -0.0196    |
|    joints_err            | 0.128      |
|    joints_vel_err        | 113        |
|    root_ori_err          | 0.0438     |
| reward_terms/            |            |
|    com_reward            | 0.0225     |
|    end_effectors_reward  | 0.0226     |
|    height_reward         | 0.0957     |
|    joints_reward         | 0.392      |
|    joints_vel_reward     | 3.87e-05   |
|    root_ori_reward       | 0.0039     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 382        |
|    ep_rew_mean           | 209        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 221        |
|    iterations            | 92         |
|    time_elapsed          | 34078      |
|    total_timesteps       | 7536640    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.58262837 |
|    clip_fraction         | 0.771      |
|    clip_range            | 0.2        |
|    entropy_loss          | 46.2       |
|    explained_variance    | 0.696      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0626    |
|    n_updates             | 910        |
|    policy_gradient_loss  | -0.0679    |
|    std                   | 0.0861     |
|    value_loss            | 0.275      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.316      |
|    end_effectors_err     | 0.688      |
|    height_err            | -0.0124    |
|    joints_err            | 0.141      |
|    joints_vel_err        | 119        |
|    root_ori_err          | 0.0432     |
| reward_terms/            |            |
|    com_reward            | 0.0366     |
|    end_effectors_reward  | 0.0336     |
|    height_reward         | 0.0964     |
|    joints_reward         | 0.385      |
|    joints_vel_reward     | 3.98e-05   |
|    root_ori_reward       | 0.00576    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 403        |
|    ep_rew_mean           | 220        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 222        |
|    iterations            | 93         |
|    time_elapsed          | 34223      |
|    total_timesteps       | 7618560    |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.59748566 |
|    clip_fraction         | 0.771      |
|    clip_range            | 0.2        |
|    entropy_loss          | 46.5       |
|    explained_variance    | 0.701      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 2.29e-06   |
|    n_updates             | 920        |
|    policy_gradient_loss  | -0.0685    |
|    std                   | 0.0856     |
|    value_loss            | 0.267      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.588      |
|    end_effectors_err     | 1.92       |
|    height_err            | -0.00779   |
|    joints_err            | 0.132      |
|    joints_vel_err        | 118        |
|    root_ori_err          | 0.0458     |
| reward_terms/            |            |
|    com_reward            | 0.0178     |
|    end_effectors_reward  | 0.0227     |
|    height_reward         | 0.097      |
|    joints_reward         | 0.39       |
|    joints_vel_reward     | 4.47e-05   |
|    root_ori_reward       | 0.0189     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 427        |
|    ep_rew_mean           | 234        |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 224        |
|    iterations            | 94         |
|    time_elapsed          | 34370      |
|    total_timesteps       | 7700480    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.63642406 |
|    clip_fraction         | 0.774      |
|    clip_range            | 0.2        |
|    entropy_loss          | 46.8       |
|    explained_variance    | 0.703      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.31       |
|    n_updates             | 930        |
|    policy_gradient_loss  | -0.0681    |
|    std                   | 0.0852     |
|    value_loss            | 0.267      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.471     |
|    end_effectors_err     | 1.4       |
|    height_err            | -0.0104   |
|    joints_err            | 0.136     |
|    joints_vel_err        | 115       |
|    root_ori_err          | 0.045     |
| reward_terms/            |           |
|    com_reward            | 0.0265    |
|    end_effectors_reward  | 0.0258    |
|    height_reward         | 0.0969    |
|    joints_reward         | 0.387     |
|    joints_vel_reward     | 4.94e-05  |
|    root_ori_reward       | 0.0089    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 408       |
|    ep_rew_mean           | 225       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00207   |
|    fps                   | 225       |
|    iterations            | 95        |
|    time_elapsed          | 34517     |
|    total_timesteps       | 7782400   |
|    train_time            | 32.2      |
| train/                   |           |
|    approx_kl             | 0.6716941 |
|    clip_fraction         | 0.777     |
|    clip_range            | 0.2       |
|    entropy_loss          | 47        |
|    explained_variance    | 0.715     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0707   |
|    n_updates             | 940       |
|    policy_gradient_loss  | -0.0713   |
|    std                   | 0.0848    |
|    value_loss            | 0.226     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.388      |
|    end_effectors_err     | 0.987      |
|    height_err            | -0.0166    |
|    joints_err            | 0.132      |
|    joints_vel_err        | 108        |
|    root_ori_err          | 0.0402     |
| reward_terms/            |            |
|    com_reward            | 0.0319     |
|    end_effectors_reward  | 0.0301     |
|    height_reward         | 0.0954     |
|    joints_reward         | 0.389      |
|    joints_vel_reward     | 3.32e-05   |
|    root_ori_reward       | 0.00501    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 414        |
|    ep_rew_mean           | 229        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 226        |
|    iterations            | 96         |
|    time_elapsed          | 34663      |
|    total_timesteps       | 7864320    |
|    train_time            | 31.9       |
| train/                   |            |
|    approx_kl             | 0.62447125 |
|    clip_fraction         | 0.772      |
|    clip_range            | 0.2        |
|    entropy_loss          | 47.2       |
|    explained_variance    | 0.68       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.032      |
|    n_updates             | 950        |
|    policy_gradient_loss  | -0.0643    |
|    std                   | 0.0844     |
|    value_loss            | 0.288      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.496      |
|    end_effectors_err     | 1.51       |
|    height_err            | 0.00329    |
|    joints_err            | 0.136      |
|    joints_vel_err        | 111        |
|    root_ori_err          | 0.0324     |
| reward_terms/            |            |
|    com_reward            | 0.028      |
|    end_effectors_reward  | 0.0252     |
|    height_reward         | 0.0972     |
|    joints_reward         | 0.387      |
|    joints_vel_reward     | 4.99e-05   |
|    root_ori_reward       | 0.019      |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 408        |
|    ep_rew_mean           | 226        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00208    |
|    fps                   | 228        |
|    iterations            | 97         |
|    time_elapsed          | 34809      |
|    total_timesteps       | 7946240    |
|    train_time            | 32.6       |
| train/                   |            |
|    approx_kl             | 0.66204405 |
|    clip_fraction         | 0.777      |
|    clip_range            | 0.2        |
|    entropy_loss          | 47.4       |
|    explained_variance    | 0.778      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.00641    |
|    n_updates             | 960        |
|    policy_gradient_loss  | -0.0743    |
|    std                   | 0.084      |
|    value_loss            | 0.202      |
-----------------------------------------
policy converted successfully and saved.
Saving video to /cluster/home/anghosh/DHProject/deep-mimic/log/2023-05-29-22-14-49-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M-residual_1/eval-video-PylocoVanilla-v0-step-8210-to-step-9210.mp4
Eval num_timesteps=8000000, episode_reward=292.06 +/- 0.00
Episode length: 527.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.33      |
|    end_effectors_err     | 0.769     |
|    height_err            | -0.00772  |
|    joints_err            | 0.127     |
|    joints_vel_err        | 114       |
|    root_ori_err          | 0.0709    |
| eval/                    |           |
|    mean_ep_length        | 527       |
|    mean_reward           | 292       |
|    success_rate          | 1         |
| reward_terms/            |           |
|    com_reward            | 0.0398    |
|    end_effectors_reward  | 0.033     |
|    height_reward         | 0.0964    |
|    joints_reward         | 0.394     |
|    joints_vel_reward     | 8.12e-05  |
|    root_ori_reward       | 0.00988   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| time/                    |           |
|    evaluate_actions_time | 0.00204   |
|    total_timesteps       | 8000000   |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 0.6651546 |
|    clip_fraction         | 0.776     |
|    clip_range            | 0.2       |
|    entropy_loss          | 47.7      |
|    explained_variance    | 0.706     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.00424  |
|    n_updates             | 970       |
|    policy_gradient_loss  | -0.0655   |
|    std                   | 0.0835    |
|    value_loss            | 0.282     |
----------------------------------------
--------------------------------------
| err_terms/              |          |
|    com_err              | 0.439    |
|    end_effectors_err    | 1.15     |
|    height_err           | -0.0136  |
|    joints_err           | 0.135    |
|    joints_vel_err       | 110      |
|    root_ori_err         | 0.0449   |
| reward_terms/           |          |
|    com_reward           | 0.0283   |
|    end_effectors_reward | 0.0283   |
|    height_reward        | 0.0966   |
|    joints_reward        | 0.388    |
|    joints_vel_reward    | 8.02e-05 |
|    root_ori_reward      | 0.00797  |
|    smoothness1_reward   | 0        |
|    smoothness2_reward   | 0        |
|    smoothness_reward    | 0        |
| rollout/                |          |
|    ep_len_mean          | 399      |
|    ep_rew_mean          | 221      |
| time/                   |          |
|    collect_time         | 6.36e+03 |
|    fps                  | 194      |
|    iterations           | 98       |
|    time_elapsed         | 41198    |
|    total_timesteps      | 8028160  |
--------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.374      |
|    end_effectors_err     | 0.825      |
|    height_err            | -0.00549   |
|    joints_err            | 0.127      |
|    joints_vel_err        | 112        |
|    root_ori_err          | 0.0273     |
| reward_terms/            |            |
|    com_reward            | 0.0314     |
|    end_effectors_reward  | 0.0322     |
|    height_reward         | 0.0974     |
|    joints_reward         | 0.393      |
|    joints_vel_reward     | 3.97e-05   |
|    root_ori_reward       | 0.0141     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 378        |
|    ep_rew_mean           | 210        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 196        |
|    iterations            | 99         |
|    time_elapsed          | 41343      |
|    total_timesteps       | 8110080    |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.65731937 |
|    clip_fraction         | 0.777      |
|    clip_range            | 0.2        |
|    entropy_loss          | 48         |
|    explained_variance    | 0.711      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0675    |
|    n_updates             | 980        |
|    policy_gradient_loss  | -0.0683    |
|    std                   | 0.083      |
|    value_loss            | 0.262      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.368      |
|    end_effectors_err     | 0.793      |
|    height_err            | -0.00768   |
|    joints_err            | 0.116      |
|    joints_vel_err        | 116        |
|    root_ori_err          | 0.0398     |
| reward_terms/            |            |
|    com_reward            | 0.0311     |
|    end_effectors_reward  | 0.0324     |
|    height_reward         | 0.0983     |
|    joints_reward         | 0.402      |
|    joints_vel_reward     | 5.66e-05   |
|    root_ori_reward       | 0.0266     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 398        |
|    ep_rew_mean           | 222        |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00205    |
|    fps                   | 197        |
|    iterations            | 100        |
|    time_elapsed          | 41489      |
|    total_timesteps       | 8192000    |
|    train_time            | 32.2       |
| train/                   |            |
|    approx_kl             | 0.64499205 |
|    clip_fraction         | 0.775      |
|    clip_range            | 0.2        |
|    entropy_loss          | 48.2       |
|    explained_variance    | 0.664      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0572    |
|    n_updates             | 990        |
|    policy_gradient_loss  | -0.0639    |
|    std                   | 0.0825     |
|    value_loss            | 0.316      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.448     |
|    end_effectors_err     | 1.3       |
|    height_err            | -0.0092   |
|    joints_err            | 0.124     |
|    joints_vel_err        | 103       |
|    root_ori_err          | 0.046     |
| reward_terms/            |           |
|    com_reward            | 0.027     |
|    end_effectors_reward  | 0.0262    |
|    height_reward         | 0.0979    |
|    joints_reward         | 0.396     |
|    joints_vel_reward     | 6.03e-05  |
|    root_ori_reward       | 0.00844   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 416       |
|    ep_rew_mean           | 232       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 198       |
|    iterations            | 101       |
|    time_elapsed          | 41635     |
|    total_timesteps       | 8273920   |
|    train_time            | 31.6      |
| train/                   |           |
|    approx_kl             | 0.6619173 |
|    clip_fraction         | 0.777     |
|    clip_range            | 0.2       |
|    entropy_loss          | 48.5      |
|    explained_variance    | 0.663     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0686    |
|    n_updates             | 1000      |
|    policy_gradient_loss  | -0.0622   |
|    std                   | 0.0821    |
|    value_loss            | 0.326     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.525     |
|    end_effectors_err     | 1.5       |
|    height_err            | -0.0178   |
|    joints_err            | 0.123     |
|    joints_vel_err        | 104       |
|    root_ori_err          | 0.0335    |
| reward_terms/            |           |
|    com_reward            | 0.0187    |
|    end_effectors_reward  | 0.0244    |
|    height_reward         | 0.0961    |
|    joints_reward         | 0.397     |
|    joints_vel_reward     | 7.64e-05  |
|    root_ori_reward       | 0.00908   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 404       |
|    ep_rew_mean           | 225       |
| time/                    |           |
|    collect_time          | 113       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 199       |
|    iterations            | 102       |
|    time_elapsed          | 41780     |
|    total_timesteps       | 8355840   |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 0.6658872 |
|    clip_fraction         | 0.779     |
|    clip_range            | 0.2       |
|    entropy_loss          | 48.7      |
|    explained_variance    | 0.703     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.222     |
|    n_updates             | 1010      |
|    policy_gradient_loss  | -0.0692   |
|    std                   | 0.0817    |
|    value_loss            | 0.253     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.567     |
|    end_effectors_err     | 1.82      |
|    height_err            | -0.0101   |
|    joints_err            | 0.123     |
|    joints_vel_err        | 110       |
|    root_ori_err          | 0.0499    |
| reward_terms/            |           |
|    com_reward            | 0.0188    |
|    end_effectors_reward  | 0.0232    |
|    height_reward         | 0.098     |
|    joints_reward         | 0.397     |
|    joints_vel_reward     | 5.45e-05  |
|    root_ori_reward       | 0.0113    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 374       |
|    ep_rew_mean           | 210       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00205   |
|    fps                   | 201       |
|    iterations            | 103       |
|    time_elapsed          | 41926     |
|    total_timesteps       | 8437760   |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 0.6899816 |
|    clip_fraction         | 0.779     |
|    clip_range            | 0.2       |
|    entropy_loss          | 49        |
|    explained_variance    | 0.676     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.202     |
|    n_updates             | 1020      |
|    policy_gradient_loss  | -0.0635   |
|    std                   | 0.0812    |
|    value_loss            | 0.299     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.336     |
|    end_effectors_err     | 0.848     |
|    height_err            | -0.00484  |
|    joints_err            | 0.116     |
|    joints_vel_err        | 108       |
|    root_ori_err          | 0.0661    |
| reward_terms/            |           |
|    com_reward            | 0.0389    |
|    end_effectors_reward  | 0.0321    |
|    height_reward         | 0.0965    |
|    joints_reward         | 0.403     |
|    joints_vel_reward     | 6.64e-05  |
|    root_ori_reward       | 0.00928   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 404       |
|    ep_rew_mean           | 227       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00205   |
|    fps                   | 202       |
|    iterations            | 104       |
|    time_elapsed          | 42074     |
|    total_timesteps       | 8519680   |
|    train_time            | 32.2      |
| train/                   |           |
|    approx_kl             | 0.6967007 |
|    clip_fraction         | 0.779     |
|    clip_range            | 0.2       |
|    entropy_loss          | 49.2      |
|    explained_variance    | 0.682     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0207    |
|    n_updates             | 1030      |
|    policy_gradient_loss  | -0.0649   |
|    std                   | 0.0808    |
|    value_loss            | 0.298     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.448     |
|    end_effectors_err     | 1.2       |
|    height_err            | -0.00407  |
|    joints_err            | 0.127     |
|    joints_vel_err        | 105       |
|    root_ori_err          | 0.0301    |
| reward_terms/            |           |
|    com_reward            | 0.0299    |
|    end_effectors_reward  | 0.0284    |
|    height_reward         | 0.0973    |
|    joints_reward         | 0.394     |
|    joints_vel_reward     | 5.12e-05  |
|    root_ori_reward       | 0.0221    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 413       |
|    ep_rew_mean           | 232       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00205   |
|    fps                   | 203       |
|    iterations            | 105       |
|    time_elapsed          | 42222     |
|    total_timesteps       | 8601600   |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 0.7869088 |
|    clip_fraction         | 0.784     |
|    clip_range            | 0.2       |
|    entropy_loss          | 49.4      |
|    explained_variance    | 0.713     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0269    |
|    n_updates             | 1040      |
|    policy_gradient_loss  | -0.0684   |
|    std                   | 0.0805    |
|    value_loss            | 0.247     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.494     |
|    end_effectors_err     | 1.74      |
|    height_err            | -0.0106   |
|    joints_err            | 0.116     |
|    joints_vel_err        | 98.3      |
|    root_ori_err          | 0.0612    |
| reward_terms/            |           |
|    com_reward            | 0.0245    |
|    end_effectors_reward  | 0.0224    |
|    height_reward         | 0.0967    |
|    joints_reward         | 0.402     |
|    joints_vel_reward     | 5.95e-05  |
|    root_ori_reward       | 0.00187   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 413       |
|    ep_rew_mean           | 233       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 204       |
|    iterations            | 106       |
|    time_elapsed          | 42368     |
|    total_timesteps       | 8683520   |
|    train_time            | 31.4      |
| train/                   |           |
|    approx_kl             | 0.7200102 |
|    clip_fraction         | 0.781     |
|    clip_range            | 0.2       |
|    entropy_loss          | 49.7      |
|    explained_variance    | 0.704     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0724   |
|    n_updates             | 1050      |
|    policy_gradient_loss  | -0.0647   |
|    std                   | 0.0801    |
|    value_loss            | 0.279     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.508      |
|    end_effectors_err     | 1.49       |
|    height_err            | 0.00121    |
|    joints_err            | 0.115      |
|    joints_vel_err        | 104        |
|    root_ori_err          | 0.0237     |
| reward_terms/            |            |
|    com_reward            | 0.0238     |
|    end_effectors_reward  | 0.0257     |
|    height_reward         | 0.0981     |
|    joints_reward         | 0.403      |
|    joints_vel_reward     | 6.04e-05   |
|    root_ori_reward       | 0.0226     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 405        |
|    ep_rew_mean           | 229        |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 206        |
|    iterations            | 107        |
|    time_elapsed          | 42512      |
|    total_timesteps       | 8765440    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.71265036 |
|    clip_fraction         | 0.78       |
|    clip_range            | 0.2        |
|    entropy_loss          | 49.9       |
|    explained_variance    | 0.664      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.437      |
|    n_updates             | 1060       |
|    policy_gradient_loss  | -0.0565    |
|    std                   | 0.0796     |
|    value_loss            | 0.357      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.374     |
|    end_effectors_err     | 0.845     |
|    height_err            | -0.00516  |
|    joints_err            | 0.125     |
|    joints_vel_err        | 112       |
|    root_ori_err          | 0.0332    |
| reward_terms/            |           |
|    com_reward            | 0.0331    |
|    end_effectors_reward  | 0.0322    |
|    height_reward         | 0.0977    |
|    joints_reward         | 0.396     |
|    joints_vel_reward     | 5.26e-05  |
|    root_ori_reward       | 0.0256    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 389       |
|    ep_rew_mean           | 221       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 207       |
|    iterations            | 108       |
|    time_elapsed          | 42660     |
|    total_timesteps       | 8847360   |
|    train_time            | 32        |
| train/                   |           |
|    approx_kl             | 0.7056021 |
|    clip_fraction         | 0.78      |
|    clip_range            | 0.2       |
|    entropy_loss          | 50.2      |
|    explained_variance    | 0.633     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0102    |
|    n_updates             | 1070      |
|    policy_gradient_loss  | -0.0585   |
|    std                   | 0.0793    |
|    value_loss            | 0.374     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.701     |
|    end_effectors_err     | 2.76      |
|    height_err            | -0.00492  |
|    joints_err            | 0.121     |
|    joints_vel_err        | 108       |
|    root_ori_err          | 0.0507    |
| reward_terms/            |           |
|    com_reward            | 0.0158    |
|    end_effectors_reward  | 0.0179    |
|    height_reward         | 0.0964    |
|    joints_reward         | 0.399     |
|    joints_vel_reward     | 7.82e-05  |
|    root_ori_reward       | 0.013     |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 405       |
|    ep_rew_mean           | 229       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 208       |
|    iterations            | 109       |
|    time_elapsed          | 42807     |
|    total_timesteps       | 8929280   |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 0.7021656 |
|    clip_fraction         | 0.781     |
|    clip_range            | 0.2       |
|    entropy_loss          | 50.3      |
|    explained_variance    | 0.654     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.236     |
|    n_updates             | 1080      |
|    policy_gradient_loss  | -0.0595   |
|    std                   | 0.079     |
|    value_loss            | 0.356     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.661     |
|    end_effectors_err     | 2.33      |
|    height_err            | -0.00271  |
|    joints_err            | 0.124     |
|    joints_vel_err        | 112       |
|    root_ori_err          | 0.0399    |
| reward_terms/            |           |
|    com_reward            | 0.0154    |
|    end_effectors_reward  | 0.0193    |
|    height_reward         | 0.0977    |
|    joints_reward         | 0.396     |
|    joints_vel_reward     | 5.62e-05  |
|    root_ori_reward       | 0.0218    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 400       |
|    ep_rew_mean           | 226       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00199   |
|    fps                   | 209       |
|    iterations            | 110       |
|    time_elapsed          | 42953     |
|    total_timesteps       | 9011200   |
|    train_time            | 31.3      |
| train/                   |           |
|    approx_kl             | 0.7626673 |
|    clip_fraction         | 0.787     |
|    clip_range            | 0.2       |
|    entropy_loss          | 50.5      |
|    explained_variance    | 0.704     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0261    |
|    n_updates             | 1090      |
|    policy_gradient_loss  | -0.0654   |
|    std                   | 0.0786    |
|    value_loss            | 0.275     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.459     |
|    end_effectors_err     | 1.21      |
|    height_err            | -0.00268  |
|    joints_err            | 0.117     |
|    joints_vel_err        | 98.8      |
|    root_ori_err          | 0.0368    |
| reward_terms/            |           |
|    com_reward            | 0.0259    |
|    end_effectors_reward  | 0.0277    |
|    height_reward         | 0.0974    |
|    joints_reward         | 0.401     |
|    joints_vel_reward     | 6.53e-05  |
|    root_ori_reward       | 0.0269    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 408       |
|    ep_rew_mean           | 232       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 210       |
|    iterations            | 111       |
|    time_elapsed          | 43100     |
|    total_timesteps       | 9093120   |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 0.7911614 |
|    clip_fraction         | 0.784     |
|    clip_range            | 0.2       |
|    entropy_loss          | 50.8      |
|    explained_variance    | 0.693     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0554   |
|    n_updates             | 1100      |
|    policy_gradient_loss  | -0.062    |
|    std                   | 0.0781    |
|    value_loss            | 0.325     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.431     |
|    end_effectors_err     | 1.13      |
|    height_err            | -0.00939  |
|    joints_err            | 0.108     |
|    joints_vel_err        | 104       |
|    root_ori_err          | 0.0342    |
| reward_terms/            |           |
|    com_reward            | 0.0274    |
|    end_effectors_reward  | 0.0283    |
|    height_reward         | 0.0978    |
|    joints_reward         | 0.408     |
|    joints_vel_reward     | 9.63e-05  |
|    root_ori_reward       | 0.0108    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 390       |
|    ep_rew_mean           | 224       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 212       |
|    iterations            | 112       |
|    time_elapsed          | 43247     |
|    total_timesteps       | 9175040   |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 0.7917332 |
|    clip_fraction         | 0.791     |
|    clip_range            | 0.2       |
|    entropy_loss          | 51.1      |
|    explained_variance    | 0.739     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0597   |
|    n_updates             | 1110      |
|    policy_gradient_loss  | -0.0686   |
|    std                   | 0.0777    |
|    value_loss            | 0.242     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.454      |
|    end_effectors_err     | 1.2        |
|    height_err            | -0.000861  |
|    joints_err            | 0.114      |
|    joints_vel_err        | 101        |
|    root_ori_err          | 0.0116     |
| reward_terms/            |            |
|    com_reward            | 0.024      |
|    end_effectors_reward  | 0.0271     |
|    height_reward         | 0.0982     |
|    joints_reward         | 0.403      |
|    joints_vel_reward     | 9.15e-05   |
|    root_ori_reward       | 0.022      |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 408        |
|    ep_rew_mean           | 233        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 213        |
|    iterations            | 113        |
|    time_elapsed          | 43393      |
|    total_timesteps       | 9256960    |
|    train_time            | 31.5       |
| train/                   |            |
|    approx_kl             | 0.74733585 |
|    clip_fraction         | 0.782      |
|    clip_range            | 0.2        |
|    entropy_loss          | 51.3       |
|    explained_variance    | 0.665      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0647    |
|    n_updates             | 1120       |
|    policy_gradient_loss  | -0.0563    |
|    std                   | 0.0772     |
|    value_loss            | 0.372      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.313      |
|    end_effectors_err     | 0.704      |
|    height_err            | -0.00654   |
|    joints_err            | 0.103      |
|    joints_vel_err        | 101        |
|    root_ori_err          | 0.0578     |
| reward_terms/            |            |
|    com_reward            | 0.0421     |
|    end_effectors_reward  | 0.0347     |
|    height_reward         | 0.0971     |
|    joints_reward         | 0.411      |
|    joints_vel_reward     | 0.000112   |
|    root_ori_reward       | 0.0225     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 397        |
|    ep_rew_mean           | 228        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 214        |
|    iterations            | 114        |
|    time_elapsed          | 43539      |
|    total_timesteps       | 9338880    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.80644816 |
|    clip_fraction         | 0.791      |
|    clip_range            | 0.2        |
|    entropy_loss          | 51.6       |
|    explained_variance    | 0.722      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.0552     |
|    n_updates             | 1130       |
|    policy_gradient_loss  | -0.0649    |
|    std                   | 0.0769     |
|    value_loss            | 0.273      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.58      |
|    end_effectors_err     | 2.05      |
|    height_err            | 0.000957  |
|    joints_err            | 0.115     |
|    joints_vel_err        | 106       |
|    root_ori_err          | 0.066     |
| reward_terms/            |           |
|    com_reward            | 0.023     |
|    end_effectors_reward  | 0.0218    |
|    height_reward         | 0.0983    |
|    joints_reward         | 0.403     |
|    joints_vel_reward     | 8.89e-05  |
|    root_ori_reward       | 0.00724   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 410       |
|    ep_rew_mean           | 235       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 215       |
|    iterations            | 115       |
|    time_elapsed          | 43685     |
|    total_timesteps       | 9420800   |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 0.7725617 |
|    clip_fraction         | 0.787     |
|    clip_range            | 0.2       |
|    entropy_loss          | 51.8      |
|    explained_variance    | 0.683     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.145     |
|    n_updates             | 1140      |
|    policy_gradient_loss  | -0.0588   |
|    std                   | 0.0765    |
|    value_loss            | 0.324     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.432     |
|    end_effectors_err     | 1.27      |
|    height_err            | 0.00062   |
|    joints_err            | 0.113     |
|    joints_vel_err        | 101       |
|    root_ori_err          | 0.0403    |
| reward_terms/            |           |
|    com_reward            | 0.03      |
|    end_effectors_reward  | 0.0272    |
|    height_reward         | 0.0985    |
|    joints_reward         | 0.404     |
|    joints_vel_reward     | 8.34e-05  |
|    root_ori_reward       | 0.00475   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 411       |
|    ep_rew_mean           | 235       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00199   |
|    fps                   | 216       |
|    iterations            | 116       |
|    time_elapsed          | 43831     |
|    total_timesteps       | 9502720   |
|    train_time            | 31.3      |
| train/                   |           |
|    approx_kl             | 0.8512232 |
|    clip_fraction         | 0.791     |
|    clip_range            | 0.2       |
|    entropy_loss          | 52        |
|    explained_variance    | 0.705     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0529   |
|    n_updates             | 1150      |
|    policy_gradient_loss  | -0.0638   |
|    std                   | 0.0761    |
|    value_loss            | 0.284     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.195      |
|    end_effectors_err     | 0.306      |
|    height_err            | -0.00844   |
|    joints_err            | 0.123      |
|    joints_vel_err        | 96.1       |
|    root_ori_err          | 0.0279     |
| reward_terms/            |            |
|    com_reward            | 0.0614     |
|    end_effectors_reward  | 0.0421     |
|    height_reward         | 0.0975     |
|    joints_reward         | 0.397      |
|    joints_vel_reward     | 0.000135   |
|    root_ori_reward       | 0.0232     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 388        |
|    ep_rew_mean           | 223        |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 217        |
|    iterations            | 117        |
|    time_elapsed          | 43977      |
|    total_timesteps       | 9584640    |
|    train_time            | 31.3       |
| train/                   |            |
|    approx_kl             | 0.79084826 |
|    clip_fraction         | 0.787      |
|    clip_range            | 0.2        |
|    entropy_loss          | 52.3       |
|    explained_variance    | 0.692      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0493    |
|    n_updates             | 1160       |
|    policy_gradient_loss  | -0.0598    |
|    std                   | 0.0757     |
|    value_loss            | 0.316      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.464     |
|    end_effectors_err     | 1.33      |
|    height_err            | -0.00168  |
|    joints_err            | 0.113     |
|    joints_vel_err        | 94.9      |
|    root_ori_err          | 0.0209    |
| reward_terms/            |           |
|    com_reward            | 0.0301    |
|    end_effectors_reward  | 0.0269    |
|    height_reward         | 0.0982    |
|    joints_reward         | 0.403     |
|    joints_vel_reward     | 0.000105  |
|    root_ori_reward       | 0.0186    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 407       |
|    ep_rew_mean           | 235       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 219       |
|    iterations            | 118       |
|    time_elapsed          | 44123     |
|    total_timesteps       | 9666560   |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 0.8022253 |
|    clip_fraction         | 0.787     |
|    clip_range            | 0.2       |
|    entropy_loss          | 52.6      |
|    explained_variance    | 0.698     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.09      |
|    n_updates             | 1170      |
|    policy_gradient_loss  | -0.0587   |
|    std                   | 0.0753    |
|    value_loss            | 0.33      |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.406     |
|    end_effectors_err     | 1.04      |
|    height_err            | -0.00226  |
|    joints_err            | 0.106     |
|    joints_vel_err        | 93.1      |
|    root_ori_err          | 0.0202    |
| reward_terms/            |           |
|    com_reward            | 0.0302    |
|    end_effectors_reward  | 0.0296    |
|    height_reward         | 0.0987    |
|    joints_reward         | 0.409     |
|    joints_vel_reward     | 0.000117  |
|    root_ori_reward       | 0.0166    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 396       |
|    ep_rew_mean           | 228       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00199   |
|    fps                   | 220       |
|    iterations            | 119       |
|    time_elapsed          | 44270     |
|    total_timesteps       | 9748480   |
|    train_time            | 31        |
| train/                   |           |
|    approx_kl             | 0.8463818 |
|    clip_fraction         | 0.796     |
|    clip_range            | 0.2       |
|    entropy_loss          | 52.8      |
|    explained_variance    | 0.739     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0753   |
|    n_updates             | 1180      |
|    policy_gradient_loss  | -0.071    |
|    std                   | 0.075     |
|    value_loss            | 0.227     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.321     |
|    end_effectors_err     | 0.641     |
|    height_err            | -0.00207  |
|    joints_err            | 0.113     |
|    joints_vel_err        | 101       |
|    root_ori_err          | 0.0236    |
| reward_terms/            |           |
|    com_reward            | 0.0392    |
|    end_effectors_reward  | 0.0352    |
|    height_reward         | 0.0975    |
|    joints_reward         | 0.405     |
|    joints_vel_reward     | 0.000132  |
|    root_ori_reward       | 0.0275    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 399       |
|    ep_rew_mean           | 231       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00207   |
|    fps                   | 221       |
|    iterations            | 120       |
|    time_elapsed          | 44417     |
|    total_timesteps       | 9830400   |
|    train_time            | 32.2      |
| train/                   |           |
|    approx_kl             | 0.7794014 |
|    clip_fraction         | 0.787     |
|    clip_range            | 0.2       |
|    entropy_loss          | 53        |
|    explained_variance    | 0.686     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0788    |
|    n_updates             | 1190      |
|    policy_gradient_loss  | -0.0551   |
|    std                   | 0.0746    |
|    value_loss            | 0.348     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.317      |
|    end_effectors_err     | 0.754      |
|    height_err            | -0.00615   |
|    joints_err            | 0.108      |
|    joints_vel_err        | 94.2       |
|    root_ori_err          | 0.0458     |
| reward_terms/            |            |
|    com_reward            | 0.0415     |
|    end_effectors_reward  | 0.0333     |
|    height_reward         | 0.0981     |
|    joints_reward         | 0.408      |
|    joints_vel_reward     | 0.000144   |
|    root_ori_reward       | 0.00498    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 406        |
|    ep_rew_mean           | 235        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 222        |
|    iterations            | 121        |
|    time_elapsed          | 44563      |
|    total_timesteps       | 9912320    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.84352267 |
|    clip_fraction         | 0.794      |
|    clip_range            | 0.2        |
|    entropy_loss          | 53.3       |
|    explained_variance    | 0.71       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0755    |
|    n_updates             | 1200       |
|    policy_gradient_loss  | -0.0643    |
|    std                   | 0.0742     |
|    value_loss            | 0.286      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.562     |
|    end_effectors_err     | 2.24      |
|    height_err            | -0.000197 |
|    joints_err            | 0.109     |
|    joints_vel_err        | 91.1      |
|    root_ori_err          | 0.0619    |
| reward_terms/            |           |
|    com_reward            | 0.0256    |
|    end_effectors_reward  | 0.0224    |
|    height_reward         | 0.097     |
|    joints_reward         | 0.407     |
|    joints_vel_reward     | 0.000119  |
|    root_ori_reward       | 0.0108    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 393       |
|    ep_rew_mean           | 228       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 223       |
|    iterations            | 122       |
|    time_elapsed          | 44710     |
|    total_timesteps       | 9994240   |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 0.8472004 |
|    clip_fraction         | 0.791     |
|    clip_range            | 0.2       |
|    entropy_loss          | 53.5      |
|    explained_variance    | 0.735     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0633   |
|    n_updates             | 1210      |
|    policy_gradient_loss  | -0.0617   |
|    std                   | 0.0738    |
|    value_loss            | 0.274     |
----------------------------------------
policy converted successfully and saved.
Saving video to /cluster/home/anghosh/DHProject/deep-mimic/log/2023-05-29-22-14-49-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M-residual_1/eval-video-PylocoVanilla-v0-step-10845-to-step-11845.mp4
Eval num_timesteps=10000000, episode_reward=292.15 +/- 0.00
Episode length: 527.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.519     |
|    end_effectors_err     | 1.7       |
|    height_err            | -0.013    |
|    joints_err            | 0.103     |
|    joints_vel_err        | 95.7      |
|    root_ori_err          | 0.0383    |
| eval/                    |           |
|    mean_ep_length        | 527       |
|    mean_reward           | 292       |
|    success_rate          | 1         |
| reward_terms/            |           |
|    com_reward            | 0.0241    |
|    end_effectors_reward  | 0.0239    |
|    height_reward         | 0.0972    |
|    joints_reward         | 0.412     |
|    joints_vel_reward     | 0.000128  |
|    root_ori_reward       | 0.0169    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| time/                    |           |
|    evaluate_actions_time | 0.00205   |
|    total_timesteps       | 10000000  |
|    train_time            | 32.1      |
| train/                   |           |
|    approx_kl             | 0.8683367 |
|    clip_fraction         | 0.794     |
|    clip_range            | 0.2       |
|    entropy_loss          | 53.7      |
|    explained_variance    | 0.712     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.00122  |
|    n_updates             | 1220      |
|    policy_gradient_loss  | -0.062    |
|    std                   | 0.0734    |
|    value_loss            | 0.292     |
----------------------------------------
--------------------------------------
| err_terms/              |          |
|    com_err              | 0.081    |
|    end_effectors_err    | 0.143    |
|    height_err           | -0.0105  |
|    joints_err           | 0.0998   |
|    joints_vel_err       | 83.5     |
|    root_ori_err         | 0.00595  |
| reward_terms/           |          |
|    com_reward           | 0.0897   |
|    end_effectors_reward | 0.0458   |
|    height_reward        | 0.0983   |
|    joints_reward        | 0.414    |
|    joints_vel_reward    | 0.000149 |
|    root_ori_reward      | 0.0218   |
|    smoothness1_reward   | 0        |
|    smoothness2_reward   | 0        |
|    smoothness_reward    | 0        |
| rollout/                |          |
|    ep_len_mean          | 391      |
|    ep_rew_mean          | 228      |
| time/                   |          |
|    collect_time         | 6.41e+03 |
|    fps                  | 196      |
|    iterations           | 123      |
|    time_elapsed         | 51151    |
|    total_timesteps      | 10076160 |
--------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.414      |
|    end_effectors_err     | 0.977      |
|    height_err            | -0.00196   |
|    joints_err            | 0.104      |
|    joints_vel_err        | 84.5       |
|    root_ori_err          | 0.0186     |
| reward_terms/            |            |
|    com_reward            | 0.0267     |
|    end_effectors_reward  | 0.0298     |
|    height_reward         | 0.0981     |
|    joints_reward         | 0.411      |
|    joints_vel_reward     | 0.000188   |
|    root_ori_reward       | 0.0254     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 406        |
|    ep_rew_mean           | 236        |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 198        |
|    iterations            | 124        |
|    time_elapsed          | 51298      |
|    total_timesteps       | 10158080   |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.86304873 |
|    clip_fraction         | 0.795      |
|    clip_range            | 0.2        |
|    entropy_loss          | 54         |
|    explained_variance    | 0.703      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.121      |
|    n_updates             | 1230       |
|    policy_gradient_loss  | -0.0614    |
|    std                   | 0.073      |
|    value_loss            | 0.298      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.553      |
|    end_effectors_err     | 1.94       |
|    height_err            | -0.00798   |
|    joints_err            | 0.105      |
|    joints_vel_err        | 92.8       |
|    root_ori_err          | 0.0482     |
| reward_terms/            |            |
|    com_reward            | 0.0237     |
|    end_effectors_reward  | 0.0218     |
|    height_reward         | 0.0976     |
|    joints_reward         | 0.411      |
|    joints_vel_reward     | 0.000163   |
|    root_ori_reward       | 0.00658    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 385        |
|    ep_rew_mean           | 224        |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 199        |
|    iterations            | 125        |
|    time_elapsed          | 51445      |
|    total_timesteps       | 10240000   |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.89711726 |
|    clip_fraction         | 0.797      |
|    clip_range            | 0.2        |
|    entropy_loss          | 54.2       |
|    explained_variance    | 0.707      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0138    |
|    n_updates             | 1240       |
|    policy_gradient_loss  | -0.0624    |
|    std                   | 0.0726     |
|    value_loss            | 0.27       |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.378      |
|    end_effectors_err     | 0.96       |
|    height_err            | -0.0148    |
|    joints_err            | 0.107      |
|    joints_vel_err        | 88.9       |
|    root_ori_err          | 0.0386     |
| reward_terms/            |            |
|    com_reward            | 0.0321     |
|    end_effectors_reward  | 0.0304     |
|    height_reward         | 0.0969     |
|    joints_reward         | 0.409      |
|    joints_vel_reward     | 0.000142   |
|    root_ori_reward       | 0.00449    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 386        |
|    ep_rew_mean           | 226        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 200        |
|    iterations            | 126        |
|    time_elapsed          | 51592      |
|    total_timesteps       | 10321920   |
|    train_time            | 32.2       |
| train/                   |            |
|    approx_kl             | 0.88839227 |
|    clip_fraction         | 0.793      |
|    clip_range            | 0.2        |
|    entropy_loss          | 54.5       |
|    explained_variance    | 0.638      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.17       |
|    n_updates             | 1250       |
|    policy_gradient_loss  | -0.0537    |
|    std                   | 0.0723     |
|    value_loss            | 0.387      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.457      |
|    end_effectors_err     | 1.31       |
|    height_err            | -0.00458   |
|    joints_err            | 0.107      |
|    joints_vel_err        | 98.5       |
|    root_ori_err          | 0.0216     |
| reward_terms/            |            |
|    com_reward            | 0.0287     |
|    end_effectors_reward  | 0.0267     |
|    height_reward         | 0.0974     |
|    joints_reward         | 0.41       |
|    joints_vel_reward     | 0.000122   |
|    root_ori_reward       | 0.0221     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 387        |
|    ep_rew_mean           | 227        |
| time/                    |            |
|    collect_time          | 116        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 201        |
|    iterations            | 127        |
|    time_elapsed          | 51740      |
|    total_timesteps       | 10403840   |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.87485754 |
|    clip_fraction         | 0.793      |
|    clip_range            | 0.2        |
|    entropy_loss          | 54.7       |
|    explained_variance    | 0.635      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0703    |
|    n_updates             | 1260       |
|    policy_gradient_loss  | -0.054     |
|    std                   | 0.072      |
|    value_loss            | 0.381      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.453      |
|    end_effectors_err     | 1.3        |
|    height_err            | -0.00198   |
|    joints_err            | 0.1        |
|    joints_vel_err        | 90         |
|    root_ori_err          | 0.0251     |
| reward_terms/            |            |
|    com_reward            | 0.0246     |
|    end_effectors_reward  | 0.0254     |
|    height_reward         | 0.098      |
|    joints_reward         | 0.414      |
|    joints_vel_reward     | 0.00017    |
|    root_ori_reward       | 0.0144     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 407        |
|    ep_rew_mean           | 238        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 202        |
|    iterations            | 128        |
|    time_elapsed          | 51886      |
|    total_timesteps       | 10485760   |
|    train_time            | 31.5       |
| train/                   |            |
|    approx_kl             | 0.92098236 |
|    clip_fraction         | 0.796      |
|    clip_range            | 0.2        |
|    entropy_loss          | 54.9       |
|    explained_variance    | 0.692      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.203      |
|    n_updates             | 1270       |
|    policy_gradient_loss  | -0.0569    |
|    std                   | 0.0716     |
|    value_loss            | 0.319      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.419      |
|    end_effectors_err     | 1.1        |
|    height_err            | -0.0161    |
|    joints_err            | 0.102      |
|    joints_vel_err        | 91.2       |
|    root_ori_err          | 0.0359     |
| reward_terms/            |            |
|    com_reward            | 0.031      |
|    end_effectors_reward  | 0.0296     |
|    height_reward         | 0.0968     |
|    joints_reward         | 0.413      |
|    joints_vel_reward     | 0.000173   |
|    root_ori_reward       | 0.0152     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 403        |
|    ep_rew_mean           | 236        |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 203        |
|    iterations            | 129        |
|    time_elapsed          | 52033      |
|    total_timesteps       | 10567680   |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.98710406 |
|    clip_fraction         | 0.803      |
|    clip_range            | 0.2        |
|    entropy_loss          | 55.2       |
|    explained_variance    | 0.78       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.175      |
|    n_updates             | 1280       |
|    policy_gradient_loss  | -0.0674    |
|    std                   | 0.0713     |
|    value_loss            | 0.21       |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.411      |
|    end_effectors_err     | 1.1        |
|    height_err            | -0.00148   |
|    joints_err            | 0.102      |
|    joints_vel_err        | 89.9       |
|    root_ori_err          | 0.0186     |
| reward_terms/            |            |
|    com_reward            | 0.0341     |
|    end_effectors_reward  | 0.0298     |
|    height_reward         | 0.098      |
|    joints_reward         | 0.413      |
|    joints_vel_reward     | 0.00017    |
|    root_ori_reward       | 0.0174     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 402        |
|    ep_rew_mean           | 236        |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00199    |
|    fps                   | 204        |
|    iterations            | 130        |
|    time_elapsed          | 52179      |
|    total_timesteps       | 10649600   |
|    train_time            | 31.4       |
| train/                   |            |
|    approx_kl             | 0.89793146 |
|    clip_fraction         | 0.796      |
|    clip_range            | 0.2        |
|    entropy_loss          | 55.4       |
|    explained_variance    | 0.711      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0477    |
|    n_updates             | 1290       |
|    policy_gradient_loss  | -0.0585    |
|    std                   | 0.0709     |
|    value_loss            | 0.311      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.414     |
|    end_effectors_err     | 1.07      |
|    height_err            | 0.00178   |
|    joints_err            | 0.0976    |
|    joints_vel_err        | 86.9      |
|    root_ori_err          | 0.0254    |
| reward_terms/            |           |
|    com_reward            | 0.0296    |
|    end_effectors_reward  | 0.0294    |
|    height_reward         | 0.0979    |
|    joints_reward         | 0.416     |
|    joints_vel_reward     | 0.000177  |
|    root_ori_reward       | 0.0156    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 403       |
|    ep_rew_mean           | 237       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00205   |
|    fps                   | 205       |
|    iterations            | 131       |
|    time_elapsed          | 52325     |
|    total_timesteps       | 10731520  |
|    train_time            | 32.2      |
| train/                   |           |
|    approx_kl             | 0.9644302 |
|    clip_fraction         | 0.798     |
|    clip_range            | 0.2       |
|    entropy_loss          | 55.7      |
|    explained_variance    | 0.689     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.133     |
|    n_updates             | 1300      |
|    policy_gradient_loss  | -0.0589   |
|    std                   | 0.0705    |
|    value_loss            | 0.313     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.482     |
|    end_effectors_err     | 1.56      |
|    height_err            | -0.00679  |
|    joints_err            | 0.0976    |
|    joints_vel_err        | 85        |
|    root_ori_err          | 0.0555    |
| reward_terms/            |           |
|    com_reward            | 0.024     |
|    end_effectors_reward  | 0.0239    |
|    height_reward         | 0.0973    |
|    joints_reward         | 0.416     |
|    joints_vel_reward     | 0.00018   |
|    root_ori_reward       | 0.00118   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 407       |
|    ep_rew_mean           | 239       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 206       |
|    iterations            | 132       |
|    time_elapsed          | 52472     |
|    total_timesteps       | 10813440  |
|    train_time            | 31.5      |
| train/                   |           |
|    approx_kl             | 0.9760893 |
|    clip_fraction         | 0.796     |
|    clip_range            | 0.2       |
|    entropy_loss          | 56        |
|    explained_variance    | 0.644     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0342    |
|    n_updates             | 1310      |
|    policy_gradient_loss  | -0.0547   |
|    std                   | 0.0701    |
|    value_loss            | 0.391     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.353     |
|    end_effectors_err     | 1.05      |
|    height_err            | -0.00917  |
|    joints_err            | 0.0971    |
|    joints_vel_err        | 91.4      |
|    root_ori_err          | 0.046     |
| reward_terms/            |           |
|    com_reward            | 0.0384    |
|    end_effectors_reward  | 0.0295    |
|    height_reward         | 0.0972    |
|    joints_reward         | 0.416     |
|    joints_vel_reward     | 0.000127  |
|    root_ori_reward       | 0.0021    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 411       |
|    ep_rew_mean           | 241       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00198   |
|    fps                   | 207       |
|    iterations            | 133       |
|    time_elapsed          | 52618     |
|    total_timesteps       | 10895360  |
|    train_time            | 31        |
| train/                   |           |
|    approx_kl             | 1.0229595 |
|    clip_fraction         | 0.804     |
|    clip_range            | 0.2       |
|    entropy_loss          | 56.2      |
|    explained_variance    | 0.719     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0107    |
|    n_updates             | 1320      |
|    policy_gradient_loss  | -0.0635   |
|    std                   | 0.0698    |
|    value_loss            | 0.263     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.465      |
|    end_effectors_err     | 1.36       |
|    height_err            | 0.00496    |
|    joints_err            | 0.102      |
|    joints_vel_err        | 86.8       |
|    root_ori_err          | 0.0229     |
| reward_terms/            |            |
|    com_reward            | 0.0273     |
|    end_effectors_reward  | 0.0259     |
|    height_reward         | 0.0979     |
|    joints_reward         | 0.413      |
|    joints_vel_reward     | 0.000183   |
|    root_ori_reward       | 0.0184     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 412        |
|    ep_rew_mean           | 244        |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 208        |
|    iterations            | 134        |
|    time_elapsed          | 52764      |
|    total_timesteps       | 10977280   |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.99337655 |
|    clip_fraction         | 0.803      |
|    clip_range            | 0.2        |
|    entropy_loss          | 56.4       |
|    explained_variance    | 0.694      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0304    |
|    n_updates             | 1330       |
|    policy_gradient_loss  | -0.064     |
|    std                   | 0.0694     |
|    value_loss            | 0.285      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.218     |
|    end_effectors_err     | 0.364     |
|    height_err            | -0.0119   |
|    joints_err            | 0.0994    |
|    joints_vel_err        | 88        |
|    root_ori_err          | 0.0358    |
| reward_terms/            |           |
|    com_reward            | 0.0573    |
|    end_effectors_reward  | 0.0405    |
|    height_reward         | 0.0976    |
|    joints_reward         | 0.415     |
|    joints_vel_reward     | 0.00019   |
|    root_ori_reward       | 0.0343    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 404       |
|    ep_rew_mean           | 240       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00198   |
|    fps                   | 209       |
|    iterations            | 135       |
|    time_elapsed          | 52910     |
|    total_timesteps       | 11059200  |
|    train_time            | 31        |
| train/                   |           |
|    approx_kl             | 0.9630079 |
|    clip_fraction         | 0.801     |
|    clip_range            | 0.2       |
|    entropy_loss          | 56.7      |
|    explained_variance    | 0.713     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.172     |
|    n_updates             | 1340      |
|    policy_gradient_loss  | -0.06     |
|    std                   | 0.069     |
|    value_loss            | 0.301     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.313      |
|    end_effectors_err     | 0.661      |
|    height_err            | -0.00219   |
|    joints_err            | 0.0936     |
|    joints_vel_err        | 87.7       |
|    root_ori_err          | 0.0275     |
| reward_terms/            |            |
|    com_reward            | 0.0423     |
|    end_effectors_reward  | 0.0352     |
|    height_reward         | 0.0976     |
|    joints_reward         | 0.419      |
|    joints_vel_reward     | 0.000211   |
|    root_ori_reward       | 0.0251     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 385        |
|    ep_rew_mean           | 229        |
| time/                    |            |
|    collect_time          | 116        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 209        |
|    iterations            | 136        |
|    time_elapsed          | 53058      |
|    total_timesteps       | 11141120   |
|    train_time            | 32         |
| train/                   |            |
|    approx_kl             | 0.95846975 |
|    clip_fraction         | 0.802      |
|    clip_range            | 0.2        |
|    entropy_loss          | 56.9       |
|    explained_variance    | 0.715      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.301      |
|    n_updates             | 1350       |
|    policy_gradient_loss  | -0.0598    |
|    std                   | 0.0686     |
|    value_loss            | 0.298      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.0452    |
|    end_effectors_err     | 0.037     |
|    height_err            | 0.0115    |
|    joints_err            | 0.0677    |
|    joints_vel_err        | 91        |
|    root_ori_err          | 0.00234   |
| reward_terms/            |           |
|    com_reward            | 0.0971    |
|    end_effectors_reward  | 0.0489    |
|    height_reward         | 0.0989    |
|    joints_reward         | 0.44      |
|    joints_vel_reward     | 0.000152  |
|    root_ori_reward       | 0.043     |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 404       |
|    ep_rew_mean           | 240       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.002     |
|    fps                   | 210       |
|    iterations            | 137       |
|    time_elapsed          | 53205     |
|    total_timesteps       | 11223040  |
|    train_time            | 31.4      |
| train/                   |           |
|    approx_kl             | 0.9412233 |
|    clip_fraction         | 0.8       |
|    clip_range            | 0.2       |
|    entropy_loss          | 57.2      |
|    explained_variance    | 0.695     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.215     |
|    n_updates             | 1360      |
|    policy_gradient_loss  | -0.055    |
|    std                   | 0.0683    |
|    value_loss            | 0.347     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.397     |
|    end_effectors_err     | 0.987     |
|    height_err            | -0.0132   |
|    joints_err            | 0.0898    |
|    joints_vel_err        | 82.8      |
|    root_ori_err          | 0.0316    |
| reward_terms/            |           |
|    com_reward            | 0.0329    |
|    end_effectors_reward  | 0.0307    |
|    height_reward         | 0.0974    |
|    joints_reward         | 0.422     |
|    joints_vel_reward     | 0.000243  |
|    root_ori_reward       | 0.028     |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 401       |
|    ep_rew_mean           | 239       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00206   |
|    fps                   | 211       |
|    iterations            | 138       |
|    time_elapsed          | 53351     |
|    total_timesteps       | 11304960  |
|    train_time            | 32.1      |
| train/                   |           |
|    approx_kl             | 0.9999822 |
|    clip_fraction         | 0.801     |
|    clip_range            | 0.2       |
|    entropy_loss          | 57.4      |
|    explained_variance    | 0.708     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.15      |
|    n_updates             | 1370      |
|    policy_gradient_loss  | -0.0602   |
|    std                   | 0.068     |
|    value_loss            | 0.299     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.344     |
|    end_effectors_err     | 0.769     |
|    height_err            | -0.00307  |
|    joints_err            | 0.0969    |
|    joints_vel_err        | 85        |
|    root_ori_err          | 0.0258    |
| reward_terms/            |           |
|    com_reward            | 0.0389    |
|    end_effectors_reward  | 0.0334    |
|    height_reward         | 0.0971    |
|    joints_reward         | 0.416     |
|    joints_vel_reward     | 0.000197  |
|    root_ori_reward       | 0.0169    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 403       |
|    ep_rew_mean           | 240       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 212       |
|    iterations            | 139       |
|    time_elapsed          | 53497     |
|    total_timesteps       | 11386880  |
|    train_time            | 31.5      |
| train/                   |           |
|    approx_kl             | 1.0260403 |
|    clip_fraction         | 0.806     |
|    clip_range            | 0.2       |
|    entropy_loss          | 57.6      |
|    explained_variance    | 0.761     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.156     |
|    n_updates             | 1380      |
|    policy_gradient_loss  | -0.0681   |
|    std                   | 0.0677    |
|    value_loss            | 0.221     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.393     |
|    end_effectors_err     | 0.923     |
|    height_err            | -0.0104   |
|    joints_err            | 0.089     |
|    joints_vel_err        | 84.3      |
|    root_ori_err          | 0.0291    |
| reward_terms/            |           |
|    com_reward            | 0.0286    |
|    end_effectors_reward  | 0.0304    |
|    height_reward         | 0.0975    |
|    joints_reward         | 0.422     |
|    joints_vel_reward     | 0.000234  |
|    root_ori_reward       | 0.0293    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 418       |
|    ep_rew_mean           | 247       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 213       |
|    iterations            | 140       |
|    time_elapsed          | 53643     |
|    total_timesteps       | 11468800  |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 1.0516573 |
|    clip_fraction         | 0.8       |
|    clip_range            | 0.2       |
|    entropy_loss          | 57.8      |
|    explained_variance    | 0.711     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.111     |
|    n_updates             | 1390      |
|    policy_gradient_loss  | -0.0581   |
|    std                   | 0.0673    |
|    value_loss            | 0.298     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.441     |
|    end_effectors_err     | 1.23      |
|    height_err            | 0.003     |
|    joints_err            | 0.0965    |
|    joints_vel_err        | 79.5      |
|    root_ori_err          | 0.0325    |
| reward_terms/            |           |
|    com_reward            | 0.0254    |
|    end_effectors_reward  | 0.0271    |
|    height_reward         | 0.0982    |
|    joints_reward         | 0.417     |
|    joints_vel_reward     | 0.000282  |
|    root_ori_reward       | 0.00773   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 406       |
|    ep_rew_mean           | 243       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 214       |
|    iterations            | 141       |
|    time_elapsed          | 53789     |
|    total_timesteps       | 11550720  |
|    train_time            | 31.5      |
| train/                   |           |
|    approx_kl             | 1.0669897 |
|    clip_fraction         | 0.804     |
|    clip_range            | 0.2       |
|    entropy_loss          | 58.1      |
|    explained_variance    | 0.737     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0692   |
|    n_updates             | 1400      |
|    policy_gradient_loss  | -0.0629   |
|    std                   | 0.067     |
|    value_loss            | 0.249     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.395     |
|    end_effectors_err     | 0.887     |
|    height_err            | -0.000363 |
|    joints_err            | 0.0919    |
|    joints_vel_err        | 82.6      |
|    root_ori_err          | 0.00587   |
| reward_terms/            |           |
|    com_reward            | 0.0262    |
|    end_effectors_reward  | 0.0305    |
|    height_reward         | 0.0984    |
|    joints_reward         | 0.42      |
|    joints_vel_reward     | 0.00019   |
|    root_ori_reward       | 0.0309    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 407       |
|    ep_rew_mean           | 243       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 215       |
|    iterations            | 142       |
|    time_elapsed          | 53936     |
|    total_timesteps       | 11632640  |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 0.9986022 |
|    clip_fraction         | 0.803     |
|    clip_range            | 0.2       |
|    entropy_loss          | 58.3      |
|    explained_variance    | 0.694     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0798   |
|    n_updates             | 1410      |
|    policy_gradient_loss  | -0.0594   |
|    std                   | 0.0666    |
|    value_loss            | 0.306     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.595     |
|    end_effectors_err     | 1.91      |
|    height_err            | 0.00348   |
|    joints_err            | 0.0923    |
|    joints_vel_err        | 90.1      |
|    root_ori_err          | 0.0343    |
| reward_terms/            |           |
|    com_reward            | 0.0169    |
|    end_effectors_reward  | 0.0211    |
|    height_reward         | 0.098     |
|    joints_reward         | 0.42      |
|    joints_vel_reward     | 0.000203  |
|    root_ori_reward       | 0.0245    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 390       |
|    ep_rew_mean           | 234       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 216       |
|    iterations            | 143       |
|    time_elapsed          | 54083     |
|    total_timesteps       | 11714560  |
|    train_time            | 31.6      |
| train/                   |           |
|    approx_kl             | 1.0637684 |
|    clip_fraction         | 0.802     |
|    clip_range            | 0.2       |
|    entropy_loss          | 58.6      |
|    explained_variance    | 0.665     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0244   |
|    n_updates             | 1420      |
|    policy_gradient_loss  | -0.0535   |
|    std                   | 0.0663    |
|    value_loss            | 0.358     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.488     |
|    end_effectors_err     | 1.33      |
|    height_err            | -0.0115   |
|    joints_err            | 0.09      |
|    joints_vel_err        | 86.9      |
|    root_ori_err          | 0.0198    |
| reward_terms/            |           |
|    com_reward            | 0.0196    |
|    end_effectors_reward  | 0.0258    |
|    height_reward         | 0.097     |
|    joints_reward         | 0.422     |
|    joints_vel_reward     | 0.00026   |
|    root_ori_reward       | 0.0172    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 400       |
|    ep_rew_mean           | 239       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 217       |
|    iterations            | 144       |
|    time_elapsed          | 54230     |
|    total_timesteps       | 11796480  |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 1.0424008 |
|    clip_fraction         | 0.802     |
|    clip_range            | 0.2       |
|    entropy_loss          | 58.8      |
|    explained_variance    | 0.669     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.198     |
|    n_updates             | 1430      |
|    policy_gradient_loss  | -0.0539   |
|    std                   | 0.066     |
|    value_loss            | 0.357     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.465     |
|    end_effectors_err     | 1.3       |
|    height_err            | -0.00648  |
|    joints_err            | 0.094     |
|    joints_vel_err        | 84.5      |
|    root_ori_err          | 0.0231    |
| reward_terms/            |           |
|    com_reward            | 0.0273    |
|    end_effectors_reward  | 0.0269    |
|    height_reward         | 0.0978    |
|    joints_reward         | 0.419     |
|    joints_vel_reward     | 0.000272  |
|    root_ori_reward       | 0.0237    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 383       |
|    ep_rew_mean           | 231       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 218       |
|    iterations            | 145       |
|    time_elapsed          | 54378     |
|    total_timesteps       | 11878400  |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 1.1104519 |
|    clip_fraction         | 0.804     |
|    clip_range            | 0.2       |
|    entropy_loss          | 59        |
|    explained_variance    | 0.7       |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0631   |
|    n_updates             | 1440      |
|    policy_gradient_loss  | -0.0567   |
|    std                   | 0.0657    |
|    value_loss            | 0.317     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.481     |
|    end_effectors_err     | 1.74      |
|    height_err            | -0.00165  |
|    joints_err            | 0.0951    |
|    joints_vel_err        | 80        |
|    root_ori_err          | 0.0467    |
| reward_terms/            |           |
|    com_reward            | 0.0289    |
|    end_effectors_reward  | 0.024     |
|    height_reward         | 0.098     |
|    joints_reward         | 0.418     |
|    joints_vel_reward     | 0.000255  |
|    root_ori_reward       | 0.0126    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 411       |
|    ep_rew_mean           | 248       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.002     |
|    fps                   | 219       |
|    iterations            | 146       |
|    time_elapsed          | 54524     |
|    total_timesteps       | 11960320  |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 1.0886395 |
|    clip_fraction         | 0.803     |
|    clip_range            | 0.2       |
|    entropy_loss          | 59.2      |
|    explained_variance    | 0.678     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.239     |
|    n_updates             | 1450      |
|    policy_gradient_loss  | -0.0528   |
|    std                   | 0.0654    |
|    value_loss            | 0.371     |
----------------------------------------
policy converted successfully and saved.
Saving video to /cluster/home/anghosh/DHProject/deep-mimic/log/2023-05-29-22-14-49-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M-residual_1/eval-video-PylocoVanilla-v0-step-13480-to-step-14480.mp4
Eval num_timesteps=12000000, episode_reward=300.08 +/- 0.00
Episode length: 527.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.426     |
|    end_effectors_err     | 1.11      |
|    height_err            | 0.00335   |
|    joints_err            | 0.09      |
|    joints_vel_err        | 80.9      |
|    root_ori_err          | 0.0176    |
| eval/                    |           |
|    mean_ep_length        | 527       |
|    mean_reward           | 300       |
|    success_rate          | 1         |
| reward_terms/            |           |
|    com_reward            | 0.0304    |
|    end_effectors_reward  | 0.029     |
|    height_reward         | 0.0985    |
|    joints_reward         | 0.422     |
|    joints_vel_reward     | 0.000275  |
|    root_ori_reward       | 0.037     |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| time/                    |           |
|    evaluate_actions_time | 0.00205   |
|    total_timesteps       | 12000000  |
|    train_time            | 32.2      |
| train/                   |           |
|    approx_kl             | 1.1022799 |
|    clip_fraction         | 0.803     |
|    clip_range            | 0.2       |
|    entropy_loss          | 59.4      |
|    explained_variance    | 0.702     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.148     |
|    n_updates             | 1460      |
|    policy_gradient_loss  | -0.0558   |
|    std                   | 0.0651    |
|    value_loss            | 0.328     |
----------------------------------------
New best mean reward!
--------------------------------------
| err_terms/              |          |
|    com_err              | 0.46     |
|    end_effectors_err    | 1.29     |
|    height_err           | 0.00222  |
|    joints_err           | 0.0876   |
|    joints_vel_err       | 76.4     |
|    root_ori_err         | 0.0179   |
| reward_terms/           |          |
|    com_reward           | 0.0268   |
|    end_effectors_reward | 0.0267   |
|    height_reward        | 0.0978   |
|    joints_reward        | 0.423    |
|    joints_vel_reward    | 0.000308 |
|    root_ori_reward      | 0.034    |
|    smoothness1_reward   | 0        |
|    smoothness2_reward   | 0        |
|    smoothness_reward    | 0        |
| rollout/                |          |
|    ep_len_mean          | 400      |
|    ep_rew_mean          | 241      |
| time/                   |          |
|    collect_time         | 6.39e+03 |
|    fps                  | 197      |
|    iterations           | 147      |
|    time_elapsed         | 60948    |
|    total_timesteps      | 12042240 |
--------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.328     |
|    end_effectors_err     | 0.674     |
|    height_err            | 0.000798  |
|    joints_err            | 0.0943    |
|    joints_vel_err        | 81.2      |
|    root_ori_err          | 0.0408    |
| reward_terms/            |           |
|    com_reward            | 0.0371    |
|    end_effectors_reward  | 0.0344    |
|    height_reward         | 0.0979    |
|    joints_reward         | 0.419     |
|    joints_vel_reward     | 0.000255  |
|    root_ori_reward       | 0.0247    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 392       |
|    ep_rew_mean           | 236       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 198       |
|    iterations            | 148       |
|    time_elapsed          | 61096     |
|    total_timesteps       | 12124160  |
|    train_time            | 31.6      |
| train/                   |           |
|    approx_kl             | 1.1057215 |
|    clip_fraction         | 0.804     |
|    clip_range            | 0.2       |
|    entropy_loss          | 59.7      |
|    explained_variance    | 0.675     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.283     |
|    n_updates             | 1470      |
|    policy_gradient_loss  | -0.0536   |
|    std                   | 0.0649    |
|    value_loss            | 0.368     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.424     |
|    end_effectors_err     | 1.14      |
|    height_err            | -0.00209  |
|    joints_err            | 0.0847    |
|    joints_vel_err        | 80.8      |
|    root_ori_err          | 0.0375    |
| reward_terms/            |           |
|    com_reward            | 0.0308    |
|    end_effectors_reward  | 0.0289    |
|    height_reward         | 0.0984    |
|    joints_reward         | 0.426     |
|    joints_vel_reward     | 0.000256  |
|    root_ori_reward       | 0.0235    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 397       |
|    ep_rew_mean           | 239       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 199       |
|    iterations            | 149       |
|    time_elapsed          | 61243     |
|    total_timesteps       | 12206080  |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 1.0849632 |
|    clip_fraction         | 0.805     |
|    clip_range            | 0.2       |
|    entropy_loss          | 59.9      |
|    explained_variance    | 0.664     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0283    |
|    n_updates             | 1480      |
|    policy_gradient_loss  | -0.0506   |
|    std                   | 0.0646    |
|    value_loss            | 0.372     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.341     |
|    end_effectors_err     | 0.696     |
|    height_err            | -0.00135  |
|    joints_err            | 0.091     |
|    joints_vel_err        | 77.2      |
|    root_ori_err          | 0.0163    |
| reward_terms/            |           |
|    com_reward            | 0.0356    |
|    end_effectors_reward  | 0.0342    |
|    height_reward         | 0.0977    |
|    joints_reward         | 0.421     |
|    joints_vel_reward     | 0.000359  |
|    root_ori_reward       | 0.03      |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 413       |
|    ep_rew_mean           | 248       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 200       |
|    iterations            | 150       |
|    time_elapsed          | 61389     |
|    total_timesteps       | 12288000  |
|    train_time            | 31.4      |
| train/                   |           |
|    approx_kl             | 1.2099947 |
|    clip_fraction         | 0.812     |
|    clip_range            | 0.2       |
|    entropy_loss          | 60        |
|    explained_variance    | 0.727     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.000738  |
|    n_updates             | 1490      |
|    policy_gradient_loss  | -0.0598   |
|    std                   | 0.0643    |
|    value_loss            | 0.289     |
----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 0.49     |
|    end_effectors_err     | 1.51     |
|    height_err            | -0.00191 |
|    joints_err            | 0.0867   |
|    joints_vel_err        | 88       |
|    root_ori_err          | 0.0974   |
| reward_terms/            |          |
|    com_reward            | 0.0223   |
|    end_effectors_reward  | 0.0225   |
|    height_reward         | 0.098    |
|    joints_reward         | 0.425    |
|    joints_vel_reward     | 0.000296 |
|    root_ori_reward       | 0.0121   |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 413      |
|    ep_rew_mean           | 248      |
| time/                    |          |
|    collect_time          | 113      |
|    evaluate_actions_time | 0.00202  |
|    fps                   | 201      |
|    iterations            | 151      |
|    time_elapsed          | 61534    |
|    total_timesteps       | 12369920 |
|    train_time            | 31.8     |
| train/                   |          |
|    approx_kl             | 1.106559 |
|    clip_fraction         | 0.81     |
|    clip_range            | 0.2      |
|    entropy_loss          | 60.3     |
|    explained_variance    | 0.721    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | -0.0273  |
|    n_updates             | 1500     |
|    policy_gradient_loss  | -0.0575  |
|    std                   | 0.0639   |
|    value_loss            | 0.301    |
---------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.444     |
|    end_effectors_err     | 1.48      |
|    height_err            | -0.00287  |
|    joints_err            | 0.0889    |
|    joints_vel_err        | 77.1      |
|    root_ori_err          | 0.0602    |
| reward_terms/            |           |
|    com_reward            | 0.0307    |
|    end_effectors_reward  | 0.0251    |
|    height_reward         | 0.0976    |
|    joints_reward         | 0.423     |
|    joints_vel_reward     | 0.000328  |
|    root_ori_reward       | 0.00976   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 408       |
|    ep_rew_mean           | 247       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 201       |
|    iterations            | 152       |
|    time_elapsed          | 61681     |
|    total_timesteps       | 12451840  |
|    train_time            | 31.5      |
| train/                   |           |
|    approx_kl             | 1.1635997 |
|    clip_fraction         | 0.807     |
|    clip_range            | 0.2       |
|    entropy_loss          | 60.5      |
|    explained_variance    | 0.679     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.00651   |
|    n_updates             | 1510      |
|    policy_gradient_loss  | -0.0541   |
|    std                   | 0.0636    |
|    value_loss            | 0.37      |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.0206    |
|    end_effectors_err     | 0.00383   |
|    height_err            | 0.00975   |
|    joints_err            | 0.0374    |
|    joints_vel_err        | 66.6      |
|    root_ori_err          | 0.00206   |
| reward_terms/            |           |
|    com_reward            | 0.0993    |
|    end_effectors_reward  | 0.0499    |
|    height_reward         | 0.0994    |
|    joints_reward         | 0.465     |
|    joints_vel_reward     | 0.000686  |
|    root_ori_reward       | 0.0497    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 389       |
|    ep_rew_mean           | 235       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 202       |
|    iterations            | 153       |
|    time_elapsed          | 61828     |
|    total_timesteps       | 12533760  |
|    train_time            | 31.3      |
| train/                   |           |
|    approx_kl             | 1.2212623 |
|    clip_fraction         | 0.81      |
|    clip_range            | 0.2       |
|    entropy_loss          | 60.8      |
|    explained_variance    | 0.727     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0246    |
|    n_updates             | 1520      |
|    policy_gradient_loss  | -0.0567   |
|    std                   | 0.0633    |
|    value_loss            | 0.301     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.478     |
|    end_effectors_err     | 1.48      |
|    height_err            | -0.00343  |
|    joints_err            | 0.0864    |
|    joints_vel_err        | 80.2      |
|    root_ori_err          | 0.0277    |
| reward_terms/            |           |
|    com_reward            | 0.0311    |
|    end_effectors_reward  | 0.0269    |
|    height_reward         | 0.0982    |
|    joints_reward         | 0.425     |
|    joints_vel_reward     | 0.000332  |
|    root_ori_reward       | 0.0273    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 379       |
|    ep_rew_mean           | 230       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 203       |
|    iterations            | 154       |
|    time_elapsed          | 61975     |
|    total_timesteps       | 12615680  |
|    train_time            | 31.6      |
| train/                   |           |
|    approx_kl             | 1.2038567 |
|    clip_fraction         | 0.805     |
|    clip_range            | 0.2       |
|    entropy_loss          | 61        |
|    explained_variance    | 0.672     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0158   |
|    n_updates             | 1530      |
|    policy_gradient_loss  | -0.0498   |
|    std                   | 0.063     |
|    value_loss            | 0.395     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.37      |
|    end_effectors_err     | 0.888     |
|    height_err            | 0.00362   |
|    joints_err            | 0.0845    |
|    joints_vel_err        | 81.6      |
|    root_ori_err          | 0.0232    |
| reward_terms/            |           |
|    com_reward            | 0.0301    |
|    end_effectors_reward  | 0.0307    |
|    height_reward         | 0.0986    |
|    joints_reward         | 0.426     |
|    joints_vel_reward     | 0.000255  |
|    root_ori_reward       | 0.0277    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 416       |
|    ep_rew_mean           | 251       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 204       |
|    iterations            | 155       |
|    time_elapsed          | 62121     |
|    total_timesteps       | 12697600  |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 1.2009336 |
|    clip_fraction         | 0.81      |
|    clip_range            | 0.2       |
|    entropy_loss          | 61.2      |
|    explained_variance    | 0.678     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0609   |
|    n_updates             | 1540      |
|    policy_gradient_loss  | -0.0516   |
|    std                   | 0.0627    |
|    value_loss            | 0.359     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.512     |
|    end_effectors_err     | 1.47      |
|    height_err            | 0.0001    |
|    joints_err            | 0.0821    |
|    joints_vel_err        | 76.8      |
|    root_ori_err          | 0.0178    |
| reward_terms/            |           |
|    com_reward            | 0.0217    |
|    end_effectors_reward  | 0.0254    |
|    height_reward         | 0.0984    |
|    joints_reward         | 0.428     |
|    joints_vel_reward     | 0.000333  |
|    root_ori_reward       | 0.0279    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 395       |
|    ep_rew_mean           | 241       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 205       |
|    iterations            | 156       |
|    time_elapsed          | 62269     |
|    total_timesteps       | 12779520  |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 1.5571263 |
|    clip_fraction         | 0.813     |
|    clip_range            | 0.2       |
|    entropy_loss          | 61.4      |
|    explained_variance    | 0.725     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0397   |
|    n_updates             | 1550      |
|    policy_gradient_loss  | -0.0594   |
|    std                   | 0.0624    |
|    value_loss            | 0.269     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.128     |
|    end_effectors_err     | 0.139     |
|    height_err            | -0.00994  |
|    joints_err            | 0.0873    |
|    joints_vel_err        | 93.9      |
|    root_ori_err          | 0.0414    |
| reward_terms/            |           |
|    com_reward            | 0.0808    |
|    end_effectors_reward  | 0.0459    |
|    height_reward         | 0.0989    |
|    joints_reward         | 0.424     |
|    joints_vel_reward     | 0.000174  |
|    root_ori_reward       | 0.0299    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 397       |
|    ep_rew_mean           | 243       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00205   |
|    fps                   | 206       |
|    iterations            | 157       |
|    time_elapsed          | 62416     |
|    total_timesteps       | 12861440  |
|    train_time            | 32        |
| train/                   |           |
|    approx_kl             | 1.2139987 |
|    clip_fraction         | 0.808     |
|    clip_range            | 0.2       |
|    entropy_loss          | 61.6      |
|    explained_variance    | 0.693     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.296     |
|    n_updates             | 1560      |
|    policy_gradient_loss  | -0.0543   |
|    std                   | 0.0621    |
|    value_loss            | 0.344     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.327     |
|    end_effectors_err     | 0.698     |
|    height_err            | -0.0104   |
|    joints_err            | 0.0819    |
|    joints_vel_err        | 82.2      |
|    root_ori_err          | 0.0637    |
| reward_terms/            |           |
|    com_reward            | 0.0404    |
|    end_effectors_reward  | 0.0345    |
|    height_reward         | 0.0986    |
|    joints_reward         | 0.428     |
|    joints_vel_reward     | 0.00039   |
|    root_ori_reward       | 0.038     |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 414       |
|    ep_rew_mean           | 252       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 206       |
|    iterations            | 158       |
|    time_elapsed          | 62562     |
|    total_timesteps       | 12943360  |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 1.2336725 |
|    clip_fraction         | 0.813     |
|    clip_range            | 0.2       |
|    entropy_loss          | 61.8      |
|    explained_variance    | 0.734     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0827   |
|    n_updates             | 1570      |
|    policy_gradient_loss  | -0.0575   |
|    std                   | 0.0619    |
|    value_loss            | 0.294     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.461     |
|    end_effectors_err     | 1.27      |
|    height_err            | 0.00542   |
|    joints_err            | 0.0815    |
|    joints_vel_err        | 73.5      |
|    root_ori_err          | 0.0251    |
| reward_terms/            |           |
|    com_reward            | 0.0248    |
|    end_effectors_reward  | 0.0274    |
|    height_reward         | 0.0985    |
|    joints_reward         | 0.429     |
|    joints_vel_reward     | 0.000424  |
|    root_ori_reward       | 0.0221    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 390       |
|    ep_rew_mean           | 239       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 207       |
|    iterations            | 159       |
|    time_elapsed          | 62709     |
|    total_timesteps       | 13025280  |
|    train_time            | 32        |
| train/                   |           |
|    approx_kl             | 1.3433065 |
|    clip_fraction         | 0.812     |
|    clip_range            | 0.2       |
|    entropy_loss          | 62.1      |
|    explained_variance    | 0.766     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.124     |
|    n_updates             | 1580      |
|    policy_gradient_loss  | -0.0609   |
|    std                   | 0.0616    |
|    value_loss            | 0.254     |
----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 0.507    |
|    end_effectors_err     | 1.48     |
|    height_err            | -0.00622 |
|    joints_err            | 0.0843   |
|    joints_vel_err        | 76.3     |
|    root_ori_err          | 0.0188   |
| reward_terms/            |          |
|    com_reward            | 0.0225   |
|    end_effectors_reward  | 0.0253   |
|    height_reward         | 0.098    |
|    joints_reward         | 0.426    |
|    joints_vel_reward     | 0.000373 |
|    root_ori_reward       | 0.0296   |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 424      |
|    ep_rew_mean           | 258      |
| time/                    |          |
|    collect_time          | 115      |
|    evaluate_actions_time | 0.00196  |
|    fps                   | 208      |
|    iterations            | 160      |
|    time_elapsed          | 62855    |
|    total_timesteps       | 13107200 |
|    train_time            | 30.7     |
| train/                   |          |
|    approx_kl             | 1.88344  |
|    clip_fraction         | 0.809    |
|    clip_range            | 0.2      |
|    entropy_loss          | 62.3     |
|    explained_variance    | 0.74     |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | -0.0818  |
|    n_updates             | 1590     |
|    policy_gradient_loss  | -0.0581  |
|    std                   | 0.0612   |
|    value_loss            | 0.295    |
---------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.345     |
|    end_effectors_err     | 0.955     |
|    height_err            | -0.000759 |
|    joints_err            | 0.0852    |
|    joints_vel_err        | 77.5      |
|    root_ori_err          | 0.0462    |
| reward_terms/            |           |
|    com_reward            | 0.036     |
|    end_effectors_reward  | 0.0298    |
|    height_reward         | 0.0979    |
|    joints_reward         | 0.426     |
|    joints_vel_reward     | 0.000428  |
|    root_ori_reward       | 0.00133   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 413       |
|    ep_rew_mean           | 252       |
| time/                    |           |
|    collect_time          | 113       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 209       |
|    iterations            | 161       |
|    time_elapsed          | 63000     |
|    total_timesteps       | 13189120  |
|    train_time            | 31.5      |
| train/                   |           |
|    approx_kl             | 1.3299354 |
|    clip_fraction         | 0.815     |
|    clip_range            | 0.2       |
|    entropy_loss          | 62.5      |
|    explained_variance    | 0.777     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0892    |
|    n_updates             | 1600      |
|    policy_gradient_loss  | -0.0654   |
|    std                   | 0.061     |
|    value_loss            | 0.215     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.356     |
|    end_effectors_err     | 0.793     |
|    height_err            | -0.00228  |
|    joints_err            | 0.0744    |
|    joints_vel_err        | 83.8      |
|    root_ori_err          | 0.0529    |
| reward_terms/            |           |
|    com_reward            | 0.0346    |
|    end_effectors_reward  | 0.0329    |
|    height_reward         | 0.0986    |
|    joints_reward         | 0.435     |
|    joints_vel_reward     | 0.000318  |
|    root_ori_reward       | 0.029     |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 413       |
|    ep_rew_mean           | 253       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00198   |
|    fps                   | 210       |
|    iterations            | 162       |
|    time_elapsed          | 63146     |
|    total_timesteps       | 13271040  |
|    train_time            | 31        |
| train/                   |           |
|    approx_kl             | 1.4104226 |
|    clip_fraction         | 0.814     |
|    clip_range            | 0.2       |
|    entropy_loss          | 62.8      |
|    explained_variance    | 0.762     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.155     |
|    n_updates             | 1610      |
|    policy_gradient_loss  | -0.0613   |
|    std                   | 0.0607    |
|    value_loss            | 0.24      |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.371     |
|    end_effectors_err     | 0.897     |
|    height_err            | -0.00566  |
|    joints_err            | 0.0837    |
|    joints_vel_err        | 74        |
|    root_ori_err          | 0.018     |
| reward_terms/            |           |
|    com_reward            | 0.0374    |
|    end_effectors_reward  | 0.032     |
|    height_reward         | 0.0982    |
|    joints_reward         | 0.427     |
|    joints_vel_reward     | 0.00049   |
|    root_ori_reward       | 0.0332    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 403       |
|    ep_rew_mean           | 248       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 210       |
|    iterations            | 163       |
|    time_elapsed          | 63292     |
|    total_timesteps       | 13352960  |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 1.2793127 |
|    clip_fraction         | 0.812     |
|    clip_range            | 0.2       |
|    entropy_loss          | 63        |
|    explained_variance    | 0.773     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.077    |
|    n_updates             | 1620      |
|    policy_gradient_loss  | -0.0618   |
|    std                   | 0.0603    |
|    value_loss            | 0.23      |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.264     |
|    end_effectors_err     | 0.487     |
|    height_err            | -0.00895  |
|    joints_err            | 0.0879    |
|    joints_vel_err        | 80.4      |
|    root_ori_err          | 0.0276    |
| reward_terms/            |           |
|    com_reward            | 0.0491    |
|    end_effectors_reward  | 0.038     |
|    height_reward         | 0.0982    |
|    joints_reward         | 0.424     |
|    joints_vel_reward     | 0.000254  |
|    root_ori_reward       | 0.0235    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 411       |
|    ep_rew_mean           | 252       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 211       |
|    iterations            | 164       |
|    time_elapsed          | 63440     |
|    total_timesteps       | 13434880  |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 1.2627374 |
|    clip_fraction         | 0.811     |
|    clip_range            | 0.2       |
|    entropy_loss          | 63.2      |
|    explained_variance    | 0.724     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0848   |
|    n_updates             | 1630      |
|    policy_gradient_loss  | -0.055    |
|    std                   | 0.0601    |
|    value_loss            | 0.311     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.464     |
|    end_effectors_err     | 1.27      |
|    height_err            | 0.00149   |
|    joints_err            | 0.0828    |
|    joints_vel_err        | 77.2      |
|    root_ori_err          | 0.0135    |
| reward_terms/            |           |
|    com_reward            | 0.0235    |
|    end_effectors_reward  | 0.0259    |
|    height_reward         | 0.0981    |
|    joints_reward         | 0.428     |
|    joints_vel_reward     | 0.000453  |
|    root_ori_reward       | 0.0349    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 423       |
|    ep_rew_mean           | 261       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 212       |
|    iterations            | 165       |
|    time_elapsed          | 63588     |
|    total_timesteps       | 13516800  |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 1.3438963 |
|    clip_fraction         | 0.816     |
|    clip_range            | 0.2       |
|    entropy_loss          | 63.4      |
|    explained_variance    | 0.751     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0267    |
|    n_updates             | 1640      |
|    policy_gradient_loss  | -0.0589   |
|    std                   | 0.0598    |
|    value_loss            | 0.272     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.263     |
|    end_effectors_err     | 0.525     |
|    height_err            | -0.00399  |
|    joints_err            | 0.0815    |
|    joints_vel_err        | 74.6      |
|    root_ori_err          | 0.0256    |
| reward_terms/            |           |
|    com_reward            | 0.0487    |
|    end_effectors_reward  | 0.0373    |
|    height_reward         | 0.0986    |
|    joints_reward         | 0.429     |
|    joints_vel_reward     | 0.000534  |
|    root_ori_reward       | 0.0272    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 401       |
|    ep_rew_mean           | 247       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 213       |
|    iterations            | 166       |
|    time_elapsed          | 63735     |
|    total_timesteps       | 13598720  |
|    train_time            | 32.1      |
| train/                   |           |
|    approx_kl             | 1.4141088 |
|    clip_fraction         | 0.815     |
|    clip_range            | 0.2       |
|    entropy_loss          | 63.7      |
|    explained_variance    | 0.761     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0587   |
|    n_updates             | 1650      |
|    policy_gradient_loss  | -0.0608   |
|    std                   | 0.0594    |
|    value_loss            | 0.261     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.502     |
|    end_effectors_err     | 1.45      |
|    height_err            | -0.0122   |
|    joints_err            | 0.0819    |
|    joints_vel_err        | 79.2      |
|    root_ori_err          | 0.0559    |
| reward_terms/            |           |
|    com_reward            | 0.0226    |
|    end_effectors_reward  | 0.025     |
|    height_reward         | 0.0972    |
|    joints_reward         | 0.428     |
|    joints_vel_reward     | 0.000378  |
|    root_ori_reward       | 0.0297    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 412       |
|    ep_rew_mean           | 255       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 214       |
|    iterations            | 167       |
|    time_elapsed          | 63881     |
|    total_timesteps       | 13680640  |
|    train_time            | 32.3      |
| train/                   |           |
|    approx_kl             | 1.3253574 |
|    clip_fraction         | 0.815     |
|    clip_range            | 0.2       |
|    entropy_loss          | 63.9      |
|    explained_variance    | 0.769     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0719   |
|    n_updates             | 1660      |
|    policy_gradient_loss  | -0.06     |
|    std                   | 0.0591    |
|    value_loss            | 0.253     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.342     |
|    end_effectors_err     | 0.772     |
|    height_err            | -0.00655  |
|    joints_err            | 0.0803    |
|    joints_vel_err        | 68.8      |
|    root_ori_err          | 0.0325    |
| reward_terms/            |           |
|    com_reward            | 0.0378    |
|    end_effectors_reward  | 0.0336    |
|    height_reward         | 0.0982    |
|    joints_reward         | 0.43      |
|    joints_vel_reward     | 0.000535  |
|    root_ori_reward       | 0.0304    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 420       |
|    ep_rew_mean           | 258       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 214       |
|    iterations            | 168       |
|    time_elapsed          | 64029     |
|    total_timesteps       | 13762560  |
|    train_time            | 32.1      |
| train/                   |           |
|    approx_kl             | 1.4370133 |
|    clip_fraction         | 0.818     |
|    clip_range            | 0.2       |
|    entropy_loss          | 64.2      |
|    explained_variance    | 0.796     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0929   |
|    n_updates             | 1670      |
|    policy_gradient_loss  | -0.0657   |
|    std                   | 0.0587    |
|    value_loss            | 0.193     |
----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 0.403    |
|    end_effectors_err     | 0.99     |
|    height_err            | 0.00307  |
|    joints_err            | 0.0734   |
|    joints_vel_err        | 68.6     |
|    root_ori_err          | 0.0246   |
| reward_terms/            |          |
|    com_reward            | 0.0274   |
|    end_effectors_reward  | 0.0296   |
|    height_reward         | 0.0987   |
|    joints_reward         | 0.435    |
|    joints_vel_reward     | 0.00054  |
|    root_ori_reward       | 0.0217   |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 403      |
|    ep_rew_mean           | 250      |
| time/                    |          |
|    collect_time          | 114      |
|    evaluate_actions_time | 0.00202  |
|    fps                   | 215      |
|    iterations            | 169      |
|    time_elapsed          | 64175    |
|    total_timesteps       | 13844480 |
|    train_time            | 31.9     |
| train/                   |          |
|    approx_kl             | 1.431801 |
|    clip_fraction         | 0.818    |
|    clip_range            | 0.2      |
|    entropy_loss          | 64.5     |
|    explained_variance    | 0.833    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | -0.0766  |
|    n_updates             | 1680     |
|    policy_gradient_loss  | -0.0732  |
|    std                   | 0.0584   |
|    value_loss            | 0.155    |
---------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.366     |
|    end_effectors_err     | 0.838     |
|    height_err            | 0.00354   |
|    joints_err            | 0.0783    |
|    joints_vel_err        | 68        |
|    root_ori_err          | 0.0226    |
| reward_terms/            |           |
|    com_reward            | 0.0331    |
|    end_effectors_reward  | 0.0318    |
|    height_reward         | 0.0988    |
|    joints_reward         | 0.431     |
|    joints_vel_reward     | 0.000519  |
|    root_ori_reward       | 0.0411    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 422       |
|    ep_rew_mean           | 260       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 216       |
|    iterations            | 170       |
|    time_elapsed          | 64321     |
|    total_timesteps       | 13926400  |
|    train_time            | 31.2      |
| train/                   |           |
|    approx_kl             | 1.8723732 |
|    clip_fraction         | 0.815     |
|    clip_range            | 0.2       |
|    entropy_loss          | 64.7      |
|    explained_variance    | 0.776     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0676    |
|    n_updates             | 1690      |
|    policy_gradient_loss  | -0.0614   |
|    std                   | 0.0582    |
|    value_loss            | 0.233     |
----------------------------------------
policy converted successfully and saved.
Saving video to /cluster/home/anghosh/DHProject/deep-mimic/log/2023-05-29-22-14-49-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M-residual_1/eval-video-PylocoVanilla-v0-step-16115-to-step-17115.mp4
Eval num_timesteps=14000000, episode_reward=292.98 +/- 0.00
Episode length: 527.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.375     |
|    end_effectors_err     | 0.879     |
|    height_err            | 0.000359  |
|    joints_err            | 0.0745    |
|    joints_vel_err        | 70.2      |
|    root_ori_err          | 0.0321    |
| eval/                    |           |
|    mean_ep_length        | 527       |
|    mean_reward           | 293       |
|    success_rate          | 1         |
| reward_terms/            |           |
|    com_reward            | 0.0346    |
|    end_effectors_reward  | 0.0322    |
|    height_reward         | 0.0988    |
|    joints_reward         | 0.434     |
|    joints_vel_reward     | 0.000645  |
|    root_ori_reward       | 0.0231    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| time/                    |           |
|    evaluate_actions_time | 0.00203   |
|    total_timesteps       | 14000000  |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 1.4013952 |
|    clip_fraction         | 0.819     |
|    clip_range            | 0.2       |
|    entropy_loss          | 65        |
|    explained_variance    | 0.8       |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.089    |
|    n_updates             | 1700      |
|    policy_gradient_loss  | -0.0657   |
|    std                   | 0.0578    |
|    value_loss            | 0.187     |
----------------------------------------
--------------------------------------
| err_terms/              |          |
|    com_err              | 0.356    |
|    end_effectors_err    | 0.799    |
|    height_err           | 0.000618 |
|    joints_err           | 0.0801   |
|    joints_vel_err       | 75.2     |
|    root_ori_err         | 0.0368   |
| reward_terms/           |          |
|    com_reward           | 0.0324   |
|    end_effectors_reward | 0.0323   |
|    height_reward        | 0.0986   |
|    joints_reward        | 0.43     |
|    joints_vel_reward    | 0.000499 |
|    root_ori_reward      | 0.0235   |
|    smoothness1_reward   | 0        |
|    smoothness2_reward   | 0        |
|    smoothness_reward    | 0        |
| rollout/                |          |
|    ep_len_mean          | 431      |
|    ep_rew_mean          | 266      |
| time/                   |          |
|    collect_time         | 6.4e+03  |
|    fps                  | 197      |
|    iterations           | 171      |
|    time_elapsed         | 70750    |
|    total_timesteps      | 14008320 |
--------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.281     |
|    end_effectors_err     | 0.526     |
|    height_err            | -0.00494  |
|    joints_err            | 0.0736    |
|    joints_vel_err        | 68.8      |
|    root_ori_err          | 0.0163    |
| reward_terms/            |           |
|    com_reward            | 0.0462    |
|    end_effectors_reward  | 0.0374    |
|    height_reward         | 0.0986    |
|    joints_reward         | 0.435     |
|    joints_vel_reward     | 0.00068   |
|    root_ori_reward       | 0.0369    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 412       |
|    ep_rew_mean           | 255       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 198       |
|    iterations            | 172       |
|    time_elapsed          | 70896     |
|    total_timesteps       | 14090240  |
|    train_time            | 31.1      |
| train/                   |           |
|    approx_kl             | 1.5739152 |
|    clip_fraction         | 0.821     |
|    clip_range            | 0.2       |
|    entropy_loss          | 65.2      |
|    explained_variance    | 0.829     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0718   |
|    n_updates             | 1710      |
|    policy_gradient_loss  | -0.071    |
|    std                   | 0.0576    |
|    value_loss            | 0.156     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.313     |
|    end_effectors_err     | 0.76      |
|    height_err            | -0.00317  |
|    joints_err            | 0.0774    |
|    joints_vel_err        | 75.7      |
|    root_ori_err          | 0.0344    |
| reward_terms/            |           |
|    com_reward            | 0.0424    |
|    end_effectors_reward  | 0.0337    |
|    height_reward         | 0.0986    |
|    joints_reward         | 0.432     |
|    joints_vel_reward     | 0.000366  |
|    root_ori_reward       | 0.0161    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 416       |
|    ep_rew_mean           | 258       |
| time/                    |           |
|    collect_time          | 117       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 199       |
|    iterations            | 173       |
|    time_elapsed          | 71045     |
|    total_timesteps       | 14172160  |
|    train_time            | 31.5      |
| train/                   |           |
|    approx_kl             | 1.4136791 |
|    clip_fraction         | 0.818     |
|    clip_range            | 0.2       |
|    entropy_loss          | 65.4      |
|    explained_variance    | 0.792     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0768   |
|    n_updates             | 1720      |
|    policy_gradient_loss  | -0.064    |
|    std                   | 0.0572    |
|    value_loss            | 0.208     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.462     |
|    end_effectors_err     | 1.2       |
|    height_err            | -0.00827  |
|    joints_err            | 0.0736    |
|    joints_vel_err        | 68.7      |
|    root_ori_err          | 0.0295    |
| reward_terms/            |           |
|    com_reward            | 0.0242    |
|    end_effectors_reward  | 0.0277    |
|    height_reward         | 0.0979    |
|    joints_reward         | 0.435     |
|    joints_vel_reward     | 0.000651  |
|    root_ori_reward       | 0.0343    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 397       |
|    ep_rew_mean           | 249       |
| time/                    |           |
|    collect_time          | 113       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 200       |
|    iterations            | 174       |
|    time_elapsed          | 71191     |
|    total_timesteps       | 14254080  |
|    train_time            | 32        |
| train/                   |           |
|    approx_kl             | 1.6303526 |
|    clip_fraction         | 0.818     |
|    clip_range            | 0.2       |
|    entropy_loss          | 65.7      |
|    explained_variance    | 0.784     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0815   |
|    n_updates             | 1730      |
|    policy_gradient_loss  | -0.0664   |
|    std                   | 0.0569    |
|    value_loss            | 0.193     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.477     |
|    end_effectors_err     | 1.33      |
|    height_err            | -0.00338  |
|    joints_err            | 0.0756    |
|    joints_vel_err        | 67.7      |
|    root_ori_err          | 0.0268    |
| reward_terms/            |           |
|    com_reward            | 0.0261    |
|    end_effectors_reward  | 0.0267    |
|    height_reward         | 0.0985    |
|    joints_reward         | 0.434     |
|    joints_vel_reward     | 0.000626  |
|    root_ori_reward       | 0.0243    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 419       |
|    ep_rew_mean           | 262       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 200       |
|    iterations            | 175       |
|    time_elapsed          | 71338     |
|    total_timesteps       | 14336000  |
|    train_time            | 31.6      |
| train/                   |           |
|    approx_kl             | 1.5250509 |
|    clip_fraction         | 0.816     |
|    clip_range            | 0.2       |
|    entropy_loss          | 65.9      |
|    explained_variance    | 0.752     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0765   |
|    n_updates             | 1740      |
|    policy_gradient_loss  | -0.058    |
|    std                   | 0.0567    |
|    value_loss            | 0.253     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.486     |
|    end_effectors_err     | 1.53      |
|    height_err            | -0.00698  |
|    joints_err            | 0.0777    |
|    joints_vel_err        | 66.7      |
|    root_ori_err          | 0.0653    |
| reward_terms/            |           |
|    com_reward            | 0.0255    |
|    end_effectors_reward  | 0.0241    |
|    height_reward         | 0.0986    |
|    joints_reward         | 0.432     |
|    joints_vel_reward     | 0.000685  |
|    root_ori_reward       | 0.0104    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 418       |
|    ep_rew_mean           | 260       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00198   |
|    fps                   | 201       |
|    iterations            | 176       |
|    time_elapsed          | 71485     |
|    total_timesteps       | 14417920  |
|    train_time            | 30.9      |
| train/                   |           |
|    approx_kl             | 1.5466087 |
|    clip_fraction         | 0.818     |
|    clip_range            | 0.2       |
|    entropy_loss          | 66.1      |
|    explained_variance    | 0.77      |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0856   |
|    n_updates             | 1750      |
|    policy_gradient_loss  | -0.0577   |
|    std                   | 0.0564    |
|    value_loss            | 0.251     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.314     |
|    end_effectors_err     | 0.643     |
|    height_err            | -0.00882  |
|    joints_err            | 0.0676    |
|    joints_vel_err        | 65.7      |
|    root_ori_err          | 0.0147    |
| reward_terms/            |           |
|    com_reward            | 0.0401    |
|    end_effectors_reward  | 0.0352    |
|    height_reward         | 0.098     |
|    joints_reward         | 0.44      |
|    joints_vel_reward     | 0.000891  |
|    root_ori_reward       | 0.0359    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 416       |
|    ep_rew_mean           | 260       |
| time/                    |           |
|    collect_time          | 117       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 202       |
|    iterations            | 177       |
|    time_elapsed          | 71634     |
|    total_timesteps       | 14499840  |
|    train_time            | 32        |
| train/                   |           |
|    approx_kl             | 1.6231279 |
|    clip_fraction         | 0.822     |
|    clip_range            | 0.2       |
|    entropy_loss          | 66.3      |
|    explained_variance    | 0.825     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0552   |
|    n_updates             | 1760      |
|    policy_gradient_loss  | -0.0675   |
|    std                   | 0.0562    |
|    value_loss            | 0.163     |
----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 0.226    |
|    end_effectors_err     | 0.444    |
|    height_err            | -0.00823 |
|    joints_err            | 0.0735   |
|    joints_vel_err        | 66.5     |
|    root_ori_err          | 0.0406   |
| reward_terms/            |          |
|    com_reward            | 0.0545   |
|    end_effectors_reward  | 0.0387   |
|    height_reward         | 0.0988   |
|    joints_reward         | 0.435    |
|    joints_vel_reward     | 0.000802 |
|    root_ori_reward       | 0.0266   |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 429      |
|    ep_rew_mean           | 265      |
| time/                    |          |
|    collect_time          | 115      |
|    evaluate_actions_time | 0.00203  |
|    fps                   | 203      |
|    iterations            | 178      |
|    time_elapsed          | 71781    |
|    total_timesteps       | 14581760 |
|    train_time            | 32       |
| train/                   |          |
|    approx_kl             | 1.522702 |
|    clip_fraction         | 0.817    |
|    clip_range            | 0.2      |
|    entropy_loss          | 66.6     |
|    explained_variance    | 0.761    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | 0.184    |
|    n_updates             | 1770     |
|    policy_gradient_loss  | -0.0604  |
|    std                   | 0.0558   |
|    value_loss            | 0.226    |
---------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.379     |
|    end_effectors_err     | 0.876     |
|    height_err            | -0.00653  |
|    joints_err            | 0.0864    |
|    joints_vel_err        | 75.2      |
|    root_ori_err          | 0.0461    |
| reward_terms/            |           |
|    com_reward            | 0.0332    |
|    end_effectors_reward  | 0.0317    |
|    height_reward         | 0.0978    |
|    joints_reward         | 0.425     |
|    joints_vel_reward     | 0.000618  |
|    root_ori_reward       | 0.032     |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 427       |
|    ep_rew_mean           | 267       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 203       |
|    iterations            | 179       |
|    time_elapsed          | 71928     |
|    total_timesteps       | 14663680  |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 1.5308769 |
|    clip_fraction         | 0.82      |
|    clip_range            | 0.2       |
|    entropy_loss          | 66.8      |
|    explained_variance    | 0.756     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0817   |
|    n_updates             | 1780      |
|    policy_gradient_loss  | -0.0612   |
|    std                   | 0.0556    |
|    value_loss            | 0.228     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.431     |
|    end_effectors_err     | 1.51      |
|    height_err            | -0.00644  |
|    joints_err            | 0.0802    |
|    joints_vel_err        | 75.9      |
|    root_ori_err          | 0.0603    |
| reward_terms/            |           |
|    com_reward            | 0.0307    |
|    end_effectors_reward  | 0.0248    |
|    height_reward         | 0.0982    |
|    joints_reward         | 0.43      |
|    joints_vel_reward     | 0.000613  |
|    root_ori_reward       | 0.00452   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 432       |
|    ep_rew_mean           | 269       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00199   |
|    fps                   | 204       |
|    iterations            | 180       |
|    time_elapsed          | 72073     |
|    total_timesteps       | 14745600  |
|    train_time            | 31.3      |
| train/                   |           |
|    approx_kl             | 1.6090683 |
|    clip_fraction         | 0.819     |
|    clip_range            | 0.2       |
|    entropy_loss          | 67        |
|    explained_variance    | 0.754     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0634   |
|    n_updates             | 1790      |
|    policy_gradient_loss  | -0.0567   |
|    std                   | 0.0553    |
|    value_loss            | 0.255     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.242     |
|    end_effectors_err     | 0.435     |
|    height_err            | -0.00575  |
|    joints_err            | 0.0706    |
|    joints_vel_err        | 62.3      |
|    root_ori_err          | 0.0257    |
| reward_terms/            |           |
|    com_reward            | 0.0531    |
|    end_effectors_reward  | 0.0392    |
|    height_reward         | 0.0982    |
|    joints_reward         | 0.438     |
|    joints_vel_reward     | 0.000967  |
|    root_ori_reward       | 0.0367    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 419       |
|    ep_rew_mean           | 262       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 205       |
|    iterations            | 181       |
|    time_elapsed          | 72220     |
|    total_timesteps       | 14827520  |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 1.7002704 |
|    clip_fraction         | 0.825     |
|    clip_range            | 0.2       |
|    entropy_loss          | 67.2      |
|    explained_variance    | 0.8       |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0724   |
|    n_updates             | 1800      |
|    policy_gradient_loss  | -0.0646   |
|    std                   | 0.0551    |
|    value_loss            | 0.187     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.292     |
|    end_effectors_err     | 0.644     |
|    height_err            | -0.0159   |
|    joints_err            | 0.0777    |
|    joints_vel_err        | 72.3      |
|    root_ori_err          | 0.0229    |
| reward_terms/            |           |
|    com_reward            | 0.0434    |
|    end_effectors_reward  | 0.0347    |
|    height_reward         | 0.0969    |
|    joints_reward         | 0.432     |
|    joints_vel_reward     | 0.000807  |
|    root_ori_reward       | 0.0168    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 406       |
|    ep_rew_mean           | 254       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.002     |
|    fps                   | 206       |
|    iterations            | 182       |
|    time_elapsed          | 72366     |
|    total_timesteps       | 14909440  |
|    train_time            | 31.6      |
| train/                   |           |
|    approx_kl             | 1.4563423 |
|    clip_fraction         | 0.819     |
|    clip_range            | 0.2       |
|    entropy_loss          | 67.4      |
|    explained_variance    | 0.767     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.091    |
|    n_updates             | 1810      |
|    policy_gradient_loss  | -0.0593   |
|    std                   | 0.0549    |
|    value_loss            | 0.245     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.408     |
|    end_effectors_err     | 1.1       |
|    height_err            | 0.00451   |
|    joints_err            | 0.0734    |
|    joints_vel_err        | 67.4      |
|    root_ori_err          | 0.0177    |
| reward_terms/            |           |
|    com_reward            | 0.0333    |
|    end_effectors_reward  | 0.0293    |
|    height_reward         | 0.0989    |
|    joints_reward         | 0.436     |
|    joints_vel_reward     | 0.000803  |
|    root_ori_reward       | 0.0292    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 418       |
|    ep_rew_mean           | 263       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 206       |
|    iterations            | 183       |
|    time_elapsed          | 72513     |
|    total_timesteps       | 14991360  |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 1.6434788 |
|    clip_fraction         | 0.826     |
|    clip_range            | 0.2       |
|    entropy_loss          | 67.6      |
|    explained_variance    | 0.818     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0748   |
|    n_updates             | 1820      |
|    policy_gradient_loss  | -0.0684   |
|    std                   | 0.0546    |
|    value_loss            | 0.175     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.476     |
|    end_effectors_err     | 1.44      |
|    height_err            | -0.00542  |
|    joints_err            | 0.076     |
|    joints_vel_err        | 74.2      |
|    root_ori_err          | 0.0262    |
| reward_terms/            |           |
|    com_reward            | 0.0313    |
|    end_effectors_reward  | 0.0273    |
|    height_reward         | 0.098     |
|    joints_reward         | 0.433     |
|    joints_vel_reward     | 0.000653  |
|    root_ori_reward       | 0.0362    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 416       |
|    ep_rew_mean           | 260       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 207       |
|    iterations            | 184       |
|    time_elapsed          | 72662     |
|    total_timesteps       | 15073280  |
|    train_time            | 31.6      |
| train/                   |           |
|    approx_kl             | 1.4980453 |
|    clip_fraction         | 0.818     |
|    clip_range            | 0.2       |
|    entropy_loss          | 67.8      |
|    explained_variance    | 0.731     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.00812   |
|    n_updates             | 1830      |
|    policy_gradient_loss  | -0.0546   |
|    std                   | 0.0544    |
|    value_loss            | 0.28      |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.498     |
|    end_effectors_err     | 1.36      |
|    height_err            | -0.0118   |
|    joints_err            | 0.0726    |
|    joints_vel_err        | 65.2      |
|    root_ori_err          | 0.0255    |
| reward_terms/            |           |
|    com_reward            | 0.0203    |
|    end_effectors_reward  | 0.0254    |
|    height_reward         | 0.0975    |
|    joints_reward         | 0.436     |
|    joints_vel_reward     | 0.000741  |
|    root_ori_reward       | 0.0359    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 423       |
|    ep_rew_mean           | 265       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 208       |
|    iterations            | 185       |
|    time_elapsed          | 72808     |
|    total_timesteps       | 15155200  |
|    train_time            | 31.4      |
| train/                   |           |
|    approx_kl             | 1.5920944 |
|    clip_fraction         | 0.819     |
|    clip_range            | 0.2       |
|    entropy_loss          | 68        |
|    explained_variance    | 0.742     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0865   |
|    n_updates             | 1840      |
|    policy_gradient_loss  | -0.0537   |
|    std                   | 0.0542    |
|    value_loss            | 0.279     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.301     |
|    end_effectors_err     | 0.612     |
|    height_err            | -0.00585  |
|    joints_err            | 0.0748    |
|    joints_vel_err        | 61.6      |
|    root_ori_err          | 0.0327    |
| reward_terms/            |           |
|    com_reward            | 0.0423    |
|    end_effectors_reward  | 0.0357    |
|    height_reward         | 0.0983    |
|    joints_reward         | 0.434     |
|    joints_vel_reward     | 0.000897  |
|    root_ori_reward       | 0.0202    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 425       |
|    ep_rew_mean           | 266       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00198   |
|    fps                   | 208       |
|    iterations            | 186       |
|    time_elapsed          | 72954     |
|    total_timesteps       | 15237120  |
|    train_time            | 31.2      |
| train/                   |           |
|    approx_kl             | 1.8632854 |
|    clip_fraction         | 0.827     |
|    clip_range            | 0.2       |
|    entropy_loss          | 68.2      |
|    explained_variance    | 0.794     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0868   |
|    n_updates             | 1850      |
|    policy_gradient_loss  | -0.0641   |
|    std                   | 0.0539    |
|    value_loss            | 0.197     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.427     |
|    end_effectors_err     | 1.24      |
|    height_err            | -0.00984  |
|    joints_err            | 0.0761    |
|    joints_vel_err        | 69.7      |
|    root_ori_err          | 0.0434    |
| reward_terms/            |           |
|    com_reward            | 0.0284    |
|    end_effectors_reward  | 0.0267    |
|    height_reward         | 0.0981    |
|    joints_reward         | 0.433     |
|    joints_vel_reward     | 0.000845  |
|    root_ori_reward       | 0.0165    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 406       |
|    ep_rew_mean           | 255       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00199   |
|    fps                   | 209       |
|    iterations            | 187       |
|    time_elapsed          | 73101     |
|    total_timesteps       | 15319040  |
|    train_time            | 31.2      |
| train/                   |           |
|    approx_kl             | 1.7082713 |
|    clip_fraction         | 0.824     |
|    clip_range            | 0.2       |
|    entropy_loss          | 68.5      |
|    explained_variance    | 0.812     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.105    |
|    n_updates             | 1860      |
|    policy_gradient_loss  | -0.065    |
|    std                   | 0.0536    |
|    value_loss            | 0.19      |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.281     |
|    end_effectors_err     | 0.548     |
|    height_err            | -0.00402  |
|    joints_err            | 0.0737    |
|    joints_vel_err        | 63.5      |
|    root_ori_err          | 0.031     |
| reward_terms/            |           |
|    com_reward            | 0.0459    |
|    end_effectors_reward  | 0.037     |
|    height_reward         | 0.0979    |
|    joints_reward         | 0.435     |
|    joints_vel_reward     | 0.000902  |
|    root_ori_reward       | 0.0442    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 423       |
|    ep_rew_mean           | 267       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 210       |
|    iterations            | 188       |
|    time_elapsed          | 73249     |
|    total_timesteps       | 15400960  |
|    train_time            | 32        |
| train/                   |           |
|    approx_kl             | 1.8437954 |
|    clip_fraction         | 0.823     |
|    clip_range            | 0.2       |
|    entropy_loss          | 68.7      |
|    explained_variance    | 0.787     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.00515   |
|    n_updates             | 1870      |
|    policy_gradient_loss  | -0.0612   |
|    std                   | 0.0534    |
|    value_loss            | 0.226     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.282     |
|    end_effectors_err     | 0.533     |
|    height_err            | -0.00374  |
|    joints_err            | 0.0706    |
|    joints_vel_err        | 62.8      |
|    root_ori_err          | 0.0146    |
| reward_terms/            |           |
|    com_reward            | 0.0433    |
|    end_effectors_reward  | 0.0369    |
|    height_reward         | 0.0986    |
|    joints_reward         | 0.437     |
|    joints_vel_reward     | 0.000835  |
|    root_ori_reward       | 0.039     |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 420       |
|    ep_rew_mean           | 265       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 210       |
|    iterations            | 189       |
|    time_elapsed          | 73397     |
|    total_timesteps       | 15482880  |
|    train_time            | 32        |
| train/                   |           |
|    approx_kl             | 1.7176498 |
|    clip_fraction         | 0.825     |
|    clip_range            | 0.2       |
|    entropy_loss          | 68.9      |
|    explained_variance    | 0.795     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0835   |
|    n_updates             | 1880      |
|    policy_gradient_loss  | -0.0635   |
|    std                   | 0.0532    |
|    value_loss            | 0.199     |
----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 0.374    |
|    end_effectors_err     | 1.02     |
|    height_err            | -0.00601 |
|    joints_err            | 0.0751   |
|    joints_vel_err        | 76       |
|    root_ori_err          | 0.0455   |
| reward_terms/            |          |
|    com_reward            | 0.0351   |
|    end_effectors_reward  | 0.0297   |
|    height_reward         | 0.0982   |
|    joints_reward         | 0.434    |
|    joints_vel_reward     | 0.000503 |
|    root_ori_reward       | 0.0108   |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 414      |
|    ep_rew_mean           | 261      |
| time/                    |          |
|    collect_time          | 115      |
|    evaluate_actions_time | 0.00204  |
|    fps                   | 211      |
|    iterations            | 190      |
|    time_elapsed          | 73544    |
|    total_timesteps       | 15564800 |
|    train_time            | 31.6     |
| train/                   |          |
|    approx_kl             | 2.094445 |
|    clip_fraction         | 0.824    |
|    clip_range            | 0.2      |
|    entropy_loss          | 69.1     |
|    explained_variance    | 0.763    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | -0.0794  |
|    n_updates             | 1890     |
|    policy_gradient_loss  | -0.0557  |
|    std                   | 0.0529   |
|    value_loss            | 0.259    |
---------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.276     |
|    end_effectors_err     | 0.593     |
|    height_err            | -0.000481 |
|    joints_err            | 0.0704    |
|    joints_vel_err        | 71.9      |
|    root_ori_err          | 0.021     |
| reward_terms/            |           |
|    com_reward            | 0.0482    |
|    end_effectors_reward  | 0.0361    |
|    height_reward         | 0.0983    |
|    joints_reward         | 0.438     |
|    joints_vel_reward     | 0.000582  |
|    root_ori_reward       | 0.0331    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 416       |
|    ep_rew_mean           | 263       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 212       |
|    iterations            | 191       |
|    time_elapsed          | 73692     |
|    total_timesteps       | 15646720  |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 1.7028528 |
|    clip_fraction         | 0.824     |
|    clip_range            | 0.2       |
|    entropy_loss          | 69.3      |
|    explained_variance    | 0.73      |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0378   |
|    n_updates             | 1900      |
|    policy_gradient_loss  | -0.0539   |
|    std                   | 0.0527    |
|    value_loss            | 0.269     |
----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 0.528    |
|    end_effectors_err     | 1.7      |
|    height_err            | -0.00945 |
|    joints_err            | 0.0767   |
|    joints_vel_err        | 67.3     |
|    root_ori_err          | 0.0245   |
| reward_terms/            |          |
|    com_reward            | 0.0217   |
|    end_effectors_reward  | 0.0229   |
|    height_reward         | 0.0984   |
|    joints_reward         | 0.433    |
|    joints_vel_reward     | 0.000781 |
|    root_ori_reward       | 0.0218   |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 418      |
|    ep_rew_mean           | 265      |
| time/                    |          |
|    collect_time          | 116      |
|    evaluate_actions_time | 0.00203  |
|    fps                   | 213      |
|    iterations            | 192      |
|    time_elapsed          | 73839    |
|    total_timesteps       | 15728640 |
|    train_time            | 31.8     |
| train/                   |          |
|    approx_kl             | 1.714669 |
|    clip_fraction         | 0.823    |
|    clip_range            | 0.2      |
|    entropy_loss          | 69.5     |
|    explained_variance    | 0.752    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | -0.0654  |
|    n_updates             | 1910     |
|    policy_gradient_loss  | -0.0514  |
|    std                   | 0.0525   |
|    value_loss            | 0.273    |
---------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.284     |
|    end_effectors_err     | 0.569     |
|    height_err            | -0.0035   |
|    joints_err            | 0.0687    |
|    joints_vel_err        | 58.9      |
|    root_ori_err          | 0.016     |
| reward_terms/            |           |
|    com_reward            | 0.0462    |
|    end_effectors_reward  | 0.0367    |
|    height_reward         | 0.0988    |
|    joints_reward         | 0.439     |
|    joints_vel_reward     | 0.00105   |
|    root_ori_reward       | 0.0346    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 412       |
|    ep_rew_mean           | 261       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 213       |
|    iterations            | 193       |
|    time_elapsed          | 73987     |
|    total_timesteps       | 15810560  |
|    train_time            | 32        |
| train/                   |           |
|    approx_kl             | 1.7950741 |
|    clip_fraction         | 0.826     |
|    clip_range            | 0.2       |
|    entropy_loss          | 69.7      |
|    explained_variance    | 0.777     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.111     |
|    n_updates             | 1920      |
|    policy_gradient_loss  | -0.0569   |
|    std                   | 0.0523    |
|    value_loss            | 0.237     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.362     |
|    end_effectors_err     | 0.786     |
|    height_err            | 0.000594  |
|    joints_err            | 0.0705    |
|    joints_vel_err        | 62.4      |
|    root_ori_err          | 0.0172    |
| reward_terms/            |           |
|    com_reward            | 0.0311    |
|    end_effectors_reward  | 0.0322    |
|    height_reward         | 0.0988    |
|    joints_reward         | 0.438     |
|    joints_vel_reward     | 0.000866  |
|    root_ori_reward       | 0.042     |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 418       |
|    ep_rew_mean           | 264       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00197   |
|    fps                   | 214       |
|    iterations            | 194       |
|    time_elapsed          | 74133     |
|    total_timesteps       | 15892480  |
|    train_time            | 31.1      |
| train/                   |           |
|    approx_kl             | 1.8709882 |
|    clip_fraction         | 0.826     |
|    clip_range            | 0.2       |
|    entropy_loss          | 69.9      |
|    explained_variance    | 0.756     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.172     |
|    n_updates             | 1930      |
|    policy_gradient_loss  | -0.0539   |
|    std                   | 0.052     |
|    value_loss            | 0.272     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.535     |
|    end_effectors_err     | 1.74      |
|    height_err            | -0.00159  |
|    joints_err            | 0.0763    |
|    joints_vel_err        | 66.9      |
|    root_ori_err          | 0.0199    |
| reward_terms/            |           |
|    com_reward            | 0.0199    |
|    end_effectors_reward  | 0.0224    |
|    height_reward         | 0.0982    |
|    joints_reward         | 0.433     |
|    joints_vel_reward     | 0.000722  |
|    root_ori_reward       | 0.0269    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 422       |
|    ep_rew_mean           | 268       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00205   |
|    fps                   | 215       |
|    iterations            | 195       |
|    time_elapsed          | 74280     |
|    total_timesteps       | 15974400  |
|    train_time            | 32.1      |
| train/                   |           |
|    approx_kl             | 1.8614241 |
|    clip_fraction         | 0.826     |
|    clip_range            | 0.2       |
|    entropy_loss          | 70.1      |
|    explained_variance    | 0.741     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.0859    |
|    n_updates             | 1940      |
|    policy_gradient_loss  | -0.0525   |
|    std                   | 0.0518    |
|    value_loss            | 0.289     |
----------------------------------------
policy converted successfully and saved.
Saving video to /cluster/home/anghosh/DHProject/deep-mimic/log/2023-05-29-22-14-49-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M-residual_1/eval-video-PylocoVanilla-v0-step-18750-to-step-19750.mp4
Eval num_timesteps=16000000, episode_reward=289.16 +/- 0.00
Episode length: 527.00 +/- 0.00
Success rate: 100.00%
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.39      |
|    end_effectors_err     | 1.01      |
|    height_err            | -0.00377  |
|    joints_err            | 0.0703    |
|    joints_vel_err        | 60.1      |
|    root_ori_err          | 0.0237    |
| eval/                    |           |
|    mean_ep_length        | 527       |
|    mean_reward           | 289       |
|    success_rate          | 1         |
| reward_terms/            |           |
|    com_reward            | 0.034     |
|    end_effectors_reward  | 0.0304    |
|    height_reward         | 0.0989    |
|    joints_reward         | 0.438     |
|    joints_vel_reward     | 0.000926  |
|    root_ori_reward       | 0.0252    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| time/                    |           |
|    evaluate_actions_time | 0.00203   |
|    total_timesteps       | 16000000  |
|    train_time            | 31.5      |
| train/                   |           |
|    approx_kl             | 1.7156913 |
|    clip_fraction         | 0.828     |
|    clip_range            | 0.2       |
|    entropy_loss          | 70.3      |
|    explained_variance    | 0.766     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.127     |
|    n_updates             | 1950      |
|    policy_gradient_loss  | -0.0563   |
|    std                   | 0.0515    |
|    value_loss            | 0.263     |
----------------------------------------
--------------------------------------
| err_terms/              |          |
|    com_err              | 0.34     |
|    end_effectors_err    | 0.795    |
|    height_err           | -0.00555 |
|    joints_err           | 0.0725   |
|    joints_vel_err       | 69.9     |
|    root_ori_err         | 0.0289   |
| reward_terms/           |          |
|    com_reward           | 0.039    |
|    end_effectors_reward | 0.0332   |
|    height_reward        | 0.0984   |
|    joints_reward        | 0.436    |
|    joints_vel_reward    | 0.000887 |
|    root_ori_reward      | 0.0223   |
|    smoothness1_reward   | 0        |
|    smoothness2_reward   | 0        |
|    smoothness_reward    | 0        |
| rollout/                |          |
|    ep_len_mean          | 395      |
|    ep_rew_mean          | 251      |
| time/                   |          |
|    collect_time         | 6.37e+03 |
|    fps                  | 199      |
|    iterations           | 196      |
|    time_elapsed         | 80682    |
|    total_timesteps      | 16056320 |
--------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.337     |
|    end_effectors_err     | 0.726     |
|    height_err            | 0.000689  |
|    joints_err            | 0.0681    |
|    joints_vel_err        | 67.4      |
|    root_ori_err          | 0.0223    |
| reward_terms/            |           |
|    com_reward            | 0.0358    |
|    end_effectors_reward  | 0.0337    |
|    height_reward         | 0.0987    |
|    joints_reward         | 0.44      |
|    joints_vel_reward     | 0.000919  |
|    root_ori_reward       | 0.0356    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 410       |
|    ep_rew_mean           | 261       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.002     |
|    fps                   | 199       |
|    iterations            | 197       |
|    time_elapsed          | 80828     |
|    total_timesteps       | 16138240  |
|    train_time            | 31.3      |
| train/                   |           |
|    approx_kl             | 2.3453515 |
|    clip_fraction         | 0.824     |
|    clip_range            | 0.2       |
|    entropy_loss          | 70.5      |
|    explained_variance    | 0.709     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.301     |
|    n_updates             | 1960      |
|    policy_gradient_loss  | -0.0469   |
|    std                   | 0.0513    |
|    value_loss            | 0.333     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.657     |
|    end_effectors_err     | 2.61      |
|    height_err            | -0.00443  |
|    joints_err            | 0.0711    |
|    joints_vel_err        | 63.6      |
|    root_ori_err          | 0.0328    |
| reward_terms/            |           |
|    com_reward            | 0.0147    |
|    end_effectors_reward  | 0.017     |
|    height_reward         | 0.0984    |
|    joints_reward         | 0.437     |
|    joints_vel_reward     | 0.000905  |
|    root_ori_reward       | 0.0166    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 402       |
|    ep_rew_mean           | 257       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 200       |
|    iterations            | 198       |
|    time_elapsed          | 80976     |
|    total_timesteps       | 16220160  |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 1.8987677 |
|    clip_fraction         | 0.828     |
|    clip_range            | 0.2       |
|    entropy_loss          | 70.8      |
|    explained_variance    | 0.748     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.323     |
|    n_updates             | 1970      |
|    policy_gradient_loss  | -0.0522   |
|    std                   | 0.0511    |
|    value_loss            | 0.273     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.465     |
|    end_effectors_err     | 1.24      |
|    height_err            | -0.000184 |
|    joints_err            | 0.0727    |
|    joints_vel_err        | 64.4      |
|    root_ori_err          | 0.0158    |
| reward_terms/            |           |
|    com_reward            | 0.0239    |
|    end_effectors_reward  | 0.0269    |
|    height_reward         | 0.0988    |
|    joints_reward         | 0.436     |
|    joints_vel_reward     | 0.000889  |
|    root_ori_reward       | 0.0474    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 411       |
|    ep_rew_mean           | 261       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 200       |
|    iterations            | 199       |
|    time_elapsed          | 81124     |
|    total_timesteps       | 16302080  |
|    train_time            | 31.6      |
| train/                   |           |
|    approx_kl             | 2.1505303 |
|    clip_fraction         | 0.827     |
|    clip_range            | 0.2       |
|    entropy_loss          | 71        |
|    explained_variance    | 0.742     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.07     |
|    n_updates             | 1980      |
|    policy_gradient_loss  | -0.0538   |
|    std                   | 0.0508    |
|    value_loss            | 0.266     |
----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 0.445    |
|    end_effectors_err     | 1.14     |
|    height_err            | -0.00615 |
|    joints_err            | 0.069    |
|    joints_vel_err        | 59.4     |
|    root_ori_err          | 0.02     |
| reward_terms/            |          |
|    com_reward            | 0.0253   |
|    end_effectors_reward  | 0.028    |
|    height_reward         | 0.0985   |
|    joints_reward         | 0.439    |
|    joints_vel_reward     | 0.001    |
|    root_ori_reward       | 0.0366   |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 417      |
|    ep_rew_mean           | 264      |
| time/                    |          |
|    collect_time          | 116      |
|    evaluate_actions_time | 0.00203  |
|    fps                   | 201      |
|    iterations            | 200      |
|    time_elapsed          | 81272    |
|    total_timesteps       | 16384000 |
|    train_time            | 32       |
| train/                   |          |
|    approx_kl             | 2.038052 |
|    clip_fraction         | 0.829    |
|    clip_range            | 0.2      |
|    entropy_loss          | 71.2     |
|    explained_variance    | 0.752    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | 0.125    |
|    n_updates             | 1990     |
|    policy_gradient_loss  | -0.052   |
|    std                   | 0.0506   |
|    value_loss            | 0.281    |
---------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.343     |
|    end_effectors_err     | 0.861     |
|    height_err            | -8.92e-05 |
|    joints_err            | 0.0743    |
|    joints_vel_err        | 62.9      |
|    root_ori_err          | 0.0215    |
| reward_terms/            |           |
|    com_reward            | 0.0382    |
|    end_effectors_reward  | 0.0317    |
|    height_reward         | 0.0991    |
|    joints_reward         | 0.435     |
|    joints_vel_reward     | 0.00085   |
|    root_ori_reward       | 0.0204    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 402       |
|    ep_rew_mean           | 257       |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 202       |
|    iterations            | 201       |
|    time_elapsed          | 81419     |
|    total_timesteps       | 16465920  |
|    train_time            | 31.6      |
| train/                   |           |
|    approx_kl             | 2.3443813 |
|    clip_fraction         | 0.828     |
|    clip_range            | 0.2       |
|    entropy_loss          | 71.4      |
|    explained_variance    | 0.739     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.265     |
|    n_updates             | 2000      |
|    policy_gradient_loss  | -0.0543   |
|    std                   | 0.0504    |
|    value_loss            | 0.265     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.281     |
|    end_effectors_err     | 0.572     |
|    height_err            | -0.00594  |
|    joints_err            | 0.0711    |
|    joints_vel_err        | 62.4      |
|    root_ori_err          | 0.0273    |
| reward_terms/            |           |
|    com_reward            | 0.0445    |
|    end_effectors_reward  | 0.0363    |
|    height_reward         | 0.0989    |
|    joints_reward         | 0.437     |
|    joints_vel_reward     | 0.000866  |
|    root_ori_reward       | 0.022     |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 406       |
|    ep_rew_mean           | 261       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.002     |
|    fps                   | 202       |
|    iterations            | 202       |
|    time_elapsed          | 81565     |
|    total_timesteps       | 16547840  |
|    train_time            | 31.4      |
| train/                   |           |
|    approx_kl             | 2.0853715 |
|    clip_fraction         | 0.829     |
|    clip_range            | 0.2       |
|    entropy_loss          | 71.6      |
|    explained_variance    | 0.73      |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.596     |
|    n_updates             | 2010      |
|    policy_gradient_loss  | -0.0503   |
|    std                   | 0.0501    |
|    value_loss            | 0.311     |
----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 0.351    |
|    end_effectors_err     | 0.767    |
|    height_err            | -0.00617 |
|    joints_err            | 0.0739   |
|    joints_vel_err        | 59.2     |
|    root_ori_err          | 0.0264   |
| reward_terms/            |          |
|    com_reward            | 0.0339   |
|    end_effectors_reward  | 0.033    |
|    height_reward         | 0.0981   |
|    joints_reward         | 0.435    |
|    joints_vel_reward     | 0.00103  |
|    root_ori_reward       | 0.0479   |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 419      |
|    ep_rew_mean           | 268      |
| time/                    |          |
|    collect_time          | 116      |
|    evaluate_actions_time | 0.002    |
|    fps                   | 203      |
|    iterations            | 203      |
|    time_elapsed          | 81713    |
|    total_timesteps       | 16629760 |
|    train_time            | 31.8     |
| train/                   |          |
|    approx_kl             | 1.965052 |
|    clip_fraction         | 0.828    |
|    clip_range            | 0.2      |
|    entropy_loss          | 71.8     |
|    explained_variance    | 0.727    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | -0.0339  |
|    n_updates             | 2020     |
|    policy_gradient_loss  | -0.0514  |
|    std                   | 0.0499   |
|    value_loss            | 0.308    |
---------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.375     |
|    end_effectors_err     | 0.854     |
|    height_err            | 0.000162  |
|    joints_err            | 0.0689    |
|    joints_vel_err        | 67.3      |
|    root_ori_err          | 0.0215    |
| reward_terms/            |           |
|    com_reward            | 0.032     |
|    end_effectors_reward  | 0.0318    |
|    height_reward         | 0.0981    |
|    joints_reward         | 0.439     |
|    joints_vel_reward     | 0.000783  |
|    root_ori_reward       | 0.0427    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 398       |
|    ep_rew_mean           | 257       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 204       |
|    iterations            | 204       |
|    time_elapsed          | 81859     |
|    total_timesteps       | 16711680  |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 1.7684548 |
|    clip_fraction         | 0.83      |
|    clip_range            | 0.2       |
|    entropy_loss          | 72        |
|    explained_variance    | 0.757     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0752   |
|    n_updates             | 2030      |
|    policy_gradient_loss  | -0.054    |
|    std                   | 0.0497    |
|    value_loss            | 0.253     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.353     |
|    end_effectors_err     | 0.846     |
|    height_err            | -0.00701  |
|    joints_err            | 0.0706    |
|    joints_vel_err        | 64.2      |
|    root_ori_err          | 0.0193    |
| reward_terms/            |           |
|    com_reward            | 0.0335    |
|    end_effectors_reward  | 0.0312    |
|    height_reward         | 0.0989    |
|    joints_reward         | 0.438     |
|    joints_vel_reward     | 0.00116   |
|    root_ori_reward       | 0.0296    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 415       |
|    ep_rew_mean           | 265       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 204       |
|    iterations            | 205       |
|    time_elapsed          | 82007     |
|    total_timesteps       | 16793600  |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 1.9839392 |
|    clip_fraction         | 0.828     |
|    clip_range            | 0.2       |
|    entropy_loss          | 72.2      |
|    explained_variance    | 0.748     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.00809  |
|    n_updates             | 2040      |
|    policy_gradient_loss  | -0.0527   |
|    std                   | 0.0495    |
|    value_loss            | 0.282     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.29      |
|    end_effectors_err     | 0.605     |
|    height_err            | -0.000476 |
|    joints_err            | 0.0709    |
|    joints_vel_err        | 57.9      |
|    root_ori_err          | 0.0133    |
| reward_terms/            |           |
|    com_reward            | 0.0417    |
|    end_effectors_reward  | 0.0353    |
|    height_reward         | 0.0986    |
|    joints_reward         | 0.437     |
|    joints_vel_reward     | 0.00119   |
|    root_ori_reward       | 0.0463    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 412       |
|    ep_rew_mean           | 266       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 205       |
|    iterations            | 206       |
|    time_elapsed          | 82155     |
|    total_timesteps       | 16875520  |
|    train_time            | 32.1      |
| train/                   |           |
|    approx_kl             | 2.0969093 |
|    clip_fraction         | 0.833     |
|    clip_range            | 0.2       |
|    entropy_loss          | 72.4      |
|    explained_variance    | 0.798     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0745   |
|    n_updates             | 2050      |
|    policy_gradient_loss  | -0.0591   |
|    std                   | 0.0493    |
|    value_loss            | 0.216     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.371     |
|    end_effectors_err     | 0.898     |
|    height_err            | 0.000319  |
|    joints_err            | 0.0683    |
|    joints_vel_err        | 58.2      |
|    root_ori_err          | 0.015     |
| reward_terms/            |           |
|    com_reward            | 0.0347    |
|    end_effectors_reward  | 0.0314    |
|    height_reward         | 0.0979    |
|    joints_reward         | 0.439     |
|    joints_vel_reward     | 0.00124   |
|    root_ori_reward       | 0.0295    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 421       |
|    ep_rew_mean           | 271       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 206       |
|    iterations            | 207       |
|    time_elapsed          | 82303     |
|    total_timesteps       | 16957440  |
|    train_time            | 32        |
| train/                   |           |
|    approx_kl             | 2.267783  |
|    clip_fraction         | 0.831     |
|    clip_range            | 0.2       |
|    entropy_loss          | 72.6      |
|    explained_variance    | 0.808     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.000334 |
|    n_updates             | 2060      |
|    policy_gradient_loss  | -0.0607   |
|    std                   | 0.049     |
|    value_loss            | 0.199     |
----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 0.312    |
|    end_effectors_err     | 0.643    |
|    height_err            | -0.00816 |
|    joints_err            | 0.0659   |
|    joints_vel_err        | 55.2     |
|    root_ori_err          | 0.0205   |
| reward_terms/            |          |
|    com_reward            | 0.0398   |
|    end_effectors_reward  | 0.035    |
|    height_reward         | 0.0984   |
|    joints_reward         | 0.442    |
|    joints_vel_reward     | 0.00156  |
|    root_ori_reward       | 0.041    |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 392      |
|    ep_rew_mean           | 252      |
| time/                    |          |
|    collect_time          | 116      |
|    evaluate_actions_time | 0.00203  |
|    fps                   | 206      |
|    iterations            | 208      |
|    time_elapsed          | 82451    |
|    total_timesteps       | 17039360 |
|    train_time            | 32       |
| train/                   |          |
|    approx_kl             | 2.031797 |
|    clip_fraction         | 0.829    |
|    clip_range            | 0.2      |
|    entropy_loss          | 72.8     |
|    explained_variance    | 0.776    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | 0.0171   |
|    n_updates             | 2070     |
|    policy_gradient_loss  | -0.0552  |
|    std                   | 0.0489   |
|    value_loss            | 0.257    |
---------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.401     |
|    end_effectors_err     | 1.08      |
|    height_err            | -0.00377  |
|    joints_err            | 0.0781    |
|    joints_vel_err        | 57.7      |
|    root_ori_err          | 0.0193    |
| reward_terms/            |           |
|    com_reward            | 0.0275    |
|    end_effectors_reward  | 0.0282    |
|    height_reward         | 0.0986    |
|    joints_reward         | 0.432     |
|    joints_vel_reward     | 0.00109   |
|    root_ori_reward       | 0.0252    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 404       |
|    ep_rew_mean           | 260       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 207       |
|    iterations            | 209       |
|    time_elapsed          | 82598     |
|    total_timesteps       | 17121280  |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 1.6927023 |
|    clip_fraction         | 0.824     |
|    clip_range            | 0.2       |
|    entropy_loss          | 73        |
|    explained_variance    | 0.72      |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.143     |
|    n_updates             | 2080      |
|    policy_gradient_loss  | -0.0444   |
|    std                   | 0.0486    |
|    value_loss            | 0.363     |
----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 0.286    |
|    end_effectors_err     | 0.561    |
|    height_err            | -0.00644 |
|    joints_err            | 0.0648   |
|    joints_vel_err        | 56       |
|    root_ori_err          | 0.0164   |
| reward_terms/            |          |
|    com_reward            | 0.0452   |
|    end_effectors_reward  | 0.0365   |
|    height_reward         | 0.0989   |
|    joints_reward         | 0.442    |
|    joints_vel_reward     | 0.00143  |
|    root_ori_reward       | 0.0489   |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 427      |
|    ep_rew_mean           | 275      |
| time/                    |          |
|    collect_time          | 114      |
|    evaluate_actions_time | 0.00201  |
|    fps                   | 207      |
|    iterations            | 210      |
|    time_elapsed          | 82744    |
|    total_timesteps       | 17203200 |
|    train_time            | 31.5     |
| train/                   |          |
|    approx_kl             | 2.568648 |
|    clip_fraction         | 0.831    |
|    clip_range            | 0.2      |
|    entropy_loss          | 73.2     |
|    explained_variance    | 0.716    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | -0.0661  |
|    n_updates             | 2090     |
|    policy_gradient_loss  | -0.0488  |
|    std                   | 0.0484   |
|    value_loss            | 0.302    |
---------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.5       |
|    end_effectors_err     | 1.44      |
|    height_err            | 0.00495   |
|    joints_err            | 0.0759    |
|    joints_vel_err        | 60.7      |
|    root_ori_err          | 0.0166    |
| reward_terms/            |           |
|    com_reward            | 0.0249    |
|    end_effectors_reward  | 0.0257    |
|    height_reward         | 0.0984    |
|    joints_reward         | 0.434     |
|    joints_vel_reward     | 0.00122   |
|    root_ori_reward       | 0.042     |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 422       |
|    ep_rew_mean           | 273       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00206   |
|    fps                   | 208       |
|    iterations            | 211       |
|    time_elapsed          | 82892     |
|    total_timesteps       | 17285120  |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 3.2397523 |
|    clip_fraction         | 0.838     |
|    clip_range            | 0.2       |
|    entropy_loss          | 73.4      |
|    explained_variance    | 0.785     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0257   |
|    n_updates             | 2100      |
|    policy_gradient_loss  | -0.0548   |
|    std                   | 0.0481    |
|    value_loss            | 0.232     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.494     |
|    end_effectors_err     | 1.4       |
|    height_err            | -0.00172  |
|    joints_err            | 0.0654    |
|    joints_vel_err        | 68.8      |
|    root_ori_err          | 0.0382    |
| reward_terms/            |           |
|    com_reward            | 0.0224    |
|    end_effectors_reward  | 0.0256    |
|    height_reward         | 0.0984    |
|    joints_reward         | 0.442     |
|    joints_vel_reward     | 0.000889  |
|    root_ori_reward       | 0.0419    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 378       |
|    ep_rew_mean           | 246       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00205   |
|    fps                   | 209       |
|    iterations            | 212       |
|    time_elapsed          | 83039     |
|    total_timesteps       | 17367040  |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 1.8143826 |
|    clip_fraction         | 0.831     |
|    clip_range            | 0.2       |
|    entropy_loss          | 73.7      |
|    explained_variance    | 0.721     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.402     |
|    n_updates             | 2110      |
|    policy_gradient_loss  | -0.0445   |
|    std                   | 0.0479    |
|    value_loss            | 0.329     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.261     |
|    end_effectors_err     | 0.431     |
|    height_err            | -0.0034   |
|    joints_err            | 0.0671    |
|    joints_vel_err        | 59.2      |
|    root_ori_err          | 0.0114    |
| reward_terms/            |           |
|    com_reward            | 0.0455    |
|    end_effectors_reward  | 0.0389    |
|    height_reward         | 0.0982    |
|    joints_reward         | 0.441     |
|    joints_vel_reward     | 0.00137   |
|    root_ori_reward       | 0.0533    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 416       |
|    ep_rew_mean           | 271       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 209       |
|    iterations            | 213       |
|    time_elapsed          | 83187     |
|    total_timesteps       | 17448960  |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 2.1190262 |
|    clip_fraction         | 0.827     |
|    clip_range            | 0.2       |
|    entropy_loss          | 73.9      |
|    explained_variance    | 0.634     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.109     |
|    n_updates             | 2120      |
|    policy_gradient_loss  | -0.0409   |
|    std                   | 0.0477    |
|    value_loss            | 0.418     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.296     |
|    end_effectors_err     | 0.61      |
|    height_err            | 0.00213   |
|    joints_err            | 0.0781    |
|    joints_vel_err        | 61.9      |
|    root_ori_err          | 0.0159    |
| reward_terms/            |           |
|    com_reward            | 0.0423    |
|    end_effectors_reward  | 0.0355    |
|    height_reward         | 0.0987    |
|    joints_reward         | 0.432     |
|    joints_vel_reward     | 0.00116   |
|    root_ori_reward       | 0.0464    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 421       |
|    ep_rew_mean           | 272       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.002     |
|    fps                   | 210       |
|    iterations            | 214       |
|    time_elapsed          | 83334     |
|    total_timesteps       | 17530880  |
|    train_time            | 31        |
| train/                   |           |
|    approx_kl             | 2.5569797 |
|    clip_fraction         | 0.836     |
|    clip_range            | 0.2       |
|    entropy_loss          | 74.1      |
|    explained_variance    | 0.726     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0627   |
|    n_updates             | 2130      |
|    policy_gradient_loss  | -0.0504   |
|    std                   | 0.0475    |
|    value_loss            | 0.251     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.317     |
|    end_effectors_err     | 0.61      |
|    height_err            | 0.00322   |
|    joints_err            | 0.0688    |
|    joints_vel_err        | 55        |
|    root_ori_err          | 0.02      |
| reward_terms/            |           |
|    com_reward            | 0.0363    |
|    end_effectors_reward  | 0.0354    |
|    height_reward         | 0.0986    |
|    joints_reward         | 0.439     |
|    joints_vel_reward     | 0.00149   |
|    root_ori_reward       | 0.0378    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 404       |
|    ep_rew_mean           | 263       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.002     |
|    fps                   | 210       |
|    iterations            | 215       |
|    time_elapsed          | 83481     |
|    total_timesteps       | 17612800  |
|    train_time            | 31.5      |
| train/                   |           |
|    approx_kl             | 1.9270289 |
|    clip_fraction         | 0.834     |
|    clip_range            | 0.2       |
|    entropy_loss          | 74.3      |
|    explained_variance    | 0.762     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0809   |
|    n_updates             | 2140      |
|    policy_gradient_loss  | -0.0515   |
|    std                   | 0.0473    |
|    value_loss            | 0.246     |
----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 0.351    |
|    end_effectors_err     | 0.767    |
|    height_err            | -0.00932 |
|    joints_err            | 0.0658   |
|    joints_vel_err        | 53.8     |
|    root_ori_err          | 0.0174   |
| reward_terms/            |          |
|    com_reward            | 0.0349   |
|    end_effectors_reward  | 0.0329   |
|    height_reward         | 0.0981   |
|    joints_reward         | 0.442    |
|    joints_vel_reward     | 0.00137  |
|    root_ori_reward       | 0.045    |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 407      |
|    ep_rew_mean           | 265      |
| time/                    |          |
|    collect_time          | 116      |
|    evaluate_actions_time | 0.00201  |
|    fps                   | 211      |
|    iterations            | 216      |
|    time_elapsed          | 83628    |
|    total_timesteps       | 17694720 |
|    train_time            | 31.3     |
| train/                   |          |
|    approx_kl             | 1.863383 |
|    clip_fraction         | 0.826    |
|    clip_range            | 0.2      |
|    entropy_loss          | 74.5     |
|    explained_variance    | 0.696    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | 0.381    |
|    n_updates             | 2150     |
|    policy_gradient_loss  | -0.0404  |
|    std                   | 0.0471   |
|    value_loss            | 0.383    |
---------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.34      |
|    end_effectors_err     | 0.712     |
|    height_err            | 0.00035   |
|    joints_err            | 0.0682    |
|    joints_vel_err        | 54        |
|    root_ori_err          | 0.0161    |
| reward_terms/            |           |
|    com_reward            | 0.0356    |
|    end_effectors_reward  | 0.034     |
|    height_reward         | 0.0986    |
|    joints_reward         | 0.44      |
|    joints_vel_reward     | 0.00153   |
|    root_ori_reward       | 0.0501    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 417       |
|    ep_rew_mean           | 271       |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 212       |
|    iterations            | 217       |
|    time_elapsed          | 83775     |
|    total_timesteps       | 17776640  |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 2.7266796 |
|    clip_fraction         | 0.838     |
|    clip_range            | 0.2       |
|    entropy_loss          | 74.7      |
|    explained_variance    | 0.748     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0854   |
|    n_updates             | 2160      |
|    policy_gradient_loss  | -0.0476   |
|    std                   | 0.0469    |
|    value_loss            | 0.287     |
----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 0.263    |
|    end_effectors_err     | 0.599    |
|    height_err            | 0.00315  |
|    joints_err            | 0.0762   |
|    joints_vel_err        | 70.6     |
|    root_ori_err          | 0.0437   |
| reward_terms/            |          |
|    com_reward            | 0.0493   |
|    end_effectors_reward  | 0.036    |
|    height_reward         | 0.0983   |
|    joints_reward         | 0.433    |
|    joints_vel_reward     | 0.000816 |
|    root_ori_reward       | 0.0392   |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 390      |
|    ep_rew_mean           | 254      |
| time/                    |          |
|    collect_time          | 115      |
|    evaluate_actions_time | 0.00203  |
|    fps                   | 212      |
|    iterations            | 218      |
|    time_elapsed          | 83922    |
|    total_timesteps       | 17858560 |
|    train_time            | 31.5     |
| train/                   |          |
|    approx_kl             | 2.268786 |
|    clip_fraction         | 0.834    |
|    clip_range            | 0.2      |
|    entropy_loss          | 74.8     |
|    explained_variance    | 0.737    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | 0.123    |
|    n_updates             | 2170     |
|    policy_gradient_loss  | -0.0463  |
|    std                   | 0.0467   |
|    value_loss            | 0.311    |
---------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.323     |
|    end_effectors_err     | 0.636     |
|    height_err            | -0.00793  |
|    joints_err            | 0.0689    |
|    joints_vel_err        | 53.7      |
|    root_ori_err          | 0.0211    |
| reward_terms/            |           |
|    com_reward            | 0.0378    |
|    end_effectors_reward  | 0.0352    |
|    height_reward         | 0.0979    |
|    joints_reward         | 0.439     |
|    joints_vel_reward     | 0.00151   |
|    root_ori_reward       | 0.0457    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 417       |
|    ep_rew_mean           | 272       |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 213       |
|    iterations            | 219       |
|    time_elapsed          | 84070     |
|    total_timesteps       | 17940480  |
|    train_time            | 32        |
| train/                   |           |
|    approx_kl             | 2.6133099 |
|    clip_fraction         | 0.833     |
|    clip_range            | 0.2       |
|    entropy_loss          | 75        |
|    explained_variance    | 0.711     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | 0.00106   |
|    n_updates             | 2180      |
|    policy_gradient_loss  | -0.0445   |
|    std                   | 0.0465    |
|    value_loss            | 0.33      |
----------------------------------------
