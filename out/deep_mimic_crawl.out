wandb: Currently logged in as: ankitaghosh0907 (dh-project). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /cluster/home/anghosh/DHProject/deep-mimic/log/wandb/run-20230529_075835-exql7o2w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 2023-05-29-07-58-21-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M_1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/dh-project/DH-Project
wandb: üöÄ View run at https://wandb.ai/dh-project/DH-Project/runs/exql7o2w
wandb: WARNING Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
- config file path = /cluster/home/anghosh/DHProject/deep-mimic/data/conf/bob_env_crawl.json
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Ignoring input line: position 0 0.90 0
Logging to /cluster/home/anghosh/DHProject/deep-mimic/log/2023-05-29-07-58-21-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M_1
/cluster/home/anghosh/venv/lib64/python3.9/site-packages/stable_baselines3/common/callbacks.py:345: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x2ae2d93793a0> != <stable_baselines3.common.vec_env.vec_video_recorder.VecVideoRecorder object at 0x2ae2d8c7c0d0>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
Ignoring input line: position 0 0.90 0
Eval num_timesteps=20, episode_reward=1.13 +/- 0.00
Episode length: 2.00 +/- 0.00
Success rate: 0.00%
--------------------------------------
| err_terms/              |          |
|    com_err              | 0.0101   |
|    end_effectors_err    | 0.0194   |
|    height_err           | -0.00415 |
|    joints_err           | 0.226    |
|    joints_vel_err       | 984      |
|    root_ori_err         | 0.000645 |
| eval/                   |          |
|    mean_ep_length       | 2        |
|    mean_reward          | 1.13     |
|    success_rate         | 0        |
| reward_terms/           |          |
|    com_reward           | 0.0999   |
|    end_effectors_reward | 0.0494   |
|    height_reward        | 0.0999   |
|    joints_reward        | 0.324    |
|    joints_vel_reward    | 1.31e-42 |
|    root_ori_reward      | 0.0773   |
|    smoothness1_reward   | 0        |
|    smoothness2_reward   | 0        |
|    smoothness_reward    | 0        |
| time/                   |          |
|    total_timesteps      | 20       |
--------------------------------------
New best mean reward!
--------------------------------------
| err_terms/              |          |
|    com_err              | 0.262    |
|    end_effectors_err    | 0.797    |
|    height_err           | -0.12    |
|    joints_err           | 9.91     |
|    joints_vel_err       | 182      |
|    root_ori_err         | 0.264    |
| reward_terms/           |          |
|    com_reward           | 0.0538   |
|    end_effectors_reward | 0.0351   |
|    height_reward        | 0.0505   |
|    joints_reward        | 0.0063   |
|    joints_vel_reward    | 3.46e-05 |
|    root_ori_reward      | 0.00217  |
|    smoothness1_reward   | 0        |
|    smoothness2_reward   | 0        |
|    smoothness_reward    | 0        |
| rollout/                |          |
|    ep_len_mean          | 43.1     |
|    ep_rew_mean          | 5.49     |
| time/                   |          |
|    collect_time         | 196      |
|    fps                  | 418      |
|    iterations           | 1        |
|    time_elapsed         | 195      |
|    total_timesteps      | 81920    |
--------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.0208     |
|    end_effectors_err     | 0.0343     |
|    height_err            | -0.0119    |
|    joints_err            | 0.475      |
|    joints_vel_err        | 744        |
|    root_ori_err          | 0.00107    |
| reward_terms/            |            |
|    com_reward            | 0.0994     |
|    end_effectors_reward  | 0.049      |
|    height_reward         | 0.0991     |
|    joints_reward         | 0.226      |
|    joints_vel_reward     | 6.46e-24   |
|    root_ori_reward       | 0.0659     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 44.9       |
|    ep_rew_mean           | 5.64       |
| time/                    |            |
|    collect_time          | 116        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 476        |
|    iterations            | 2          |
|    time_elapsed          | 343        |
|    total_timesteps       | 163840     |
|    train_time            | 32         |
| train/                   |            |
|    approx_kl             | 0.24743104 |
|    clip_fraction         | 0.388      |
|    clip_range            | 0.2        |
|    entropy_loss          | -5.22      |
|    explained_variance    | -0.253     |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0522    |
|    n_updates             | 10         |
|    policy_gradient_loss  | -0.0495    |
|    std                   | 0.272      |
|    value_loss            | 0.609      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.836     |
|    end_effectors_err     | 6.81      |
|    height_err            | -0.197    |
|    joints_err            | 13.3      |
|    joints_vel_err        | 146       |
|    root_ori_err          | 1.31      |
| reward_terms/            |           |
|    com_reward            | 0.0115    |
|    end_effectors_reward  | 0.00927   |
|    height_reward         | 0.0268    |
|    joints_reward         | 0.00179   |
|    joints_vel_reward     | 1.4e-05   |
|    root_ori_reward       | 0.00109   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 61        |
|    ep_rew_mean           | 6.07      |
| time/                    |           |
|    collect_time          | 116       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 499       |
|    iterations            | 3         |
|    time_elapsed          | 491       |
|    total_timesteps       | 245760    |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 0.2458462 |
|    clip_fraction         | 0.4       |
|    clip_range            | 0.2       |
|    entropy_loss          | -5.17     |
|    explained_variance    | 0.771     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0696   |
|    n_updates             | 20        |
|    policy_gradient_loss  | -0.0632   |
|    std                   | 0.272     |
|    value_loss            | 0.103     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.254      |
|    end_effectors_err     | 0.393      |
|    height_err            | -0.0657    |
|    joints_err            | 8.85       |
|    joints_vel_err        | 334        |
|    root_ori_err          | 0.118      |
| reward_terms/            |            |
|    com_reward            | 0.0482     |
|    end_effectors_reward  | 0.0396     |
|    height_reward         | 0.078      |
|    joints_reward         | 0.0183     |
|    joints_vel_reward     | 1.97e-07   |
|    root_ori_reward       | 0.00312    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 71.9       |
|    ep_rew_mean           | 6          |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 514        |
|    iterations            | 4          |
|    time_elapsed          | 637        |
|    total_timesteps       | 327680     |
|    train_time            | 31.5       |
| train/                   |            |
|    approx_kl             | 0.15404756 |
|    clip_fraction         | 0.453      |
|    clip_range            | 0.2        |
|    entropy_loss          | -5.11      |
|    explained_variance    | 0.87       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.051     |
|    n_updates             | 30         |
|    policy_gradient_loss  | -0.0564    |
|    std                   | 0.272      |
|    value_loss            | 0.0828     |
-----------------------------------------
------------------------------------------
| err_terms/               |             |
|    com_err               | 0.402       |
|    end_effectors_err     | 0.8         |
|    height_err            | -0.0757     |
|    joints_err            | 10.5        |
|    joints_vel_err        | 191         |
|    root_ori_err          | 0.714       |
| reward_terms/            |             |
|    com_reward            | 0.0288      |
|    end_effectors_reward  | 0.0327      |
|    height_reward         | 0.0711      |
|    joints_reward         | 0.0115      |
|    joints_vel_reward     | 2.27e-05    |
|    root_ori_reward       | 0.0045      |
|    smoothness1_reward    | 0           |
|    smoothness2_reward    | 0           |
|    smoothness_reward     | 0           |
| rollout/                 |             |
|    ep_len_mean           | 66.8        |
|    ep_rew_mean           | 6.23        |
| time/                    |             |
|    collect_time          | 115         |
|    evaluate_actions_time | 0.00201     |
|    fps                   | 522         |
|    iterations            | 5           |
|    time_elapsed          | 784         |
|    total_timesteps       | 409600      |
|    train_time            | 31.6        |
| train/                   |             |
|    approx_kl             | 0.069758385 |
|    clip_fraction         | 0.444       |
|    clip_range            | 0.2         |
|    entropy_loss          | -5.1        |
|    explained_variance    | 0.904       |
|    learning_rate_log_std | 0.0003      |
|    learning_rate_policy  | 5e-05       |
|    learning_rate_value   | 0.01        |
|    loss                  | -0.0237     |
|    n_updates             | 40          |
|    policy_gradient_loss  | -0.0546     |
|    std                   | 0.272       |
|    value_loss            | 0.0836      |
------------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.369     |
|    end_effectors_err     | 1.77      |
|    height_err            | -0.127    |
|    joints_err            | 12.1      |
|    joints_vel_err        | 175       |
|    root_ori_err          | 0.275     |
| reward_terms/            |           |
|    com_reward            | 0.0335    |
|    end_effectors_reward  | 0.0207    |
|    height_reward         | 0.0463    |
|    joints_reward         | 0.00726   |
|    joints_vel_reward     | 1.92e-05  |
|    root_ori_reward       | 0.00185   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 75.4      |
|    ep_rew_mean           | 6.32      |
| time/                    |           |
|    collect_time          | 113       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 528       |
|    iterations            | 6         |
|    time_elapsed          | 929       |
|    total_timesteps       | 491520    |
|    train_time            | 31.8      |
| train/                   |           |
|    approx_kl             | 0.0743145 |
|    clip_fraction         | 0.465     |
|    clip_range            | 0.2       |
|    entropy_loss          | -5.05     |
|    explained_variance    | 0.906     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.042    |
|    n_updates             | 50        |
|    policy_gradient_loss  | -0.0591   |
|    std                   | 0.271     |
|    value_loss            | 0.0814    |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.393      |
|    end_effectors_err     | 1.48       |
|    height_err            | -0.127     |
|    joints_err            | 10.6       |
|    joints_vel_err        | 180        |
|    root_ori_err          | 0.729      |
| reward_terms/            |            |
|    com_reward            | 0.039      |
|    end_effectors_reward  | 0.0273     |
|    height_reward         | 0.0439     |
|    joints_reward         | 0.00914    |
|    joints_vel_reward     | 4.41e-06   |
|    root_ori_reward       | 0.00453    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 97.6       |
|    ep_rew_mean           | 7.54       |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 532        |
|    iterations            | 7          |
|    time_elapsed          | 1076       |
|    total_timesteps       | 573440     |
|    train_time            | 32.3       |
| train/                   |            |
|    approx_kl             | 0.07528578 |
|    clip_fraction         | 0.477      |
|    clip_range            | 0.2        |
|    entropy_loss          | -5.01      |
|    explained_variance    | 0.915      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.061     |
|    n_updates             | 60         |
|    policy_gradient_loss  | -0.0573    |
|    std                   | 0.271      |
|    value_loss            | 0.0752     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.0213     |
|    end_effectors_err     | 0.0482     |
|    height_err            | -0.0183    |
|    joints_err            | 0.519      |
|    joints_vel_err        | 849        |
|    root_ori_err          | 0.000705   |
| reward_terms/            |            |
|    com_reward            | 0.0994     |
|    end_effectors_reward  | 0.0485     |
|    height_reward         | 0.0981     |
|    joints_reward         | 0.229      |
|    joints_vel_reward     | 9.66e-31   |
|    root_ori_reward       | 0.0767     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 85.4       |
|    ep_rew_mean           | 6.41       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00205    |
|    fps                   | 535        |
|    iterations            | 8          |
|    time_elapsed          | 1223       |
|    total_timesteps       | 655360     |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.07575278 |
|    clip_fraction         | 0.49       |
|    clip_range            | 0.2        |
|    entropy_loss          | -4.97      |
|    explained_variance    | 0.925      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0359    |
|    n_updates             | 70         |
|    policy_gradient_loss  | -0.0576    |
|    std                   | 0.271      |
|    value_loss            | 0.0747     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.483      |
|    end_effectors_err     | 2.56       |
|    height_err            | -0.157     |
|    joints_err            | 11.7       |
|    joints_vel_err        | 157        |
|    root_ori_err          | 0.749      |
| reward_terms/            |            |
|    com_reward            | 0.0218     |
|    end_effectors_reward  | 0.0205     |
|    height_reward         | 0.0407     |
|    joints_reward         | 0.00393    |
|    joints_vel_reward     | 2.57e-05   |
|    root_ori_reward       | 0.00136    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 79.3       |
|    ep_rew_mean           | 6.74       |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 538        |
|    iterations            | 9          |
|    time_elapsed          | 1369       |
|    total_timesteps       | 737280     |
|    train_time            | 32         |
| train/                   |            |
|    approx_kl             | 0.08113737 |
|    clip_fraction         | 0.5        |
|    clip_range            | 0.2        |
|    entropy_loss          | -4.95      |
|    explained_variance    | 0.922      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0443    |
|    n_updates             | 80         |
|    policy_gradient_loss  | -0.0575    |
|    std                   | 0.271      |
|    value_loss            | 0.0728     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.457      |
|    end_effectors_err     | 1.45       |
|    height_err            | -0.131     |
|    joints_err            | 11.5       |
|    joints_vel_err        | 165        |
|    root_ori_err          | 0.688      |
| reward_terms/            |            |
|    com_reward            | 0.0211     |
|    end_effectors_reward  | 0.0237     |
|    height_reward         | 0.0468     |
|    joints_reward         | 0.00672    |
|    joints_vel_reward     | 1.21e-05   |
|    root_ori_reward       | 0.00133    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 102        |
|    ep_rew_mean           | 7.17       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 540        |
|    iterations            | 10         |
|    time_elapsed          | 1515       |
|    total_timesteps       | 819200     |
|    train_time            | 31.9       |
| train/                   |            |
|    approx_kl             | 0.08362208 |
|    clip_fraction         | 0.508      |
|    clip_range            | 0.2        |
|    entropy_loss          | -4.91      |
|    explained_variance    | 0.922      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0672    |
|    n_updates             | 90         |
|    policy_gradient_loss  | -0.0574    |
|    std                   | 0.27       |
|    value_loss            | 0.0679     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.336      |
|    end_effectors_err     | 1.35       |
|    height_err            | -0.0764    |
|    joints_err            | 11.6       |
|    joints_vel_err        | 178        |
|    root_ori_err          | 0.21       |
| reward_terms/            |            |
|    com_reward            | 0.0377     |
|    end_effectors_reward  | 0.027      |
|    height_reward         | 0.0714     |
|    joints_reward         | 0.00284    |
|    joints_vel_reward     | 8.51e-06   |
|    root_ori_reward       | 0.00111    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 94.4       |
|    ep_rew_mean           | 7.72       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 542        |
|    iterations            | 11         |
|    time_elapsed          | 1661       |
|    total_timesteps       | 901120     |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.09151337 |
|    clip_fraction         | 0.525      |
|    clip_range            | 0.2        |
|    entropy_loss          | -4.86      |
|    explained_variance    | 0.927      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0396    |
|    n_updates             | 100        |
|    policy_gradient_loss  | -0.06      |
|    std                   | 0.27       |
|    value_loss            | 0.0571     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.239      |
|    end_effectors_err     | 1.3        |
|    height_err            | -0.128     |
|    joints_err            | 11.8       |
|    joints_vel_err        | 180        |
|    root_ori_err          | 0.0677     |
| reward_terms/            |            |
|    com_reward            | 0.0505     |
|    end_effectors_reward  | 0.0253     |
|    height_reward         | 0.0457     |
|    joints_reward         | 0.0128     |
|    joints_vel_reward     | 8e-06      |
|    root_ori_reward       | 0.00246    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 112        |
|    ep_rew_mean           | 7.66       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 543        |
|    iterations            | 12         |
|    time_elapsed          | 1807       |
|    total_timesteps       | 983040     |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.09177848 |
|    clip_fraction         | 0.529      |
|    clip_range            | 0.2        |
|    entropy_loss          | -4.83      |
|    explained_variance    | 0.931      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0173    |
|    n_updates             | 110        |
|    policy_gradient_loss  | -0.0546    |
|    std                   | 0.27       |
|    value_loss            | 0.0618     |
-----------------------------------------
------------------------------------------
| err_terms/               |             |
|    com_err               | 0.59        |
|    end_effectors_err     | 3.11        |
|    height_err            | -0.161      |
|    joints_err            | 12.4        |
|    joints_vel_err        | 199         |
|    root_ori_err          | 1.1         |
| reward_terms/            |             |
|    com_reward            | 0.0249      |
|    end_effectors_reward  | 0.0191      |
|    height_reward         | 0.0413      |
|    joints_reward         | 0.0033      |
|    joints_vel_reward     | 4.28e-06    |
|    root_ori_reward       | 0.00107     |
|    smoothness1_reward    | 0           |
|    smoothness2_reward    | 0           |
|    smoothness_reward     | 0           |
| rollout/                 |             |
|    ep_len_mean           | 118         |
|    ep_rew_mean           | 7.95        |
| time/                    |             |
|    collect_time          | 113         |
|    evaluate_actions_time | 0.00199     |
|    fps                   | 545         |
|    iterations            | 13          |
|    time_elapsed          | 1951        |
|    total_timesteps       | 1064960     |
|    train_time            | 31.3        |
| train/                   |             |
|    approx_kl             | 0.096240535 |
|    clip_fraction         | 0.538       |
|    clip_range            | 0.2         |
|    entropy_loss          | -4.79       |
|    explained_variance    | 0.937       |
|    learning_rate_log_std | 0.0003      |
|    learning_rate_policy  | 5e-05       |
|    learning_rate_value   | 0.01        |
|    loss                  | -0.0532     |
|    n_updates             | 120         |
|    policy_gradient_loss  | -0.0591     |
|    std                   | 0.27        |
|    value_loss            | 0.0422      |
------------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.0287     |
|    end_effectors_err     | 0.0814     |
|    height_err            | 0.0131     |
|    joints_err            | 0.586      |
|    joints_vel_err        | 626        |
|    root_ori_err          | 0.00916    |
| reward_terms/            |            |
|    com_reward            | 0.0989     |
|    end_effectors_reward  | 0.0476     |
|    height_reward         | 0.0991     |
|    joints_reward         | 0.215      |
|    joints_vel_reward     | 9.49e-23   |
|    root_ori_reward       | 0.0319     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 137        |
|    ep_rew_mean           | 8.6        |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 547        |
|    iterations            | 14         |
|    time_elapsed          | 2095       |
|    total_timesteps       | 1146880    |
|    train_time            | 31.4       |
| train/                   |            |
|    approx_kl             | 0.09648362 |
|    clip_fraction         | 0.545      |
|    clip_range            | 0.2        |
|    entropy_loss          | -4.76      |
|    explained_variance    | 0.942      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0516    |
|    n_updates             | 130        |
|    policy_gradient_loss  | -0.0554    |
|    std                   | 0.269      |
|    value_loss            | 0.0472     |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.0104    |
|    end_effectors_err     | 0.0159    |
|    height_err            | -0.00664  |
|    joints_err            | 0.258     |
|    joints_vel_err        | 1.15e+03  |
|    root_ori_err          | 0.00123   |
| reward_terms/            |           |
|    com_reward            | 0.0999    |
|    end_effectors_reward  | 0.0495    |
|    height_reward         | 0.0998    |
|    joints_reward         | 0.304     |
|    joints_vel_reward     | 2.19e-49  |
|    root_ori_reward       | 0.0611    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 129       |
|    ep_rew_mean           | 8.01      |
| time/                    |           |
|    collect_time          | 112       |
|    evaluate_actions_time | 0.00199   |
|    fps                   | 548       |
|    iterations            | 15        |
|    time_elapsed          | 2238      |
|    total_timesteps       | 1228800   |
|    train_time            | 31.3      |
| train/                   |           |
|    approx_kl             | 0.0987073 |
|    clip_fraction         | 0.545     |
|    clip_range            | 0.2       |
|    entropy_loss          | -4.72     |
|    explained_variance    | 0.94      |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0623   |
|    n_updates             | 140       |
|    policy_gradient_loss  | -0.056    |
|    std                   | 0.269     |
|    value_loss            | 0.045     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.0147     |
|    end_effectors_err     | 0.0395     |
|    height_err            | -0.00234   |
|    joints_err            | 0.529      |
|    joints_vel_err        | 841        |
|    root_ori_err          | 0.00235    |
| reward_terms/            |            |
|    com_reward            | 0.0997     |
|    end_effectors_reward  | 0.0488     |
|    height_reward         | 0.0999     |
|    joints_reward         | 0.211      |
|    joints_vel_reward     | 1.38e-30   |
|    root_ori_reward       | 0.0491     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 163        |
|    ep_rew_mean           | 8.8        |
| time/                    |            |
|    collect_time          | 111        |
|    evaluate_actions_time | 0.00199    |
|    fps                   | 550        |
|    iterations            | 16         |
|    time_elapsed          | 2381       |
|    total_timesteps       | 1310720    |
|    train_time            | 31.1       |
| train/                   |            |
|    approx_kl             | 0.10104082 |
|    clip_fraction         | 0.551      |
|    clip_range            | 0.2        |
|    entropy_loss          | -4.65      |
|    explained_variance    | 0.945      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0466    |
|    n_updates             | 150        |
|    policy_gradient_loss  | -0.0566    |
|    std                   | 0.269      |
|    value_loss            | 0.0422     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.322      |
|    end_effectors_err     | 1.33       |
|    height_err            | -0.168     |
|    joints_err            | 11.9       |
|    joints_vel_err        | 156        |
|    root_ori_err          | 0.182      |
| reward_terms/            |            |
|    com_reward            | 0.0313     |
|    end_effectors_reward  | 0.0257     |
|    height_reward         | 0.0307     |
|    joints_reward         | 0.00722    |
|    joints_vel_reward     | 1.55e-05   |
|    root_ori_reward       | 0.00173    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 177        |
|    ep_rew_mean           | 9.12       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 551        |
|    iterations            | 17         |
|    time_elapsed          | 2527       |
|    total_timesteps       | 1392640    |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.10249901 |
|    clip_fraction         | 0.554      |
|    clip_range            | 0.2        |
|    entropy_loss          | -4.55      |
|    explained_variance    | 0.948      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0885    |
|    n_updates             | 160        |
|    policy_gradient_loss  | -0.0584    |
|    std                   | 0.268      |
|    value_loss            | 0.0359     |
-----------------------------------------
------------------------------------------
| err_terms/               |             |
|    com_err               | 0.0127      |
|    end_effectors_err     | 0.023       |
|    height_err            | -0.00979    |
|    joints_err            | 0.262       |
|    joints_vel_err        | 1.11e+03    |
|    root_ori_err          | 0.000499    |
| reward_terms/            |             |
|    com_reward            | 0.0998      |
|    end_effectors_reward  | 0.0493      |
|    height_reward         | 0.0995      |
|    joints_reward         | 0.302       |
|    joints_vel_reward     | 9.59e-48    |
|    root_ori_reward       | 0.0819      |
|    smoothness1_reward    | 0           |
|    smoothness2_reward    | 0           |
|    smoothness_reward     | 0           |
| rollout/                 |             |
|    ep_len_mean           | 158         |
|    ep_rew_mean           | 8.48        |
| time/                    |             |
|    collect_time          | 112         |
|    evaluate_actions_time | 0.00202     |
|    fps                   | 551         |
|    iterations            | 18          |
|    time_elapsed          | 2671        |
|    total_timesteps       | 1474560     |
|    train_time            | 32          |
| train/                   |             |
|    approx_kl             | 0.107811786 |
|    clip_fraction         | 0.558       |
|    clip_range            | 0.2         |
|    entropy_loss          | -4.5        |
|    explained_variance    | 0.944       |
|    learning_rate_log_std | 0.0003      |
|    learning_rate_policy  | 5e-05       |
|    learning_rate_value   | 0.01        |
|    loss                  | -0.0407     |
|    n_updates             | 170         |
|    policy_gradient_loss  | -0.0569     |
|    std                   | 0.268       |
|    value_loss            | 0.0388      |
------------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.315      |
|    end_effectors_err     | 1.18       |
|    height_err            | -0.121     |
|    joints_err            | 9.98       |
|    joints_vel_err        | 191        |
|    root_ori_err          | 0.444      |
| reward_terms/            |            |
|    com_reward            | 0.0468     |
|    end_effectors_reward  | 0.0306     |
|    height_reward         | 0.053      |
|    joints_reward         | 0.0101     |
|    joints_vel_reward     | 3.76e-06   |
|    root_ori_reward       | 0.00375    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 185        |
|    ep_rew_mean           | 9.71       |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00199    |
|    fps                   | 552        |
|    iterations            | 19         |
|    time_elapsed          | 2817       |
|    total_timesteps       | 1556480    |
|    train_time            | 31.3       |
| train/                   |            |
|    approx_kl             | 0.11586305 |
|    clip_fraction         | 0.569      |
|    clip_range            | 0.2        |
|    entropy_loss          | -4.42      |
|    explained_variance    | 0.95       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0505    |
|    n_updates             | 180        |
|    policy_gradient_loss  | -0.0568    |
|    std                   | 0.267      |
|    value_loss            | 0.0321     |
-----------------------------------------
------------------------------------------
| err_terms/               |             |
|    com_err               | 0.0122      |
|    end_effectors_err     | 0.035       |
|    height_err            | -0.00315    |
|    joints_err            | 0.401       |
|    joints_vel_err        | 685         |
|    root_ori_err          | 0.000868    |
| reward_terms/            |             |
|    com_reward            | 0.0998      |
|    end_effectors_reward  | 0.0489      |
|    height_reward         | 0.0999      |
|    joints_reward         | 0.255       |
|    joints_vel_reward     | 2.76e-28    |
|    root_ori_reward       | 0.0723      |
|    smoothness1_reward    | 0           |
|    smoothness2_reward    | 0           |
|    smoothness_reward     | 0           |
| rollout/                 |             |
|    ep_len_mean           | 211         |
|    ep_rew_mean           | 10.2        |
| time/                    |             |
|    collect_time          | 113         |
|    evaluate_actions_time | 0.00202     |
|    fps                   | 552         |
|    iterations            | 20          |
|    time_elapsed          | 2963        |
|    total_timesteps       | 1638400     |
|    train_time            | 31.9        |
| train/                   |             |
|    approx_kl             | 0.110011734 |
|    clip_fraction         | 0.564       |
|    clip_range            | 0.2         |
|    entropy_loss          | -4.36       |
|    explained_variance    | 0.942       |
|    learning_rate_log_std | 0.0003      |
|    learning_rate_policy  | 5e-05       |
|    learning_rate_value   | 0.01        |
|    loss                  | -0.0525     |
|    n_updates             | 190         |
|    policy_gradient_loss  | -0.0555     |
|    std                   | 0.267       |
|    value_loss            | 0.036       |
------------------------------------------
------------------------------------------
| err_terms/               |             |
|    com_err               | 0.358       |
|    end_effectors_err     | 2.04        |
|    height_err            | -0.139      |
|    joints_err            | 11.3        |
|    joints_vel_err        | 154         |
|    root_ori_err          | 0.0983      |
| reward_terms/            |             |
|    com_reward            | 0.038       |
|    end_effectors_reward  | 0.0219      |
|    height_reward         | 0.0416      |
|    joints_reward         | 0.0089      |
|    joints_vel_reward     | 3.36e-05    |
|    root_ori_reward       | 0.00427     |
|    smoothness1_reward    | 0           |
|    smoothness2_reward    | 0           |
|    smoothness_reward     | 0           |
| rollout/                 |             |
|    ep_len_mean           | 222         |
|    ep_rew_mean           | 10.4        |
| time/                    |             |
|    collect_time          | 112         |
|    evaluate_actions_time | 0.00201     |
|    fps                   | 553         |
|    iterations            | 21          |
|    time_elapsed          | 3106        |
|    total_timesteps       | 1720320     |
|    train_time            | 31.6        |
| train/                   |             |
|    approx_kl             | 0.119422354 |
|    clip_fraction         | 0.573       |
|    clip_range            | 0.2         |
|    entropy_loss          | -4.33       |
|    explained_variance    | 0.951       |
|    learning_rate_log_std | 0.0003      |
|    learning_rate_policy  | 5e-05       |
|    learning_rate_value   | 0.01        |
|    loss                  | -0.0753     |
|    n_updates             | 200         |
|    policy_gradient_loss  | -0.0553     |
|    std                   | 0.267       |
|    value_loss            | 0.0271      |
------------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.0316     |
|    end_effectors_err     | 0.0854     |
|    height_err            | -0.0222    |
|    joints_err            | 1.04       |
|    joints_vel_err        | 925        |
|    root_ori_err          | 0.00337    |
| reward_terms/            |            |
|    com_reward            | 0.0985     |
|    end_effectors_reward  | 0.0475     |
|    height_reward         | 0.0969     |
|    joints_reward         | 0.127      |
|    joints_vel_reward     | 2.33e-12   |
|    root_ori_reward       | 0.0359     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 220        |
|    ep_rew_mean           | 10.7       |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 554        |
|    iterations            | 22         |
|    time_elapsed          | 3250       |
|    total_timesteps       | 1802240    |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.12235203 |
|    clip_fraction         | 0.578      |
|    clip_range            | 0.2        |
|    entropy_loss          | -4.28      |
|    explained_variance    | 0.947      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0478    |
|    n_updates             | 210        |
|    policy_gradient_loss  | -0.0586    |
|    std                   | 0.267      |
|    value_loss            | 0.0293     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.144      |
|    end_effectors_err     | 0.122      |
|    height_err            | -0.101     |
|    joints_err            | 6.11       |
|    joints_vel_err        | 250        |
|    root_ori_err          | 0.0128     |
| reward_terms/            |            |
|    com_reward            | 0.0745     |
|    end_effectors_reward  | 0.0464     |
|    height_reward         | 0.0588     |
|    joints_reward         | 0.0296     |
|    joints_vel_reward     | 2.01e-06   |
|    root_ori_reward       | 0.0326     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 201        |
|    ep_rew_mean           | 10.4       |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.00196    |
|    fps                   | 555        |
|    iterations            | 23         |
|    time_elapsed          | 3393       |
|    total_timesteps       | 1884160    |
|    train_time            | 30.9       |
| train/                   |            |
|    approx_kl             | 0.11704423 |
|    clip_fraction         | 0.574      |
|    clip_range            | 0.2        |
|    entropy_loss          | -4.24      |
|    explained_variance    | 0.951      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0744    |
|    n_updates             | 220        |
|    policy_gradient_loss  | -0.0555    |
|    std                   | 0.266      |
|    value_loss            | 0.0341     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.0197     |
|    end_effectors_err     | 0.0634     |
|    height_err            | -0.0183    |
|    joints_err            | 0.829      |
|    joints_vel_err        | 523        |
|    root_ori_err          | 0.00248    |
| reward_terms/            |            |
|    com_reward            | 0.0994     |
|    end_effectors_reward  | 0.0481     |
|    height_reward         | 0.098      |
|    joints_reward         | 0.162      |
|    joints_vel_reward     | 1.04e-17   |
|    root_ori_reward       | 0.0399     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 195        |
|    ep_rew_mean           | 10.8       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 555        |
|    iterations            | 24         |
|    time_elapsed          | 3539       |
|    total_timesteps       | 1966080    |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.12480633 |
|    clip_fraction         | 0.584      |
|    clip_range            | 0.2        |
|    entropy_loss          | -4.18      |
|    explained_variance    | 0.949      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0195    |
|    n_updates             | 230        |
|    policy_gradient_loss  | -0.0579    |
|    std                   | 0.266      |
|    value_loss            | 0.0308     |
-----------------------------------------
policy converted successfully and saved.
Saving video to /cluster/home/anghosh/DHProject/deep-mimic/log/2023-05-29-07-58-21-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M_1/eval-video-PylocoVanilla-v0-step-10-to-step-1010.mp4
Eval num_timesteps=2000000, episode_reward=19.31 +/- 0.00
Episode length: 527.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.423      |
|    end_effectors_err     | 1.95       |
|    height_err            | -0.117     |
|    joints_err            | 11.2       |
|    joints_vel_err        | 165        |
|    root_ori_err          | 0.225      |
| eval/                    |            |
|    mean_ep_length        | 527        |
|    mean_reward           | 19.3       |
|    success_rate          | 1          |
| reward_terms/            |            |
|    com_reward            | 0.029      |
|    end_effectors_reward  | 0.0207     |
|    height_reward         | 0.0511     |
|    joints_reward         | 0.00978    |
|    joints_vel_reward     | 1.42e-05   |
|    root_ori_reward       | 0.00936    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| time/                    |            |
|    evaluate_actions_time | 0.00205    |
|    total_timesteps       | 2000000    |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.13077313 |
|    clip_fraction         | 0.583      |
|    clip_range            | 0.2        |
|    entropy_loss          | -4.15      |
|    explained_variance    | 0.944      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0441    |
|    n_updates             | 240        |
|    policy_gradient_loss  | -0.0576    |
|    std                   | 0.266      |
|    value_loss            | 0.0327     |
-----------------------------------------
New best mean reward!
--------------------------------------
| err_terms/              |          |
|    com_err              | 0.0166   |
|    end_effectors_err    | 0.0368   |
|    height_err           | -0.0068  |
|    joints_err           | 0.485    |
|    joints_vel_err       | 728      |
|    root_ori_err         | 0.00182  |
| reward_terms/           |          |
|    com_reward           | 0.0996   |
|    end_effectors_reward | 0.0489   |
|    height_reward        | 0.0998   |
|    joints_reward        | 0.225    |
|    joints_vel_reward    | 1.15e-26 |
|    root_ori_reward      | 0.0492   |
|    smoothness1_reward   | 0        |
|    smoothness2_reward   | 0        |
|    smoothness_reward    | 0        |
| rollout/                |          |
|    ep_len_mean          | 199      |
|    ep_rew_mean          | 10.7     |
| time/                   |          |
|    collect_time         | 6.36e+03 |
|    fps                  | 206      |
|    iterations           | 25       |
|    time_elapsed         | 9932     |
|    total_timesteps      | 2048000  |
--------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.0133    |
|    end_effectors_err     | 0.0513    |
|    height_err            | -0.00875  |
|    joints_err            | 0.616     |
|    joints_vel_err        | 996       |
|    root_ori_err          | 0.000753  |
| reward_terms/            |           |
|    com_reward            | 0.0997    |
|    end_effectors_reward  | 0.0485    |
|    height_reward         | 0.0995    |
|    joints_reward         | 0.186     |
|    joints_vel_reward     | 7.46e-37  |
|    root_ori_reward       | 0.0746    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 204       |
|    ep_rew_mean           | 11.1      |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 211       |
|    iterations            | 26        |
|    time_elapsed          | 10078     |
|    total_timesteps       | 2129920   |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 0.1337592 |
|    clip_fraction         | 0.588     |
|    clip_range            | 0.2       |
|    entropy_loss          | -4.09     |
|    explained_variance    | 0.944     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0893   |
|    n_updates             | 250       |
|    policy_gradient_loss  | -0.0597   |
|    std                   | 0.266     |
|    value_loss            | 0.0328    |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.22       |
|    end_effectors_err     | 13         |
|    height_err            | -0.198     |
|    joints_err            | 14.1       |
|    joints_vel_err        | 156        |
|    root_ori_err          | 1.78       |
| reward_terms/            |            |
|    com_reward            | 0.0052     |
|    end_effectors_reward  | 0.00364    |
|    height_reward         | 0.023      |
|    joints_reward         | 0.00136    |
|    joints_vel_reward     | 9.62e-06   |
|    root_ori_reward       | 0.00133    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 230        |
|    ep_rew_mean           | 12         |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 216        |
|    iterations            | 27         |
|    time_elapsed          | 10223      |
|    total_timesteps       | 2211840    |
|    train_time            | 31.9       |
| train/                   |            |
|    approx_kl             | 0.12596035 |
|    clip_fraction         | 0.584      |
|    clip_range            | 0.2        |
|    entropy_loss          | -4.1       |
|    explained_variance    | 0.952      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0797    |
|    n_updates             | 260        |
|    policy_gradient_loss  | -0.0605    |
|    std                   | 0.266      |
|    value_loss            | 0.0328     |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.339     |
|    end_effectors_err     | 1.7       |
|    height_err            | -0.0422   |
|    joints_err            | 11        |
|    joints_vel_err        | 163       |
|    root_ori_err          | 0.193     |
| reward_terms/            |           |
|    com_reward            | 0.0402    |
|    end_effectors_reward  | 0.0254    |
|    height_reward         | 0.0828    |
|    joints_reward         | 0.0038    |
|    joints_vel_reward     | 1.51e-05  |
|    root_ori_reward       | 0.000748  |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 208       |
|    ep_rew_mean           | 11.3      |
| time/                    |           |
|    collect_time          | 113       |
|    evaluate_actions_time | 0.002     |
|    fps                   | 221       |
|    iterations            | 28        |
|    time_elapsed          | 10367     |
|    total_timesteps       | 2293760   |
|    train_time            | 31.4      |
| train/                   |           |
|    approx_kl             | 0.1257144 |
|    clip_fraction         | 0.584     |
|    clip_range            | 0.2       |
|    entropy_loss          | -4.05     |
|    explained_variance    | 0.946     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0623   |
|    n_updates             | 270       |
|    policy_gradient_loss  | -0.0571   |
|    std                   | 0.265     |
|    value_loss            | 0.0394    |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.575      |
|    end_effectors_err     | 2.29       |
|    height_err            | -0.11      |
|    joints_err            | 11.3       |
|    joints_vel_err        | 144        |
|    root_ori_err          | 0.237      |
| reward_terms/            |            |
|    com_reward            | 0.0247     |
|    end_effectors_reward  | 0.0209     |
|    height_reward         | 0.0528     |
|    joints_reward         | 0.00397    |
|    joints_vel_reward     | 1.5e-05    |
|    root_ori_reward       | 0.00246    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 237        |
|    ep_rew_mean           | 12.3       |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.00198    |
|    fps                   | 226        |
|    iterations            | 29         |
|    time_elapsed          | 10511      |
|    total_timesteps       | 2375680    |
|    train_time            | 31.1       |
| train/                   |            |
|    approx_kl             | 0.13215394 |
|    clip_fraction         | 0.589      |
|    clip_range            | 0.2        |
|    entropy_loss          | -4         |
|    explained_variance    | 0.948      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0789    |
|    n_updates             | 280        |
|    policy_gradient_loss  | -0.0574    |
|    std                   | 0.265      |
|    value_loss            | 0.0359     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.13       |
|    end_effectors_err     | 11.7       |
|    height_err            | -0.19      |
|    joints_err            | 14         |
|    joints_vel_err        | 173        |
|    root_ori_err          | 1.56       |
| reward_terms/            |            |
|    com_reward            | 0.00601    |
|    end_effectors_reward  | 0.00518    |
|    height_reward         | 0.0259     |
|    joints_reward         | 0.00122    |
|    joints_vel_reward     | 6.05e-06   |
|    root_ori_reward       | 0.00063    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 214        |
|    ep_rew_mean           | 12.5       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 230        |
|    iterations            | 30         |
|    time_elapsed          | 10656      |
|    total_timesteps       | 2457600    |
|    train_time            | 32         |
| train/                   |            |
|    approx_kl             | 0.13496971 |
|    clip_fraction         | 0.592      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.96      |
|    explained_variance    | 0.944      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.084     |
|    n_updates             | 290        |
|    policy_gradient_loss  | -0.0598    |
|    std                   | 0.265      |
|    value_loss            | 0.0375     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.945      |
|    end_effectors_err     | 7.19       |
|    height_err            | -0.175     |
|    joints_err            | 14.1       |
|    joints_vel_err        | 150        |
|    root_ori_err          | 1.64       |
| reward_terms/            |            |
|    com_reward            | 0.00754    |
|    end_effectors_reward  | 0.0073     |
|    height_reward         | 0.0294     |
|    joints_reward         | 0.00167    |
|    joints_vel_reward     | 2.83e-05   |
|    root_ori_reward       | 0.00172    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 196        |
|    ep_rew_mean           | 11.8       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 235        |
|    iterations            | 31         |
|    time_elapsed          | 10802      |
|    total_timesteps       | 2539520    |
|    train_time            | 32.4       |
| train/                   |            |
|    approx_kl             | 0.13223684 |
|    clip_fraction         | 0.591      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.93      |
|    explained_variance    | 0.944      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0303    |
|    n_updates             | 300        |
|    policy_gradient_loss  | -0.0572    |
|    std                   | 0.264      |
|    value_loss            | 0.0401     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.782      |
|    end_effectors_err     | 7.04       |
|    height_err            | -0.169     |
|    joints_err            | 13.1       |
|    joints_vel_err        | 162        |
|    root_ori_err          | 1.47       |
| reward_terms/            |            |
|    com_reward            | 0.0124     |
|    end_effectors_reward  | 0.0096     |
|    height_reward         | 0.0334     |
|    joints_reward         | 0.00129    |
|    joints_vel_reward     | 9.56e-06   |
|    root_ori_reward       | 0.000307   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 191        |
|    ep_rew_mean           | 11.9       |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 239        |
|    iterations            | 32         |
|    time_elapsed          | 10949      |
|    total_timesteps       | 2621440    |
|    train_time            | 32.3       |
| train/                   |            |
|    approx_kl             | 0.14257267 |
|    clip_fraction         | 0.603      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.89      |
|    explained_variance    | 0.947      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0476    |
|    n_updates             | 310        |
|    policy_gradient_loss  | -0.0612    |
|    std                   | 0.264      |
|    value_loss            | 0.0376     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.36       |
|    end_effectors_err     | 14.5       |
|    height_err            | -0.174     |
|    joints_err            | 14.2       |
|    joints_vel_err        | 179        |
|    root_ori_err          | 1.66       |
| reward_terms/            |            |
|    com_reward            | 0.00461    |
|    end_effectors_reward  | 0.00379    |
|    height_reward         | 0.0326     |
|    joints_reward         | 0.00104    |
|    joints_vel_reward     | 6.17e-06   |
|    root_ori_reward       | 0.00035    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 228        |
|    ep_rew_mean           | 13.7       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 243        |
|    iterations            | 33         |
|    time_elapsed          | 11095      |
|    total_timesteps       | 2703360    |
|    train_time            | 32.3       |
| train/                   |            |
|    approx_kl             | 0.14476506 |
|    clip_fraction         | 0.604      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.91      |
|    explained_variance    | 0.942      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0847    |
|    n_updates             | 320        |
|    policy_gradient_loss  | -0.0619    |
|    std                   | 0.264      |
|    value_loss            | 0.0374     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.393      |
|    end_effectors_err     | 1.45       |
|    height_err            | -0.144     |
|    joints_err            | 11.9       |
|    joints_vel_err        | 156        |
|    root_ori_err          | 0.156      |
| reward_terms/            |            |
|    com_reward            | 0.0278     |
|    end_effectors_reward  | 0.0242     |
|    height_reward         | 0.041      |
|    joints_reward         | 0.00612    |
|    joints_vel_reward     | 7.81e-05   |
|    root_ori_reward       | 0.00168    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 238        |
|    ep_rew_mean           | 14         |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 247        |
|    iterations            | 34         |
|    time_elapsed          | 11242      |
|    total_timesteps       | 2785280    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.14748132 |
|    clip_fraction         | 0.607      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.87      |
|    explained_variance    | 0.95       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0858    |
|    n_updates             | 330        |
|    policy_gradient_loss  | -0.0632    |
|    std                   | 0.264      |
|    value_loss            | 0.0324     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.385      |
|    end_effectors_err     | 1.49       |
|    height_err            | -0.15      |
|    joints_err            | 11.4       |
|    joints_vel_err        | 158        |
|    root_ori_err          | 0.131      |
| reward_terms/            |            |
|    com_reward            | 0.0268     |
|    end_effectors_reward  | 0.0246     |
|    height_reward         | 0.0386     |
|    joints_reward         | 0.00595    |
|    joints_vel_reward     | 1.92e-05   |
|    root_ori_reward       | 0.00315    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 252        |
|    ep_rew_mean           | 14.8       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 251        |
|    iterations            | 35         |
|    time_elapsed          | 11387      |
|    total_timesteps       | 2867200    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.14127354 |
|    clip_fraction         | 0.606      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.84      |
|    explained_variance    | 0.94       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0823    |
|    n_updates             | 340        |
|    policy_gradient_loss  | -0.0595    |
|    std                   | 0.264      |
|    value_loss            | 0.0409     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.0297     |
|    end_effectors_err     | 0.109      |
|    height_err            | -0.0217    |
|    joints_err            | 1.86       |
|    joints_vel_err        | 460        |
|    root_ori_err          | 0.00259    |
| reward_terms/            |            |
|    com_reward            | 0.0985     |
|    end_effectors_reward  | 0.0468     |
|    height_reward         | 0.0967     |
|    joints_reward         | 0.0837     |
|    joints_vel_reward     | 6.42e-12   |
|    root_ori_reward       | 0.0399     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 251        |
|    ep_rew_mean           | 15.4       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 255        |
|    iterations            | 36         |
|    time_elapsed          | 11531      |
|    total_timesteps       | 2949120    |
|    train_time            | 31.4       |
| train/                   |            |
|    approx_kl             | 0.15367283 |
|    clip_fraction         | 0.611      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.8       |
|    explained_variance    | 0.94       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0714    |
|    n_updates             | 350        |
|    policy_gradient_loss  | -0.062     |
|    std                   | 0.264      |
|    value_loss            | 0.0341     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.555      |
|    end_effectors_err     | 2.62       |
|    height_err            | -0.107     |
|    joints_err            | 11.4       |
|    joints_vel_err        | 176        |
|    root_ori_err          | 0.498      |
| reward_terms/            |            |
|    com_reward            | 0.0206     |
|    end_effectors_reward  | 0.0161     |
|    height_reward         | 0.0579     |
|    joints_reward         | 0.00528    |
|    joints_vel_reward     | 7.33e-06   |
|    root_ori_reward       | 0.00149    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 228        |
|    ep_rew_mean           | 14.6       |
| time/                    |            |
|    collect_time          | 116        |
|    evaluate_actions_time | 0.00205    |
|    fps                   | 259        |
|    iterations            | 37         |
|    time_elapsed          | 11680      |
|    total_timesteps       | 3031040    |
|    train_time            | 32.3       |
| train/                   |            |
|    approx_kl             | 0.16072129 |
|    clip_fraction         | 0.624      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.77      |
|    explained_variance    | 0.95       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0637    |
|    n_updates             | 360        |
|    policy_gradient_loss  | -0.0687    |
|    std                   | 0.264      |
|    value_loss            | 0.0318     |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 1.05      |
|    end_effectors_err     | 9.54      |
|    height_err            | -0.151    |
|    joints_err            | 14        |
|    joints_vel_err        | 145       |
|    root_ori_err          | 1.47      |
| reward_terms/            |           |
|    com_reward            | 0.0138    |
|    end_effectors_reward  | 0.00741   |
|    height_reward         | 0.0391    |
|    joints_reward         | 0.00104   |
|    joints_vel_reward     | 1.67e-05  |
|    root_ori_reward       | 0.000324  |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 267       |
|    ep_rew_mean           | 16        |
| time/                    |           |
|    collect_time          | 113       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 263       |
|    iterations            | 38        |
|    time_elapsed          | 11824     |
|    total_timesteps       | 3112960   |
|    train_time            | 31.7      |
| train/                   |           |
|    approx_kl             | 0.1573281 |
|    clip_fraction         | 0.62      |
|    clip_range            | 0.2       |
|    entropy_loss          | -3.72     |
|    explained_variance    | 0.945     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.049    |
|    n_updates             | 370       |
|    policy_gradient_loss  | -0.0661   |
|    std                   | 0.263     |
|    value_loss            | 0.0346    |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.37       |
|    end_effectors_err     | 15.5       |
|    height_err            | -0.153     |
|    joints_err            | 14.7       |
|    joints_vel_err        | 147        |
|    root_ori_err          | 1.79       |
| reward_terms/            |            |
|    com_reward            | 0.00374    |
|    end_effectors_reward  | 0.00249    |
|    height_reward         | 0.039      |
|    joints_reward         | 0.000968   |
|    joints_vel_reward     | 1.31e-05   |
|    root_ori_reward       | 0.000925   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 270        |
|    ep_rew_mean           | 16.4       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00205    |
|    fps                   | 266        |
|    iterations            | 39         |
|    time_elapsed          | 11970      |
|    total_timesteps       | 3194880    |
|    train_time            | 32         |
| train/                   |            |
|    approx_kl             | 0.15867922 |
|    clip_fraction         | 0.622      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.66      |
|    explained_variance    | 0.946      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0998    |
|    n_updates             | 380        |
|    policy_gradient_loss  | -0.0674    |
|    std                   | 0.263      |
|    value_loss            | 0.0308     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.255      |
|    end_effectors_err     | 0.282      |
|    height_err            | -0.0907    |
|    joints_err            | 7.59       |
|    joints_vel_err        | 224        |
|    root_ori_err          | 0.0606     |
| reward_terms/            |            |
|    com_reward            | 0.0448     |
|    end_effectors_reward  | 0.0421     |
|    height_reward         | 0.0657     |
|    joints_reward         | 0.00709    |
|    joints_vel_reward     | 8.58e-06   |
|    root_ori_reward       | 0.00296    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 270        |
|    ep_rew_mean           | 16.9       |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 270        |
|    iterations            | 40         |
|    time_elapsed          | 12117      |
|    total_timesteps       | 3276800    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.15687199 |
|    clip_fraction         | 0.623      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.62      |
|    explained_variance    | 0.939      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0713    |
|    n_updates             | 390        |
|    policy_gradient_loss  | -0.062     |
|    std                   | 0.263      |
|    value_loss            | 0.0388     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.08       |
|    end_effectors_err     | 10.3       |
|    height_err            | -0.129     |
|    joints_err            | 13.4       |
|    joints_vel_err        | 173        |
|    root_ori_err          | 1.43       |
| reward_terms/            |            |
|    com_reward            | 0.0136     |
|    end_effectors_reward  | 0.00881    |
|    height_reward         | 0.0482     |
|    joints_reward         | 0.00104    |
|    joints_vel_reward     | 8.56e-06   |
|    root_ori_reward       | 0.00115    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 256        |
|    ep_rew_mean           | 17.1       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 273        |
|    iterations            | 41         |
|    time_elapsed          | 12262      |
|    total_timesteps       | 3358720    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.16028953 |
|    clip_fraction         | 0.626      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.55      |
|    explained_variance    | 0.942      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0665    |
|    n_updates             | 400        |
|    policy_gradient_loss  | -0.064     |
|    std                   | 0.262      |
|    value_loss            | 0.0365     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.518      |
|    end_effectors_err     | 2.49       |
|    height_err            | -0.133     |
|    joints_err            | 11.3       |
|    joints_vel_err        | 171        |
|    root_ori_err          | 0.315      |
| reward_terms/            |            |
|    com_reward            | 0.0167     |
|    end_effectors_reward  | 0.0169     |
|    height_reward         | 0.0412     |
|    joints_reward         | 0.00508    |
|    joints_vel_reward     | 1.01e-05   |
|    root_ori_reward       | 0.00281    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 230        |
|    ep_rew_mean           | 16.3       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 277        |
|    iterations            | 42         |
|    time_elapsed          | 12408      |
|    total_timesteps       | 3440640    |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.16309957 |
|    clip_fraction         | 0.628      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.48      |
|    explained_variance    | 0.941      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0845    |
|    n_updates             | 410        |
|    policy_gradient_loss  | -0.0657    |
|    std                   | 0.262      |
|    value_loss            | 0.0377     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.36       |
|    end_effectors_err     | 14.3       |
|    height_err            | -0.116     |
|    joints_err            | 13.8       |
|    joints_vel_err        | 179        |
|    root_ori_err          | 1.23       |
| reward_terms/            |            |
|    com_reward            | 0.0126     |
|    end_effectors_reward  | 0.00942    |
|    height_reward         | 0.0491     |
|    joints_reward         | 0.0011     |
|    joints_vel_reward     | 6.3e-06    |
|    root_ori_reward       | 0.000708   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 268        |
|    ep_rew_mean           | 18.6       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 280        |
|    iterations            | 43         |
|    time_elapsed          | 12553      |
|    total_timesteps       | 3522560    |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.17802449 |
|    clip_fraction         | 0.635      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.4       |
|    explained_variance    | 0.94       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.104     |
|    n_updates             | 420        |
|    policy_gradient_loss  | -0.0681    |
|    std                   | 0.261      |
|    value_loss            | 0.0361     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.626      |
|    end_effectors_err     | 4.67       |
|    height_err            | -0.132     |
|    joints_err            | 12.4       |
|    joints_vel_err        | 176        |
|    root_ori_err          | 0.126      |
| reward_terms/            |            |
|    com_reward            | 0.00873    |
|    end_effectors_reward  | 0.00951    |
|    height_reward         | 0.0427     |
|    joints_reward         | 0.00255    |
|    joints_vel_reward     | 7.29e-06   |
|    root_ori_reward       | 0.00383    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 264        |
|    ep_rew_mean           | 18.4       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 283        |
|    iterations            | 44         |
|    time_elapsed          | 12698      |
|    total_timesteps       | 3604480    |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.17813034 |
|    clip_fraction         | 0.639      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.31      |
|    explained_variance    | 0.933      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0939    |
|    n_updates             | 430        |
|    policy_gradient_loss  | -0.0637    |
|    std                   | 0.261      |
|    value_loss            | 0.0408     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.37       |
|    end_effectors_err     | 14.4       |
|    height_err            | -0.114     |
|    joints_err            | 14.4       |
|    joints_vel_err        | 144        |
|    root_ori_err          | 1.46       |
| reward_terms/            |            |
|    com_reward            | 0.00708    |
|    end_effectors_reward  | 0.00733    |
|    height_reward         | 0.053      |
|    joints_reward         | 0.00128    |
|    joints_vel_reward     | 1.27e-05   |
|    root_ori_reward       | 0.000203   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 250        |
|    ep_rew_mean           | 18.3       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00198    |
|    fps                   | 287        |
|    iterations            | 45         |
|    time_elapsed          | 12843      |
|    total_timesteps       | 3686400    |
|    train_time            | 30.9       |
| train/                   |            |
|    approx_kl             | 0.17171976 |
|    clip_fraction         | 0.639      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.26      |
|    explained_variance    | 0.918      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0642    |
|    n_updates             | 440        |
|    policy_gradient_loss  | -0.064     |
|    std                   | 0.26       |
|    value_loss            | 0.0434     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.976      |
|    end_effectors_err     | 9.25       |
|    height_err            | -0.117     |
|    joints_err            | 14.3       |
|    joints_vel_err        | 165        |
|    root_ori_err          | 1.6        |
| reward_terms/            |            |
|    com_reward            | 0.0119     |
|    end_effectors_reward  | 0.0075     |
|    height_reward         | 0.0528     |
|    joints_reward         | 0.00156    |
|    joints_vel_reward     | 9.04e-06   |
|    root_ori_reward       | 0.00164    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 271        |
|    ep_rew_mean           | 20.5       |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 290        |
|    iterations            | 46         |
|    time_elapsed          | 12987      |
|    total_timesteps       | 3768320    |
|    train_time            | 32         |
| train/                   |            |
|    approx_kl             | 0.17771181 |
|    clip_fraction         | 0.643      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.14      |
|    explained_variance    | 0.932      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0145    |
|    n_updates             | 450        |
|    policy_gradient_loss  | -0.0648    |
|    std                   | 0.26       |
|    value_loss            | 0.0386     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.29       |
|    end_effectors_err     | 13.2       |
|    height_err            | -0.1       |
|    joints_err            | 14.6       |
|    joints_vel_err        | 180        |
|    root_ori_err          | 1.37       |
| reward_terms/            |            |
|    com_reward            | 0.00582    |
|    end_effectors_reward  | 0.00599    |
|    height_reward         | 0.0572     |
|    joints_reward         | 0.00118    |
|    joints_vel_reward     | 6.05e-06   |
|    root_ori_reward       | 0.000536   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 275        |
|    ep_rew_mean           | 20.5       |
| time/                    |            |
|    collect_time          | 112        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 293        |
|    iterations            | 47         |
|    time_elapsed          | 13131      |
|    total_timesteps       | 3850240    |
|    train_time            | 31.5       |
| train/                   |            |
|    approx_kl             | 0.17769797 |
|    clip_fraction         | 0.642      |
|    clip_range            | 0.2        |
|    entropy_loss          | -3.04      |
|    explained_variance    | 0.922      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0826    |
|    n_updates             | 460        |
|    policy_gradient_loss  | -0.0645    |
|    std                   | 0.259      |
|    value_loss            | 0.0445     |
-----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 1.33     |
|    end_effectors_err     | 15.3     |
|    height_err            | -0.0829  |
|    joints_err            | 15.8     |
|    joints_vel_err        | 144      |
|    root_ori_err          | 1.77     |
| reward_terms/            |          |
|    com_reward            | 0.00549  |
|    end_effectors_reward  | 0.00359  |
|    height_reward         | 0.062    |
|    joints_reward         | 0.0014   |
|    joints_vel_reward     | 1.85e-05 |
|    root_ori_reward       | 0.00085  |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 265      |
|    ep_rew_mean           | 20.7     |
| time/                    |          |
|    collect_time          | 113      |
|    evaluate_actions_time | 0.002    |
|    fps                   | 296      |
|    iterations            | 48       |
|    time_elapsed          | 13276    |
|    total_timesteps       | 3932160  |
|    train_time            | 31.8     |
| train/                   |          |
|    approx_kl             | 0.192848 |
|    clip_fraction         | 0.649    |
|    clip_range            | 0.2      |
|    entropy_loss          | -2.95    |
|    explained_variance    | 0.906    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | -0.0761  |
|    n_updates             | 470      |
|    policy_gradient_loss  | -0.0668  |
|    std                   | 0.259    |
|    value_loss            | 0.0479   |
---------------------------------------
policy converted successfully and saved.
Saving video to /cluster/home/anghosh/DHProject/deep-mimic/log/2023-05-29-07-58-21-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M_1/eval-video-PylocoVanilla-v0-step-2645-to-step-3645.mp4
Eval num_timesteps=4000000, episode_reward=38.71 +/- 0.00
Episode length: 527.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.3        |
|    end_effectors_err     | 14.5       |
|    height_err            | -0.0788    |
|    joints_err            | 15.9       |
|    joints_vel_err        | 137        |
|    root_ori_err          | 1.82       |
| eval/                    |            |
|    mean_ep_length        | 527        |
|    mean_reward           | 38.7       |
|    success_rate          | 1          |
| reward_terms/            |            |
|    com_reward            | 0.00434    |
|    end_effectors_reward  | 0.00287    |
|    height_reward         | 0.0624     |
|    joints_reward         | 0.00137    |
|    joints_vel_reward     | 2.24e-05   |
|    root_ori_reward       | 0.000712   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| time/                    |            |
|    evaluate_actions_time | 0.00206    |
|    total_timesteps       | 4000000    |
|    train_time            | 32.2       |
| train/                   |            |
|    approx_kl             | 0.18334325 |
|    clip_fraction         | 0.646      |
|    clip_range            | 0.2        |
|    entropy_loss          | -2.89      |
|    explained_variance    | 0.906      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0692    |
|    n_updates             | 480        |
|    policy_gradient_loss  | -0.0701    |
|    std                   | 0.258      |
|    value_loss            | 0.0422     |
-----------------------------------------
New best mean reward!
---------------------------------------
| err_terms/              |           |
|    com_err              | 0.0216    |
|    end_effectors_err    | 0.152     |
|    height_err           | -0.000991 |
|    joints_err           | 1.51      |
|    joints_vel_err       | 505       |
|    root_ori_err         | 0.00204   |
| reward_terms/           |           |
|    com_reward           | 0.0992    |
|    end_effectors_reward | 0.0456    |
|    height_reward        | 0.0996    |
|    joints_reward        | 0.103     |
|    joints_vel_reward    | 2.37e-12  |
|    root_ori_reward      | 0.0568    |
|    smoothness1_reward   | 0         |
|    smoothness2_reward   | 0         |
|    smoothness_reward    | 0         |
| rollout/                |           |
|    ep_len_mean          | 243       |
|    ep_rew_mean          | 20.1      |
| time/                   |           |
|    collect_time         | 6.33e+03  |
|    fps                  | 204       |
|    iterations           | 49        |
|    time_elapsed         | 19639     |
|    total_timesteps      | 4014080   |
---------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.19       |
|    end_effectors_err     | 12.3       |
|    height_err            | -0.0857    |
|    joints_err            | 14.8       |
|    joints_vel_err        | 146        |
|    root_ori_err          | 1.6        |
| reward_terms/            |            |
|    com_reward            | 0.00528    |
|    end_effectors_reward  | 0.00555    |
|    height_reward         | 0.0568     |
|    joints_reward         | 0.00158    |
|    joints_vel_reward     | 1.98e-05   |
|    root_ori_reward       | 0.000945   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 258        |
|    ep_rew_mean           | 21.2       |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 207        |
|    iterations            | 50         |
|    time_elapsed          | 19786      |
|    total_timesteps       | 4096000    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.18382646 |
|    clip_fraction         | 0.647      |
|    clip_range            | 0.2        |
|    entropy_loss          | -2.8       |
|    explained_variance    | 0.907      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | 0.00429    |
|    n_updates             | 490        |
|    policy_gradient_loss  | -0.0659    |
|    std                   | 0.258      |
|    value_loss            | 0.0452     |
-----------------------------------------
---------------------------------------
| err_terms/               |          |
|    com_err               | 1.14     |
|    end_effectors_err     | 11.2     |
|    height_err            | -0.0795  |
|    joints_err            | 15.7     |
|    joints_vel_err        | 152      |
|    root_ori_err          | 1.59     |
| reward_terms/            |          |
|    com_reward            | 0.00443  |
|    end_effectors_reward  | 0.00428  |
|    height_reward         | 0.0635   |
|    joints_reward         | 0.00162  |
|    joints_vel_reward     | 1.23e-05 |
|    root_ori_reward       | 0.00114  |
|    smoothness1_reward    | 0        |
|    smoothness2_reward    | 0        |
|    smoothness_reward     | 0        |
| rollout/                 |          |
|    ep_len_mean           | 301      |
|    ep_rew_mean           | 25       |
| time/                    |          |
|    collect_time          | 115      |
|    evaluate_actions_time | 0.00202  |
|    fps                   | 209      |
|    iterations            | 51       |
|    time_elapsed          | 19933    |
|    total_timesteps       | 4177920  |
|    train_time            | 31.8     |
| train/                   |          |
|    approx_kl             | 0.189219 |
|    clip_fraction         | 0.645    |
|    clip_range            | 0.2      |
|    entropy_loss          | -2.71    |
|    explained_variance    | 0.887    |
|    learning_rate_log_std | 0.0003   |
|    learning_rate_policy  | 5e-05    |
|    learning_rate_value   | 0.01     |
|    loss                  | -0.055   |
|    n_updates             | 500      |
|    policy_gradient_loss  | -0.0647  |
|    std                   | 0.257    |
|    value_loss            | 0.0494   |
---------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.36       |
|    end_effectors_err     | 15.3       |
|    height_err            | -0.071     |
|    joints_err            | 15.7       |
|    joints_vel_err        | 148        |
|    root_ori_err          | 1.55       |
| reward_terms/            |            |
|    com_reward            | 0.00528    |
|    end_effectors_reward  | 0.00528    |
|    height_reward         | 0.0654     |
|    joints_reward         | 0.00147    |
|    joints_vel_reward     | 1.32e-05   |
|    root_ori_reward       | 0.000433   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 289        |
|    ep_rew_mean           | 24.9       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 212        |
|    iterations            | 52         |
|    time_elapsed          | 20079      |
|    total_timesteps       | 4259840    |
|    train_time            | 31.9       |
| train/                   |            |
|    approx_kl             | 0.19395797 |
|    clip_fraction         | 0.649      |
|    clip_range            | 0.2        |
|    entropy_loss          | -2.61      |
|    explained_variance    | 0.898      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0902    |
|    n_updates             | 510        |
|    policy_gradient_loss  | -0.0682    |
|    std                   | 0.257      |
|    value_loss            | 0.043      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.541      |
|    end_effectors_err     | 3.14       |
|    height_err            | -0.0758    |
|    joints_err            | 11.1       |
|    joints_vel_err        | 158        |
|    root_ori_err          | 0.564      |
| reward_terms/            |            |
|    com_reward            | 0.0127     |
|    end_effectors_reward  | 0.0131     |
|    height_reward         | 0.064      |
|    joints_reward         | 0.0046     |
|    joints_vel_reward     | 6.4e-06    |
|    root_ori_reward       | 0.00251    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 279        |
|    ep_rew_mean           | 24.4       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 214        |
|    iterations            | 53         |
|    time_elapsed          | 20225      |
|    total_timesteps       | 4341760    |
|    train_time            | 32.2       |
| train/                   |            |
|    approx_kl             | 0.20130138 |
|    clip_fraction         | 0.653      |
|    clip_range            | 0.2        |
|    entropy_loss          | -2.5       |
|    explained_variance    | 0.892      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0759    |
|    n_updates             | 520        |
|    policy_gradient_loss  | -0.068     |
|    std                   | 0.256      |
|    value_loss            | 0.0479     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.35       |
|    end_effectors_err     | 14.7       |
|    height_err            | -0.0489    |
|    joints_err            | 16.5       |
|    joints_vel_err        | 148        |
|    root_ori_err          | 1.65       |
| reward_terms/            |            |
|    com_reward            | 0.0049     |
|    end_effectors_reward  | 0.00331    |
|    height_reward         | 0.0733     |
|    joints_reward         | 0.00138    |
|    joints_vel_reward     | 1.95e-05   |
|    root_ori_reward       | 0.00135    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 283        |
|    ep_rew_mean           | 25.2       |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 217        |
|    iterations            | 54         |
|    time_elapsed          | 20372      |
|    total_timesteps       | 4423680    |
|    train_time            | 31.5       |
| train/                   |            |
|    approx_kl             | 0.19016795 |
|    clip_fraction         | 0.651      |
|    clip_range            | 0.2        |
|    entropy_loss          | -2.4       |
|    explained_variance    | 0.883      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.11      |
|    n_updates             | 530        |
|    policy_gradient_loss  | -0.0669    |
|    std                   | 0.255      |
|    value_loss            | 0.0529     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.542      |
|    end_effectors_err     | 2.23       |
|    height_err            | -0.061     |
|    joints_err            | 9.97       |
|    joints_vel_err        | 167        |
|    root_ori_err          | 0.427      |
| reward_terms/            |            |
|    com_reward            | 0.018      |
|    end_effectors_reward  | 0.0215     |
|    height_reward         | 0.0686     |
|    joints_reward         | 0.00606    |
|    joints_vel_reward     | 1.08e-05   |
|    root_ori_reward       | 0.00532    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 324        |
|    ep_rew_mean           | 28.6       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 219        |
|    iterations            | 55         |
|    time_elapsed          | 20518      |
|    total_timesteps       | 4505600    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.19738045 |
|    clip_fraction         | 0.652      |
|    clip_range            | 0.2        |
|    entropy_loss          | -2.3       |
|    explained_variance    | 0.874      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0461    |
|    n_updates             | 540        |
|    policy_gradient_loss  | -0.066     |
|    std                   | 0.255      |
|    value_loss            | 0.0545     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.44       |
|    end_effectors_err     | 14.8       |
|    height_err            | -0.0248    |
|    joints_err            | 16.3       |
|    joints_vel_err        | 139        |
|    root_ori_err          | 1.63       |
| reward_terms/            |            |
|    com_reward            | 0.00324    |
|    end_effectors_reward  | 0.00489    |
|    height_reward         | 0.0761     |
|    joints_reward         | 0.00127    |
|    joints_vel_reward     | 2.97e-05   |
|    root_ori_reward       | 0.000516   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 306        |
|    ep_rew_mean           | 27.4       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 221        |
|    iterations            | 56         |
|    time_elapsed          | 20665      |
|    total_timesteps       | 4587520    |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.20069662 |
|    clip_fraction         | 0.657      |
|    clip_range            | 0.2        |
|    entropy_loss          | -2.19      |
|    explained_variance    | 0.88       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.056     |
|    n_updates             | 550        |
|    policy_gradient_loss  | -0.0697    |
|    std                   | 0.254      |
|    value_loss            | 0.0424     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.0369     |
|    end_effectors_err     | 0.108      |
|    height_err            | -0.00642   |
|    joints_err            | 1.65       |
|    joints_vel_err        | 489        |
|    root_ori_err          | 0.00475    |
| reward_terms/            |            |
|    com_reward            | 0.0977     |
|    end_effectors_reward  | 0.0468     |
|    height_reward         | 0.0997     |
|    joints_reward         | 0.0906     |
|    joints_vel_reward     | 2.26e-11   |
|    root_ori_reward       | 0.0217     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 305        |
|    ep_rew_mean           | 28         |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 224        |
|    iterations            | 57         |
|    time_elapsed          | 20811      |
|    total_timesteps       | 4669440    |
|    train_time            | 32         |
| train/                   |            |
|    approx_kl             | 0.21221331 |
|    clip_fraction         | 0.661      |
|    clip_range            | 0.2        |
|    entropy_loss          | -2.09      |
|    explained_variance    | 0.866      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0878    |
|    n_updates             | 560        |
|    policy_gradient_loss  | -0.0717    |
|    std                   | 0.254      |
|    value_loss            | 0.0433     |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 1.39      |
|    end_effectors_err     | 16.2      |
|    height_err            | -0.018    |
|    joints_err            | 16.8      |
|    joints_vel_err        | 139       |
|    root_ori_err          | 1.78      |
| reward_terms/            |           |
|    com_reward            | 0.0051    |
|    end_effectors_reward  | 0.00337   |
|    height_reward         | 0.0787    |
|    joints_reward         | 0.00139   |
|    joints_vel_reward     | 3.47e-05  |
|    root_ori_reward       | 0.001     |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 275       |
|    ep_rew_mean           | 25.6      |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 226       |
|    iterations            | 58        |
|    time_elapsed          | 20956     |
|    total_timesteps       | 4751360   |
|    train_time            | 31.9      |
| train/                   |           |
|    approx_kl             | 0.2142371 |
|    clip_fraction         | 0.661     |
|    clip_range            | 0.2       |
|    entropy_loss          | -1.99     |
|    explained_variance    | 0.836     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0805   |
|    n_updates             | 570       |
|    policy_gradient_loss  | -0.0696   |
|    std                   | 0.253     |
|    value_loss            | 0.0483    |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.0118     |
|    end_effectors_err     | 0.0349     |
|    height_err            | -0.00944   |
|    joints_err            | 0.366      |
|    joints_vel_err        | 607        |
|    root_ori_err          | 0.000903   |
| reward_terms/            |            |
|    com_reward            | 0.0998     |
|    end_effectors_reward  | 0.0489     |
|    height_reward         | 0.0995     |
|    joints_reward         | 0.265      |
|    joints_vel_reward     | 6.72e-23   |
|    root_ori_reward       | 0.0714     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 311        |
|    ep_rew_mean           | 29         |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 229        |
|    iterations            | 59         |
|    time_elapsed          | 21102      |
|    total_timesteps       | 4833280    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.20861225 |
|    clip_fraction         | 0.659      |
|    clip_range            | 0.2        |
|    entropy_loss          | -1.87      |
|    explained_variance    | 0.838      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.112     |
|    n_updates             | 580        |
|    policy_gradient_loss  | -0.0688    |
|    std                   | 0.253      |
|    value_loss            | 0.0508     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.24       |
|    end_effectors_err     | 14.4       |
|    height_err            | -0.0215    |
|    joints_err            | 15.2       |
|    joints_vel_err        | 148        |
|    root_ori_err          | 1.41       |
| reward_terms/            |            |
|    com_reward            | 0.00888    |
|    end_effectors_reward  | 0.00822    |
|    height_reward         | 0.0707     |
|    joints_reward         | 0.00162    |
|    joints_vel_reward     | 1.55e-05   |
|    root_ori_reward       | 0.000193   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 360        |
|    ep_rew_mean           | 33.3       |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 231        |
|    iterations            | 60         |
|    time_elapsed          | 21249      |
|    total_timesteps       | 4915200    |
|    train_time            | 31.9       |
| train/                   |            |
|    approx_kl             | 0.20990138 |
|    clip_fraction         | 0.667      |
|    clip_range            | 0.2        |
|    entropy_loss          | -1.8       |
|    explained_variance    | 0.866      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.081     |
|    n_updates             | 590        |
|    policy_gradient_loss  | -0.0717    |
|    std                   | 0.252      |
|    value_loss            | 0.0446     |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.836     |
|    end_effectors_err     | 5.86      |
|    height_err            | -0.0233   |
|    joints_err            | 9.73      |
|    joints_vel_err        | 175       |
|    root_ori_err          | 0.363     |
| reward_terms/            |           |
|    com_reward            | 0.0096    |
|    end_effectors_reward  | 0.00922   |
|    height_reward         | 0.0717    |
|    joints_reward         | 0.00332   |
|    joints_vel_reward     | 9.98e-06  |
|    root_ori_reward       | 0.00352   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 316       |
|    ep_rew_mean           | 30.1      |
| time/                    |           |
|    collect_time          | 112       |
|    evaluate_actions_time | 0.00203   |
|    fps                   | 233       |
|    iterations            | 61        |
|    time_elapsed          | 21393     |
|    total_timesteps       | 4997120   |
|    train_time            | 32        |
| train/                   |           |
|    approx_kl             | 0.2087284 |
|    clip_fraction         | 0.662     |
|    clip_range            | 0.2       |
|    entropy_loss          | -1.69     |
|    explained_variance    | 0.845     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0745   |
|    n_updates             | 600       |
|    policy_gradient_loss  | -0.07     |
|    std                   | 0.251     |
|    value_loss            | 0.043     |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.0277    |
|    end_effectors_err     | 0.0945    |
|    height_err            | -0.0136   |
|    joints_err            | 1.52      |
|    joints_vel_err        | 497       |
|    root_ori_err          | 0.67      |
| reward_terms/            |           |
|    com_reward            | 0.0989    |
|    end_effectors_reward  | 0.0472    |
|    height_reward         | 0.0989    |
|    joints_reward         | 0.0877    |
|    joints_vel_reward     | 4.08e-10  |
|    root_ori_reward       | 0.0179    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 308       |
|    ep_rew_mean           | 28.8      |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00205   |
|    fps                   | 235       |
|    iterations            | 62        |
|    time_elapsed          | 21540     |
|    total_timesteps       | 5079040   |
|    train_time            | 32.6      |
| train/                   |           |
|    approx_kl             | 0.2189374 |
|    clip_fraction         | 0.664     |
|    clip_range            | 0.2       |
|    entropy_loss          | -1.59     |
|    explained_variance    | 0.842     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0798   |
|    n_updates             | 610       |
|    policy_gradient_loss  | -0.0709   |
|    std                   | 0.251     |
|    value_loss            | 0.0428    |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 1.36      |
|    end_effectors_err     | 13.3      |
|    height_err            | -0.0193   |
|    joints_err            | 16.3      |
|    joints_vel_err        | 134       |
|    root_ori_err          | 1.6       |
| reward_terms/            |           |
|    com_reward            | 0.00422   |
|    end_effectors_reward  | 0.00559   |
|    height_reward         | 0.077     |
|    joints_reward         | 0.00114   |
|    joints_vel_reward     | 3.27e-05  |
|    root_ori_reward       | 0.000794  |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 326       |
|    ep_rew_mean           | 31.5      |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 237       |
|    iterations            | 63        |
|    time_elapsed          | 21687     |
|    total_timesteps       | 5160960   |
|    train_time            | 32.2      |
| train/                   |           |
|    approx_kl             | 0.2118934 |
|    clip_fraction         | 0.665     |
|    clip_range            | 0.2       |
|    entropy_loss          | -1.47     |
|    explained_variance    | 0.83      |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.102    |
|    n_updates             | 620       |
|    policy_gradient_loss  | -0.0696   |
|    std                   | 0.25      |
|    value_loss            | 0.051     |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.00882    |
|    end_effectors_err     | 0.0165     |
|    height_err            | -0.00732   |
|    joints_err            | 0.218      |
|    joints_vel_err        | 893        |
|    root_ori_err          | 0.0011     |
| reward_terms/            |            |
|    com_reward            | 0.0999     |
|    end_effectors_reward  | 0.0495     |
|    height_reward         | 0.0997     |
|    joints_reward         | 0.329      |
|    joints_vel_reward     | 7.46e-39   |
|    root_ori_reward       | 0.0645     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 330        |
|    ep_rew_mean           | 31.4       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 240        |
|    iterations            | 64         |
|    time_elapsed          | 21832      |
|    total_timesteps       | 5242880    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.21078655 |
|    clip_fraction         | 0.663      |
|    clip_range            | 0.2        |
|    entropy_loss          | -1.35      |
|    explained_variance    | 0.848      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.11      |
|    n_updates             | 630        |
|    policy_gradient_loss  | -0.0709    |
|    std                   | 0.249      |
|    value_loss            | 0.0464     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.021      |
|    end_effectors_err     | 0.0649     |
|    height_err            | -0.0167    |
|    joints_err            | 0.872      |
|    joints_vel_err        | 733        |
|    root_ori_err          | 0.00495    |
| reward_terms/            |            |
|    com_reward            | 0.0993     |
|    end_effectors_reward  | 0.0481     |
|    height_reward         | 0.0982     |
|    joints_reward         | 0.137      |
|    joints_vel_reward     | 8.81e-20   |
|    root_ori_reward       | 0.0278     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 311        |
|    ep_rew_mean           | 30         |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 242        |
|    iterations            | 65         |
|    time_elapsed          | 21979      |
|    total_timesteps       | 5324800    |
|    train_time            | 31.3       |
| train/                   |            |
|    approx_kl             | 0.21664591 |
|    clip_fraction         | 0.668      |
|    clip_range            | 0.2        |
|    entropy_loss          | -1.22      |
|    explained_variance    | 0.852      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.11      |
|    n_updates             | 640        |
|    policy_gradient_loss  | -0.0719    |
|    std                   | 0.249      |
|    value_loss            | 0.0437     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.633      |
|    end_effectors_err     | 3.16       |
|    height_err            | -0.0227    |
|    joints_err            | 9.59       |
|    joints_vel_err        | 166        |
|    root_ori_err          | 0.289      |
| reward_terms/            |            |
|    com_reward            | 0.0252     |
|    end_effectors_reward  | 0.0186     |
|    height_reward         | 0.0799     |
|    joints_reward         | 0.00287    |
|    joints_vel_reward     | 7.68e-06   |
|    root_ori_reward       | 0.0019     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 310        |
|    ep_rew_mean           | 29.9       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 244        |
|    iterations            | 66         |
|    time_elapsed          | 22124      |
|    total_timesteps       | 5406720    |
|    train_time            | 32         |
| train/                   |            |
|    approx_kl             | 0.21065244 |
|    clip_fraction         | 0.668      |
|    clip_range            | 0.2        |
|    entropy_loss          | -1.11      |
|    explained_variance    | 0.853      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0947    |
|    n_updates             | 650        |
|    policy_gradient_loss  | -0.0732    |
|    std                   | 0.248      |
|    value_loss            | 0.043      |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.426     |
|    end_effectors_err     | 1.28      |
|    height_err            | -0.0185   |
|    joints_err            | 9.38      |
|    joints_vel_err        | 174       |
|    root_ori_err          | 0.456     |
| reward_terms/            |           |
|    com_reward            | 0.0257    |
|    end_effectors_reward  | 0.0267    |
|    height_reward         | 0.084     |
|    joints_reward         | 0.00715   |
|    joints_vel_reward     | 4.74e-06  |
|    root_ori_reward       | 0.00402   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 293       |
|    ep_rew_mean           | 29        |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 246       |
|    iterations            | 67        |
|    time_elapsed          | 22271     |
|    total_timesteps       | 5488640   |
|    train_time            | 32.4      |
| train/                   |           |
|    approx_kl             | 0.2117981 |
|    clip_fraction         | 0.668     |
|    clip_range            | 0.2       |
|    entropy_loss          | -1        |
|    explained_variance    | 0.843     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0989   |
|    n_updates             | 660       |
|    policy_gradient_loss  | -0.073    |
|    std                   | 0.247     |
|    value_loss            | 0.0473    |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.66       |
|    end_effectors_err     | 19.6       |
|    height_err            | 0.0114     |
|    joints_err            | 15.7       |
|    joints_vel_err        | 187        |
|    root_ori_err          | 1.42       |
| reward_terms/            |            |
|    com_reward            | 0.00416    |
|    end_effectors_reward  | 0.0055     |
|    height_reward         | 0.0801     |
|    joints_reward         | 0.00153    |
|    joints_vel_reward     | 7e-06      |
|    root_ori_reward       | 0.00132    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 326        |
|    ep_rew_mean           | 31.7       |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 248        |
|    iterations            | 68         |
|    time_elapsed          | 22418      |
|    total_timesteps       | 5570560    |
|    train_time            | 31.5       |
| train/                   |            |
|    approx_kl             | 0.22745185 |
|    clip_fraction         | 0.673      |
|    clip_range            | 0.2        |
|    entropy_loss          | -0.857     |
|    explained_variance    | 0.823      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.109     |
|    n_updates             | 670        |
|    policy_gradient_loss  | -0.0705    |
|    std                   | 0.247      |
|    value_loss            | 0.0518     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.78       |
|    end_effectors_err     | 22.1       |
|    height_err            | 0.0167     |
|    joints_err            | 14         |
|    joints_vel_err        | 181        |
|    root_ori_err          | 1.13       |
| reward_terms/            |            |
|    com_reward            | 0.00477    |
|    end_effectors_reward  | 0.00475    |
|    height_reward         | 0.077      |
|    joints_reward         | 0.00135    |
|    joints_vel_reward     | 7.85e-06   |
|    root_ori_reward       | 0.000681   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 295        |
|    ep_rew_mean           | 28.7       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 250        |
|    iterations            | 69         |
|    time_elapsed          | 22563      |
|    total_timesteps       | 5652480    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.22902837 |
|    clip_fraction         | 0.674      |
|    clip_range            | 0.2        |
|    entropy_loss          | -0.772     |
|    explained_variance    | 0.838      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.1       |
|    n_updates             | 680        |
|    policy_gradient_loss  | -0.0742    |
|    std                   | 0.246      |
|    value_loss            | 0.0458     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.615      |
|    end_effectors_err     | 3.46       |
|    height_err            | -0.0194    |
|    joints_err            | 9.35       |
|    joints_vel_err        | 190        |
|    root_ori_err          | 0.36       |
| reward_terms/            |            |
|    com_reward            | 0.0281     |
|    end_effectors_reward  | 0.0187     |
|    height_reward         | 0.0767     |
|    joints_reward         | 0.00213    |
|    joints_vel_reward     | 6.42e-06   |
|    root_ori_reward       | 0.00208    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 337        |
|    ep_rew_mean           | 32.7       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 252        |
|    iterations            | 70         |
|    time_elapsed          | 22709      |
|    total_timesteps       | 5734400    |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.21908076 |
|    clip_fraction         | 0.671      |
|    clip_range            | 0.2        |
|    entropy_loss          | -0.682     |
|    explained_variance    | 0.828      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0637    |
|    n_updates             | 690        |
|    policy_gradient_loss  | -0.0704    |
|    std                   | 0.246      |
|    value_loss            | 0.0549     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.38       |
|    end_effectors_err     | 15.6       |
|    height_err            | -0.000146  |
|    joints_err            | 17.1       |
|    joints_vel_err        | 138        |
|    root_ori_err          | 1.66       |
| reward_terms/            |            |
|    com_reward            | 0.00438    |
|    end_effectors_reward  | 0.00339    |
|    height_reward         | 0.0802     |
|    joints_reward         | 0.0013     |
|    joints_vel_reward     | 3.55e-05   |
|    root_ori_reward       | 0.000701   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 318        |
|    ep_rew_mean           | 31         |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00199    |
|    fps                   | 254        |
|    iterations            | 71         |
|    time_elapsed          | 22855      |
|    total_timesteps       | 5816320    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.23769517 |
|    clip_fraction         | 0.676      |
|    clip_range            | 0.2        |
|    entropy_loss          | -0.554     |
|    explained_variance    | 0.838      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0687    |
|    n_updates             | 700        |
|    policy_gradient_loss  | -0.0734    |
|    std                   | 0.245      |
|    value_loss            | 0.046      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.23       |
|    end_effectors_err     | 12.9       |
|    height_err            | -0.0157    |
|    joints_err            | 15.9       |
|    joints_vel_err        | 157        |
|    root_ori_err          | 1.58       |
| reward_terms/            |            |
|    com_reward            | 0.00712    |
|    end_effectors_reward  | 0.00657    |
|    height_reward         | 0.0838     |
|    joints_reward         | 0.00178    |
|    joints_vel_reward     | 1.52e-05   |
|    root_ori_reward       | 0.000846   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 337        |
|    ep_rew_mean           | 32.8       |
| time/                    |            |
|    collect_time          | 116        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 256        |
|    iterations            | 72         |
|    time_elapsed          | 23003      |
|    total_timesteps       | 5898240    |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.22407703 |
|    clip_fraction         | 0.672      |
|    clip_range            | 0.2        |
|    entropy_loss          | -0.459     |
|    explained_variance    | 0.838      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0684    |
|    n_updates             | 710        |
|    policy_gradient_loss  | -0.0703    |
|    std                   | 0.245      |
|    value_loss            | 0.0522     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.03       |
|    end_effectors_err     | 9.54       |
|    height_err            | -0.000521  |
|    joints_err            | 15.8       |
|    joints_vel_err        | 144        |
|    root_ori_err          | 1.53       |
| reward_terms/            |            |
|    com_reward            | 0.0114     |
|    end_effectors_reward  | 0.0075     |
|    height_reward         | 0.0809     |
|    joints_reward         | 0.00183    |
|    joints_vel_reward     | 2.29e-05   |
|    root_ori_reward       | 0.00193    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 332        |
|    ep_rew_mean           | 32.3       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 258        |
|    iterations            | 73         |
|    time_elapsed          | 23149      |
|    total_timesteps       | 5980160    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.23706543 |
|    clip_fraction         | 0.674      |
|    clip_range            | 0.2        |
|    entropy_loss          | -0.352     |
|    explained_variance    | 0.843      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0623    |
|    n_updates             | 720        |
|    policy_gradient_loss  | -0.07      |
|    std                   | 0.244      |
|    value_loss            | 0.0464     |
-----------------------------------------
policy converted successfully and saved.
Saving video to /cluster/home/anghosh/DHProject/deep-mimic/log/2023-05-29-07-58-21-PylocoVanilla-v0-humanoid3d_crawl.txt-100.0M_1/eval-video-PylocoVanilla-v0-step-5280-to-step-6280.mp4
Eval num_timesteps=6000000, episode_reward=49.41 +/- 0.00
Episode length: 527.00 +/- 0.00
Success rate: 100.00%
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.71       |
|    end_effectors_err     | 21.8       |
|    height_err            | 0.0101     |
|    joints_err            | 14.1       |
|    joints_vel_err        | 149        |
|    root_ori_err          | 1.18       |
| eval/                    |            |
|    mean_ep_length        | 527        |
|    mean_reward           | 49.4       |
|    success_rate          | 1          |
| reward_terms/            |            |
|    com_reward            | 0.00399    |
|    end_effectors_reward  | 0.00447    |
|    height_reward         | 0.0789     |
|    joints_reward         | 0.0014     |
|    joints_vel_reward     | 1.76e-05   |
|    root_ori_reward       | 0.000776   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| time/                    |            |
|    evaluate_actions_time | 0.00203    |
|    total_timesteps       | 6000000    |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.24121866 |
|    clip_fraction         | 0.674      |
|    clip_range            | 0.2        |
|    entropy_loss          | -0.224     |
|    explained_variance    | 0.825      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0963    |
|    n_updates             | 730        |
|    policy_gradient_loss  | -0.0707    |
|    std                   | 0.243      |
|    value_loss            | 0.049      |
-----------------------------------------
New best mean reward!
--------------------------------------
| err_terms/              |          |
|    com_err              | 1.17     |
|    end_effectors_err    | 12.7     |
|    height_err           | 0.0167   |
|    joints_err           | 16.9     |
|    joints_vel_err       | 131      |
|    root_ori_err         | 1.72     |
| reward_terms/           |          |
|    com_reward           | 0.00473  |
|    end_effectors_reward | 0.00496  |
|    height_reward        | 0.0789   |
|    joints_reward        | 0.00165  |
|    joints_vel_reward    | 2.78e-05 |
|    root_ori_reward      | 0.00127  |
|    smoothness1_reward   | 0        |
|    smoothness2_reward   | 0        |
|    smoothness_reward    | 0        |
| rollout/                |          |
|    ep_len_mean          | 310      |
|    ep_rew_mean          | 30.4     |
| time/                   |          |
|    collect_time         | 6.35e+03 |
|    fps                  | 205      |
|    iterations           | 74       |
|    time_elapsed         | 29532    |
|    total_timesteps      | 6062080  |
--------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.0704     |
|    end_effectors_err     | 0.167      |
|    height_err            | -0.04      |
|    joints_err            | 3.19       |
|    joints_vel_err        | 306        |
|    root_ori_err          | 0.00165    |
| reward_terms/            |            |
|    com_reward            | 0.0928     |
|    end_effectors_reward  | 0.0451     |
|    height_reward         | 0.0909     |
|    joints_reward         | 0.0413     |
|    joints_vel_reward     | 8.23e-08   |
|    root_ori_reward       | 0.056      |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 330        |
|    ep_rew_mean           | 32         |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 207        |
|    iterations            | 75         |
|    time_elapsed          | 29678      |
|    total_timesteps       | 6144000    |
|    train_time            | 31.5       |
| train/                   |            |
|    approx_kl             | 0.24118695 |
|    clip_fraction         | 0.676      |
|    clip_range            | 0.2        |
|    entropy_loss          | -0.106     |
|    explained_variance    | 0.839      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.114     |
|    n_updates             | 740        |
|    policy_gradient_loss  | -0.0706    |
|    std                   | 0.243      |
|    value_loss            | 0.0517     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.09       |
|    end_effectors_err     | 11.8       |
|    height_err            | 0.0192     |
|    joints_err            | 17.1       |
|    joints_vel_err        | 133        |
|    root_ori_err          | 1.61       |
| reward_terms/            |            |
|    com_reward            | 0.00881    |
|    end_effectors_reward  | 0.00608    |
|    height_reward         | 0.0812     |
|    joints_reward         | 0.00116    |
|    joints_vel_reward     | 2.62e-05   |
|    root_ori_reward       | 0.00098    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 336        |
|    ep_rew_mean           | 33         |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 208        |
|    iterations            | 76         |
|    time_elapsed          | 29825      |
|    total_timesteps       | 6225920    |
|    train_time            | 32         |
| train/                   |            |
|    approx_kl             | 0.23626783 |
|    clip_fraction         | 0.675      |
|    clip_range            | 0.2        |
|    entropy_loss          | 0.0286     |
|    explained_variance    | 0.838      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0787    |
|    n_updates             | 750        |
|    policy_gradient_loss  | -0.0713    |
|    std                   | 0.242      |
|    value_loss            | 0.0478     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.26       |
|    end_effectors_err     | 14         |
|    height_err            | 0.0192     |
|    joints_err            | 16.2       |
|    joints_vel_err        | 134        |
|    root_ori_err          | 1.57       |
| reward_terms/            |            |
|    com_reward            | 0.00701    |
|    end_effectors_reward  | 0.0069     |
|    height_reward         | 0.0822     |
|    joints_reward         | 0.00124    |
|    joints_vel_reward     | 2.97e-05   |
|    root_ori_reward       | 0.00232    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 345        |
|    ep_rew_mean           | 33.4       |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 210        |
|    iterations            | 77         |
|    time_elapsed          | 29972      |
|    total_timesteps       | 6307840    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.24942407 |
|    clip_fraction         | 0.678      |
|    clip_range            | 0.2        |
|    entropy_loss          | 0.126      |
|    explained_variance    | 0.853      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.108     |
|    n_updates             | 760        |
|    policy_gradient_loss  | -0.0719    |
|    std                   | 0.242      |
|    value_loss            | 0.0449     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.23       |
|    end_effectors_err     | 13.5       |
|    height_err            | 0.0177     |
|    joints_err            | 17.2       |
|    joints_vel_err        | 169        |
|    root_ori_err          | 1.68       |
| reward_terms/            |            |
|    com_reward            | 0.00566    |
|    end_effectors_reward  | 0.00457    |
|    height_reward         | 0.0778     |
|    joints_reward         | 0.00136    |
|    joints_vel_reward     | 1e-05      |
|    root_ori_reward       | 0.00204    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 336        |
|    ep_rew_mean           | 32.6       |
| time/                    |            |
|    collect_time          | 111        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 212        |
|    iterations            | 78         |
|    time_elapsed          | 30116      |
|    total_timesteps       | 6389760    |
|    train_time            | 32.2       |
| train/                   |            |
|    approx_kl             | 0.25371733 |
|    clip_fraction         | 0.681      |
|    clip_range            | 0.2        |
|    entropy_loss          | 0.208      |
|    explained_variance    | 0.837      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0803    |
|    n_updates             | 770        |
|    policy_gradient_loss  | -0.0687    |
|    std                   | 0.241      |
|    value_loss            | 0.0482     |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 1.59      |
|    end_effectors_err     | 18        |
|    height_err            | 0.0037    |
|    joints_err            | 11.4      |
|    joints_vel_err        | 194       |
|    root_ori_err          | 0.667     |
| reward_terms/            |           |
|    com_reward            | 0.00428   |
|    end_effectors_reward  | 0.00591   |
|    height_reward         | 0.0788    |
|    joints_reward         | 0.00134   |
|    joints_vel_reward     | 2.44e-06  |
|    root_ori_reward       | 0.000859  |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 339       |
|    ep_rew_mean           | 33.2      |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 213       |
|    iterations            | 79        |
|    time_elapsed          | 30262     |
|    total_timesteps       | 6471680   |
|    train_time            | 31.5      |
| train/                   |           |
|    approx_kl             | 0.2652653 |
|    clip_fraction         | 0.682     |
|    clip_range            | 0.2       |
|    entropy_loss          | 0.319     |
|    explained_variance    | 0.854     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0948   |
|    n_updates             | 780       |
|    policy_gradient_loss  | -0.07     |
|    std                   | 0.24      |
|    value_loss            | 0.0467    |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.59       |
|    end_effectors_err     | 19.6       |
|    height_err            | 0.00455    |
|    joints_err            | 13.9       |
|    joints_vel_err        | 138        |
|    root_ori_err          | 1.08       |
| reward_terms/            |            |
|    com_reward            | 0.00626    |
|    end_effectors_reward  | 0.00563    |
|    height_reward         | 0.0785     |
|    joints_reward         | 0.00115    |
|    joints_vel_reward     | 2.7e-05    |
|    root_ori_reward       | 0.00179    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 342        |
|    ep_rew_mean           | 33.6       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 215        |
|    iterations            | 80         |
|    time_elapsed          | 30408      |
|    total_timesteps       | 6553600    |
|    train_time            | 32.4       |
| train/                   |            |
|    approx_kl             | 0.28453216 |
|    clip_fraction         | 0.686      |
|    clip_range            | 0.2        |
|    entropy_loss          | 0.417      |
|    explained_variance    | 0.837      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.037     |
|    n_updates             | 790        |
|    policy_gradient_loss  | -0.0691    |
|    std                   | 0.24       |
|    value_loss            | 0.041      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.25       |
|    end_effectors_err     | 13.4       |
|    height_err            | 0.00492    |
|    joints_err            | 16.2       |
|    joints_vel_err        | 144        |
|    root_ori_err          | 1.68       |
| reward_terms/            |            |
|    com_reward            | 0.00535    |
|    end_effectors_reward  | 0.00515    |
|    height_reward         | 0.0834     |
|    joints_reward         | 0.00153    |
|    joints_vel_reward     | 1.72e-05   |
|    root_ori_reward       | 0.00142    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 320        |
|    ep_rew_mean           | 31.7       |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 217        |
|    iterations            | 81         |
|    time_elapsed          | 30555      |
|    total_timesteps       | 6635520    |
|    train_time            | 32.2       |
| train/                   |            |
|    approx_kl             | 0.26940283 |
|    clip_fraction         | 0.685      |
|    clip_range            | 0.2        |
|    entropy_loss          | 0.472      |
|    explained_variance    | 0.859      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0369    |
|    n_updates             | 800        |
|    policy_gradient_loss  | -0.0684    |
|    std                   | 0.24       |
|    value_loss            | 0.0456     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.35       |
|    end_effectors_err     | 15.1       |
|    height_err            | 0.0284     |
|    joints_err            | 17.3       |
|    joints_vel_err        | 134        |
|    root_ori_err          | 1.72       |
| reward_terms/            |            |
|    com_reward            | 0.00477    |
|    end_effectors_reward  | 0.00418    |
|    height_reward         | 0.0841     |
|    joints_reward         | 0.00169    |
|    joints_vel_reward     | 2.56e-05   |
|    root_ori_reward       | 0.00198    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 316        |
|    ep_rew_mean           | 30.8       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 218        |
|    iterations            | 82         |
|    time_elapsed          | 30702      |
|    total_timesteps       | 6717440    |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.27562436 |
|    clip_fraction         | 0.684      |
|    clip_range            | 0.2        |
|    entropy_loss          | 0.583      |
|    explained_variance    | 0.858      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0798    |
|    n_updates             | 810        |
|    policy_gradient_loss  | -0.0682    |
|    std                   | 0.239      |
|    value_loss            | 0.0476     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.61       |
|    end_effectors_err     | 17.3       |
|    height_err            | 0.00336    |
|    joints_err            | 10.3       |
|    joints_vel_err        | 180        |
|    root_ori_err          | 0.575      |
| reward_terms/            |            |
|    com_reward            | 0.00454    |
|    end_effectors_reward  | 0.00667    |
|    height_reward         | 0.0756     |
|    joints_reward         | 0.00131    |
|    joints_vel_reward     | 4.47e-06   |
|    root_ori_reward       | 0.000641   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 348        |
|    ep_rew_mean           | 34.5       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 220        |
|    iterations            | 83         |
|    time_elapsed          | 30847      |
|    total_timesteps       | 6799360    |
|    train_time            | 32.2       |
| train/                   |            |
|    approx_kl             | 0.26477662 |
|    clip_fraction         | 0.685      |
|    clip_range            | 0.2        |
|    entropy_loss          | 0.668      |
|    explained_variance    | 0.866      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.109     |
|    n_updates             | 820        |
|    policy_gradient_loss  | -0.0709    |
|    std                   | 0.239      |
|    value_loss            | 0.0415     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.66       |
|    end_effectors_err     | 19.2       |
|    height_err            | -0.00015   |
|    joints_err            | 11.3       |
|    joints_vel_err        | 168        |
|    root_ori_err          | 0.641      |
| reward_terms/            |            |
|    com_reward            | 0.00463    |
|    end_effectors_reward  | 0.00689    |
|    height_reward         | 0.0743     |
|    joints_reward         | 0.0012     |
|    joints_vel_reward     | 2.23e-05   |
|    root_ori_reward       | 0.00109    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 326        |
|    ep_rew_mean           | 31.7       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00204    |
|    fps                   | 222        |
|    iterations            | 84         |
|    time_elapsed          | 30992      |
|    total_timesteps       | 6881280    |
|    train_time            | 31.9       |
| train/                   |            |
|    approx_kl             | 0.26731354 |
|    clip_fraction         | 0.682      |
|    clip_range            | 0.2        |
|    entropy_loss          | 0.792      |
|    explained_variance    | 0.861      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.079     |
|    n_updates             | 830        |
|    policy_gradient_loss  | -0.0663    |
|    std                   | 0.238      |
|    value_loss            | 0.0481     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.31       |
|    end_effectors_err     | 13.8       |
|    height_err            | 0.0113     |
|    joints_err            | 14.1       |
|    joints_vel_err        | 143        |
|    root_ori_err          | 1.3        |
| reward_terms/            |            |
|    com_reward            | 0.00901    |
|    end_effectors_reward  | 0.00729    |
|    height_reward         | 0.0783     |
|    joints_reward         | 0.00178    |
|    joints_vel_reward     | 2.91e-05   |
|    root_ori_reward       | 0.00144    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 357        |
|    ep_rew_mean           | 34.5       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 223        |
|    iterations            | 85         |
|    time_elapsed          | 31137      |
|    total_timesteps       | 6963200    |
|    train_time            | 31.5       |
| train/                   |            |
|    approx_kl             | 0.28186762 |
|    clip_fraction         | 0.687      |
|    clip_range            | 0.2        |
|    entropy_loss          | 0.93       |
|    explained_variance    | 0.876      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.106     |
|    n_updates             | 840        |
|    policy_gradient_loss  | -0.0697    |
|    std                   | 0.237      |
|    value_loss            | 0.0399     |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 1.32      |
|    end_effectors_err     | 15.4      |
|    height_err            | 0.00503   |
|    joints_err            | 16.9      |
|    joints_vel_err        | 139       |
|    root_ori_err          | 1.66      |
| reward_terms/            |           |
|    com_reward            | 0.00534   |
|    end_effectors_reward  | 0.0038    |
|    height_reward         | 0.081     |
|    joints_reward         | 0.00141   |
|    joints_vel_reward     | 1.69e-05  |
|    root_ori_reward       | 0.00165   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 317       |
|    ep_rew_mean           | 31.5      |
| time/                    |           |
|    collect_time          | 115       |
|    evaluate_actions_time | 0.00202   |
|    fps                   | 225       |
|    iterations            | 86        |
|    time_elapsed          | 31284     |
|    total_timesteps       | 7045120   |
|    train_time            | 32        |
| train/                   |           |
|    approx_kl             | 0.3014795 |
|    clip_fraction         | 0.684     |
|    clip_range            | 0.2       |
|    entropy_loss          | 1.01      |
|    explained_variance    | 0.848     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0995   |
|    n_updates             | 850       |
|    policy_gradient_loss  | -0.0649   |
|    std                   | 0.237     |
|    value_loss            | 0.0468    |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.39       |
|    end_effectors_err     | 15.9       |
|    height_err            | 0.00891    |
|    joints_err            | 16.8       |
|    joints_vel_err        | 135        |
|    root_ori_err          | 1.69       |
| reward_terms/            |            |
|    com_reward            | 0.00562    |
|    end_effectors_reward  | 0.00411    |
|    height_reward         | 0.0785     |
|    joints_reward         | 0.00144    |
|    joints_vel_reward     | 4.31e-05   |
|    root_ori_reward       | 0.00131    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 323        |
|    ep_rew_mean           | 32         |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.002      |
|    fps                   | 226        |
|    iterations            | 87         |
|    time_elapsed          | 31429      |
|    total_timesteps       | 7127040    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.29531083 |
|    clip_fraction         | 0.687      |
|    clip_range            | 0.2        |
|    entropy_loss          | 1.08       |
|    explained_variance    | 0.862      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0814    |
|    n_updates             | 860        |
|    policy_gradient_loss  | -0.0671    |
|    std                   | 0.236      |
|    value_loss            | 0.0437     |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.0108    |
|    end_effectors_err     | 0.0954    |
|    height_err            | 0.0106    |
|    joints_err            | 0.646     |
|    joints_vel_err        | 691       |
|    root_ori_err          | 0.00485   |
| reward_terms/            |           |
|    com_reward            | 0.0998    |
|    end_effectors_reward  | 0.0472    |
|    height_reward         | 0.0994    |
|    joints_reward         | 0.186     |
|    joints_vel_reward     | 6.06e-26  |
|    root_ori_reward       | 0.0251    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 348       |
|    ep_rew_mean           | 34        |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00201   |
|    fps                   | 228       |
|    iterations            | 88        |
|    time_elapsed          | 31575     |
|    total_timesteps       | 7208960   |
|    train_time            | 31.5      |
| train/                   |           |
|    approx_kl             | 0.2998777 |
|    clip_fraction         | 0.688     |
|    clip_range            | 0.2       |
|    entropy_loss          | 1.19      |
|    explained_variance    | 0.853     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0529   |
|    n_updates             | 870       |
|    policy_gradient_loss  | -0.0658   |
|    std                   | 0.236     |
|    value_loss            | 0.0446    |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.16       |
|    end_effectors_err     | 11.9       |
|    height_err            | 0.0193     |
|    joints_err            | 16.1       |
|    joints_vel_err        | 149        |
|    root_ori_err          | 1.58       |
| reward_terms/            |            |
|    com_reward            | 0.00777    |
|    end_effectors_reward  | 0.00616    |
|    height_reward         | 0.0793     |
|    joints_reward         | 0.00161    |
|    joints_vel_reward     | 1.72e-05   |
|    root_ori_reward       | 0.00164    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 345        |
|    ep_rew_mean           | 33.8       |
| time/                    |            |
|    collect_time          | 115        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 229        |
|    iterations            | 89         |
|    time_elapsed          | 31722      |
|    total_timesteps       | 7290880    |
|    train_time            | 32.1       |
| train/                   |            |
|    approx_kl             | 0.30835557 |
|    clip_fraction         | 0.692      |
|    clip_range            | 0.2        |
|    entropy_loss          | 1.26       |
|    explained_variance    | 0.86       |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0342    |
|    n_updates             | 880        |
|    policy_gradient_loss  | -0.0654    |
|    std                   | 0.235      |
|    value_loss            | 0.0453     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.25       |
|    end_effectors_err     | 14.3       |
|    height_err            | 0.0065     |
|    joints_err            | 17         |
|    joints_vel_err        | 135        |
|    root_ori_err          | 1.74       |
| reward_terms/            |            |
|    com_reward            | 0.00484    |
|    end_effectors_reward  | 0.00396    |
|    height_reward         | 0.0824     |
|    joints_reward         | 0.00128    |
|    joints_vel_reward     | 3.61e-05   |
|    root_ori_reward       | 0.000665   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 361        |
|    ep_rew_mean           | 35.2       |
| time/                    |            |
|    collect_time          | 113        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 231        |
|    iterations            | 90         |
|    time_elapsed          | 31867      |
|    total_timesteps       | 7372800    |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.29282373 |
|    clip_fraction         | 0.687      |
|    clip_range            | 0.2        |
|    entropy_loss          | 1.42       |
|    explained_variance    | 0.869      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0382    |
|    n_updates             | 890        |
|    policy_gradient_loss  | -0.0665    |
|    std                   | 0.234      |
|    value_loss            | 0.0449     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.54       |
|    end_effectors_err     | 17         |
|    height_err            | 0.0168     |
|    joints_err            | 16.2       |
|    joints_vel_err        | 172        |
|    root_ori_err          | 1.6        |
| reward_terms/            |            |
|    com_reward            | 0.00433    |
|    end_effectors_reward  | 0.00488    |
|    height_reward         | 0.0793     |
|    joints_reward         | 0.00182    |
|    joints_vel_reward     | 1.79e-05   |
|    root_ori_reward       | 0.000858   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 355        |
|    ep_rew_mean           | 34.7       |
| time/                    |            |
|    collect_time          | 116        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 232        |
|    iterations            | 91         |
|    time_elapsed          | 32014      |
|    total_timesteps       | 7454720    |
|    train_time            | 31.7       |
| train/                   |            |
|    approx_kl             | 0.32507896 |
|    clip_fraction         | 0.695      |
|    clip_range            | 0.2        |
|    entropy_loss          | 1.51       |
|    explained_variance    | 0.876      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0666    |
|    n_updates             | 900        |
|    policy_gradient_loss  | -0.0686    |
|    std                   | 0.234      |
|    value_loss            | 0.0385     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.22       |
|    end_effectors_err     | 13.6       |
|    height_err            | 0.0236     |
|    joints_err            | 14.9       |
|    joints_vel_err        | 178        |
|    root_ori_err          | 1.37       |
| reward_terms/            |            |
|    com_reward            | 0.00838    |
|    end_effectors_reward  | 0.00565    |
|    height_reward         | 0.0825     |
|    joints_reward         | 0.00136    |
|    joints_vel_reward     | 1.04e-05   |
|    root_ori_reward       | 0.0018     |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 309        |
|    ep_rew_mean           | 30.9       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 234        |
|    iterations            | 92         |
|    time_elapsed          | 32160      |
|    total_timesteps       | 7536640    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.33745402 |
|    clip_fraction         | 0.697      |
|    clip_range            | 0.2        |
|    entropy_loss          | 1.59       |
|    explained_variance    | 0.846      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0958    |
|    n_updates             | 910        |
|    policy_gradient_loss  | -0.0689    |
|    std                   | 0.234      |
|    value_loss            | 0.038      |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.42       |
|    end_effectors_err     | 16         |
|    height_err            | 0.0053     |
|    joints_err            | 16.8       |
|    joints_vel_err        | 158        |
|    root_ori_err          | 1.67       |
| reward_terms/            |            |
|    com_reward            | 0.0058     |
|    end_effectors_reward  | 0.00467    |
|    height_reward         | 0.0796     |
|    joints_reward         | 0.00109    |
|    joints_vel_reward     | 1.53e-05   |
|    root_ori_reward       | 0.00218    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 344        |
|    ep_rew_mean           | 33.9       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00202    |
|    fps                   | 235        |
|    iterations            | 93         |
|    time_elapsed          | 32306      |
|    total_timesteps       | 7618560    |
|    train_time            | 32         |
| train/                   |            |
|    approx_kl             | 0.31706485 |
|    clip_fraction         | 0.695      |
|    clip_range            | 0.2        |
|    entropy_loss          | 1.7        |
|    explained_variance    | 0.862      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0968    |
|    n_updates             | 920        |
|    policy_gradient_loss  | -0.0662    |
|    std                   | 0.233      |
|    value_loss            | 0.0441     |
-----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 0.666      |
|    end_effectors_err     | 6.42       |
|    height_err            | 0.0247     |
|    joints_err            | 10.6       |
|    joints_vel_err        | 180        |
|    root_ori_err          | 0.687      |
| reward_terms/            |            |
|    com_reward            | 0.0201     |
|    end_effectors_reward  | 0.00968    |
|    height_reward         | 0.0748     |
|    joints_reward         | 0.0017     |
|    joints_vel_reward     | 7.55e-06   |
|    root_ori_reward       | 0.000833   |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 315        |
|    ep_rew_mean           | 31.4       |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00203    |
|    fps                   | 237        |
|    iterations            | 94         |
|    time_elapsed          | 32452      |
|    total_timesteps       | 7700480    |
|    train_time            | 31.8       |
| train/                   |            |
|    approx_kl             | 0.30270833 |
|    clip_fraction         | 0.688      |
|    clip_range            | 0.2        |
|    entropy_loss          | 1.78       |
|    explained_variance    | 0.857      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0111    |
|    n_updates             | 930        |
|    policy_gradient_loss  | -0.064     |
|    std                   | 0.233      |
|    value_loss            | 0.0484     |
-----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 0.0516    |
|    end_effectors_err     | 0.228     |
|    height_err            | -0.0232   |
|    joints_err            | 4.6       |
|    joints_vel_err        | 301       |
|    root_ori_err          | 0.00727   |
| reward_terms/            |           |
|    com_reward            | 0.0963    |
|    end_effectors_reward  | 0.0436    |
|    height_reward         | 0.0962    |
|    joints_reward         | 0.0332    |
|    joints_vel_reward     | 1.07e-06  |
|    root_ori_reward       | 0.0157    |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 324       |
|    ep_rew_mean           | 32.6      |
| time/                    |           |
|    collect_time          | 114       |
|    evaluate_actions_time | 0.00204   |
|    fps                   | 238       |
|    iterations            | 95        |
|    time_elapsed          | 32599     |
|    total_timesteps       | 7782400   |
|    train_time            | 32        |
| train/                   |           |
|    approx_kl             | 0.3171957 |
|    clip_fraction         | 0.692     |
|    clip_range            | 0.2       |
|    entropy_loss          | 1.88      |
|    explained_variance    | 0.87      |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.1      |
|    n_updates             | 940       |
|    policy_gradient_loss  | -0.0662   |
|    std                   | 0.232     |
|    value_loss            | 0.0477    |
----------------------------------------
----------------------------------------
| err_terms/               |           |
|    com_err               | 1.08      |
|    end_effectors_err     | 11.7      |
|    height_err            | 0.0145    |
|    joints_err            | 17.2      |
|    joints_vel_err        | 146       |
|    root_ori_err          | 1.69      |
| reward_terms/            |           |
|    com_reward            | 0.00573   |
|    end_effectors_reward  | 0.00434   |
|    height_reward         | 0.082     |
|    joints_reward         | 0.00146   |
|    joints_vel_reward     | 2.52e-05  |
|    root_ori_reward       | 0.00102   |
|    smoothness1_reward    | 0         |
|    smoothness2_reward    | 0         |
|    smoothness_reward     | 0         |
| rollout/                 |           |
|    ep_len_mean           | 349       |
|    ep_rew_mean           | 34.4      |
| time/                    |           |
|    collect_time          | 113       |
|    evaluate_actions_time | 0.00199   |
|    fps                   | 240       |
|    iterations            | 96        |
|    time_elapsed          | 32744     |
|    total_timesteps       | 7864320   |
|    train_time            | 31.3      |
| train/                   |           |
|    approx_kl             | 0.3209117 |
|    clip_fraction         | 0.697     |
|    clip_range            | 0.2       |
|    entropy_loss          | 1.95      |
|    explained_variance    | 0.871     |
|    learning_rate_log_std | 0.0003    |
|    learning_rate_policy  | 5e-05     |
|    learning_rate_value   | 0.01      |
|    loss                  | -0.0856   |
|    n_updates             | 950       |
|    policy_gradient_loss  | -0.0676   |
|    std                   | 0.232     |
|    value_loss            | 0.0435    |
----------------------------------------
-----------------------------------------
| err_terms/               |            |
|    com_err               | 1.32       |
|    end_effectors_err     | 13.9       |
|    height_err            | 0.0177     |
|    joints_err            | 13.5       |
|    joints_vel_err        | 149        |
|    root_ori_err          | 1.09       |
| reward_terms/            |            |
|    com_reward            | 0.00719    |
|    end_effectors_reward  | 0.00683    |
|    height_reward         | 0.0758     |
|    joints_reward         | 0.00143    |
|    joints_vel_reward     | 2.2e-05    |
|    root_ori_reward       | 0.00181    |
|    smoothness1_reward    | 0          |
|    smoothness2_reward    | 0          |
|    smoothness_reward     | 0          |
| rollout/                 |            |
|    ep_len_mean           | 364        |
|    ep_rew_mean           | 36         |
| time/                    |            |
|    collect_time          | 114        |
|    evaluate_actions_time | 0.00201    |
|    fps                   | 241        |
|    iterations            | 97         |
|    time_elapsed          | 32890      |
|    total_timesteps       | 7946240    |
|    train_time            | 31.6       |
| train/                   |            |
|    approx_kl             | 0.35665235 |
|    clip_fraction         | 0.696      |
|    clip_range            | 0.2        |
|    entropy_loss          | 2.03       |
|    explained_variance    | 0.852      |
|    learning_rate_log_std | 0.0003     |
|    learning_rate_policy  | 5e-05      |
|    learning_rate_value   | 0.01       |
|    loss                  | -0.0864    |
|    n_updates             | 960        |
|    policy_gradient_loss  | -0.0671    |
|    std                   | 0.231      |
|    value_loss            | 0.0427     |
-----------------------------------------
